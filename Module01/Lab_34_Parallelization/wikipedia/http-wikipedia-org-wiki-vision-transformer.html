<!DOCTYPE html>
<html class="client-nojs" lang="en" dir="ltr">
<head>
<meta charset="UTF-8"/>
<title>Vision transformer - Wikipedia</title>
<script>document.documentElement.className="client-js";RLCONF={"wgBreakFrames":false,"wgSeparatorTransformTable":["",""],"wgDigitTransformTable":["",""],"wgDefaultDateFormat":"dmy","wgMonthNames":["","January","February","March","April","May","June","July","August","September","October","November","December"],"wgRequestId":"d941bfb0-207f-4481-b286-a527bb6ff8d6","wgCSPNonce":false,"wgCanonicalNamespace":"","wgCanonicalSpecialPageName":false,"wgNamespaceNumber":0,"wgPageName":"Vision_transformer","wgTitle":"Vision transformer","wgCurRevisionId":1073424596,"wgRevisionId":1073424596,"wgArticleId":68212199,"wgIsArticle":true,"wgIsRedirect":false,"wgAction":"view","wgUserName":null,"wgUserGroups":["*"],"wgCategories":["Neural networks","Image processing"],"wgPageContentLanguage":"en","wgPageContentModel":"wikitext","wgRelevantPageName":"Vision_transformer","wgRelevantArticleId":68212199,"wgIsProbablyEditable":true,"wgRelevantPageIsProbablyEditable":true,"wgRestrictionEdit":[],"wgRestrictionMove":[],
"wgFlaggedRevsParams":{"tags":{"status":{"levels":-1}}},"wgVisualEditor":{"pageLanguageCode":"en","pageLanguageDir":"ltr","pageVariantFallbacks":"en"},"wgMFDisplayWikibaseDescriptions":{"search":true,"nearby":true,"watchlist":true,"tagline":false},"wgWMESchemaEditAttemptStepOversample":false,"wgWMEPageLength":10000,"wgNoticeProject":"wikipedia","wgMediaViewerOnClick":true,"wgMediaViewerEnabledByDefault":true,"wgPopupsFlags":10,"wgULSCurrentAutonym":"English","wgEditSubmitButtonLabelPublish":true,"wgCentralAuthMobileDomain":false,"wgULSPosition":"interlanguage","wgULSisCompactLinksEnabled":true,"wgWikibaseItemId":"Q107675654","wgGENewcomerTasksGuidanceEnabled":true,"wgGEAskQuestionEnabled":false,"wgGELinkRecommendationsFrontendEnabled":false};RLSTATE={"ext.globalCssJs.user.styles":"ready","site.styles":"ready","user.styles":"ready","ext.globalCssJs.user":"ready","user":"ready","user.options":"loading","ext.cite.styles":"ready","skins.vector.styles.legacy":"ready",
"ext.visualEditor.desktopArticleTarget.noscript":"ready","ext.wikimediaBadges":"ready","ext.uls.interlanguage":"ready","wikibase.client.init":"ready"};RLPAGEMODULES=["ext.cite.ux-enhancements","site","mediawiki.page.ready","mediawiki.toc","skins.vector.legacy.js","ext.gadget.ReferenceTooltips","ext.gadget.charinsert","ext.gadget.extra-toolbar-buttons","ext.gadget.refToolbar","ext.gadget.switcher","mmv.head","mmv.bootstrap.autostart","ext.visualEditor.desktopArticleTarget.init","ext.visualEditor.targetLoader","ext.eventLogging","ext.wikimediaEvents","ext.navigationTiming","ext.cx.eventlogging.campaigns","ext.centralNotice.geoIP","ext.centralNotice.startUp","ext.centralauth.centralautologin","ext.popups","ext.uls.compactlinks","ext.uls.interface","ext.growthExperiments.SuggestedEditSession"];</script>
<script>(RLQ=window.RLQ||[]).push(function(){mw.loader.implement("user.options@1i9g4",function($,jQuery,require,module){mw.user.tokens.set({"patrolToken":"+\\","watchToken":"+\\","csrfToken":"+\\"});});});</script>
<link rel="stylesheet" href="/w/load.php?lang=en&amp;modules=ext.cite.styles%7Cext.uls.interlanguage%7Cext.visualEditor.desktopArticleTarget.noscript%7Cext.wikimediaBadges%7Cskins.vector.styles.legacy%7Cwikibase.client.init&amp;only=styles&amp;skin=vector"/>
<script async="" src="/w/load.php?lang=en&amp;modules=startup&amp;only=scripts&amp;raw=1&amp;skin=vector"></script>
<meta name="ResourceLoaderDynamicStyles" content=""/>
<link rel="stylesheet" href="/w/load.php?lang=en&amp;modules=site.styles&amp;only=styles&amp;skin=vector"/>
<meta name="generator" content="MediaWiki 1.38.0-wmf.25"/>
<meta name="referrer" content="origin"/>
<meta name="referrer" content="origin-when-crossorigin"/>
<meta name="referrer" content="origin-when-cross-origin"/>
<meta name="format-detection" content="telephone=no"/>
<meta property="og:title" content="Vision transformer - Wikipedia"/>
<meta property="og:type" content="website"/>
<link rel="preconnect" href="//upload.wikimedia.org"/>
<link rel="alternate" media="only screen and (max-width: 720px)" href="//en.m.wikipedia.org/wiki/Vision_transformer"/>
<link rel="alternate" type="application/x-wiki" title="Edit this page" href="/w/index.php?title=Vision_transformer&amp;action=edit"/>
<link rel="apple-touch-icon" href="/static/apple-touch/wikipedia.png"/>
<link rel="shortcut icon" href="/static/favicon/wikipedia.ico"/>
<link rel="search" type="application/opensearchdescription+xml" href="/w/opensearch_desc.php" title="Wikipedia (en)"/>
<link rel="EditURI" type="application/rsd+xml" href="//en.wikipedia.org/w/api.php?action=rsd"/>
<link rel="license" href="https://creativecommons.org/licenses/by-sa/3.0/"/>
<link rel="canonical" href="https://en.wikipedia.org/wiki/Vision_transformer"/>
<link rel="dns-prefetch" href="//meta.wikimedia.org" />
<link rel="dns-prefetch" href="//login.wikimedia.org"/>
</head>
<body class="mediawiki ltr sitedir-ltr mw-hide-empty-elt ns-0 ns-subject mw-editable page-Vision_transformer rootpage-Vision_transformer skin-vector action-view skin-vector-legacy"><div id="mw-page-base" class="noprint"></div>
<div id="mw-head-base" class="noprint"></div>
<div id="content" class="mw-body" role="main">
	<a id="top"></a>
	<div id="siteNotice"><!-- CentralNotice --></div>
	<div class="mw-indicators">
	</div>
	<h1 id="firstHeading" class="firstHeading mw-first-heading">Vision transformer</h1>
	<div id="bodyContent" class="vector-body">
		<div id="siteSub" class="noprint">From Wikipedia, the free encyclopedia</div>
		<div id="contentSub"></div>
		<div id="contentSub2"></div>
		
		<div id="jump-to-nav"></div>
		<a class="mw-jump-link" href="#mw-head">Jump to navigation</a>
		<a class="mw-jump-link" href="#searchInput">Jump to search</a>
		<div id="mw-content-text" class="mw-body-content mw-content-ltr" lang="en" dir="ltr"><div class="mw-parser-output"><p>A <b>Vision Transformer</b> (<b>ViT</b>) is a <a href="/wiki/Transformer_(machine_learning_model)" title="Transformer (machine learning model)">transformer</a> that is targeted at vision processing tasks such as <a href="/wiki/Image_recognition" class="mw-redirect" title="Image recognition">image recognition</a>.<sup id="cite_ref-:0_1-0" class="reference"><a href="#cite_note-:0-1">&#91;1&#93;</a></sup>
</p>
<div id="toc" class="toc" role="navigation" aria-labelledby="mw-toc-heading"><input type="checkbox" role="button" id="toctogglecheckbox" class="toctogglecheckbox" style="display:none" /><div class="toctitle" lang="en" dir="ltr"><h2 id="mw-toc-heading">Contents</h2><span class="toctogglespan"><label class="toctogglelabel" for="toctogglecheckbox"></label></span></div>
<ul>
<li class="toclevel-1 tocsection-1"><a href="#Vision_Transformers"><span class="tocnumber">1</span> <span class="toctext">Vision Transformers</span></a></li>
<li class="toclevel-1 tocsection-2"><a href="#History"><span class="tocnumber">2</span> <span class="toctext">History</span></a></li>
<li class="toclevel-1 tocsection-3"><a href="#Comparison_with_Convolutional_Neural_Networks"><span class="tocnumber">3</span> <span class="toctext">Comparison with Convolutional Neural Networks</span></a></li>
<li class="toclevel-1 tocsection-4"><a href="#The_Role_of_Self-Supervised_Learning"><span class="tocnumber">4</span> <span class="toctext">The Role of Self-Supervised Learning</span></a></li>
<li class="toclevel-1 tocsection-5"><a href="#Applications"><span class="tocnumber">5</span> <span class="toctext">Applications</span></a></li>
<li class="toclevel-1 tocsection-6"><a href="#Implementations"><span class="tocnumber">6</span> <span class="toctext">Implementations</span></a></li>
<li class="toclevel-1 tocsection-7"><a href="#See_also"><span class="tocnumber">7</span> <span class="toctext">See also</span></a></li>
<li class="toclevel-1 tocsection-8"><a href="#References"><span class="tocnumber">8</span> <span class="toctext">References</span></a></li>
<li class="toclevel-1 tocsection-9"><a href="#External_links"><span class="tocnumber">9</span> <span class="toctext">External links</span></a></li>
</ul>
</div>

<h2><span class="mw-headline" id="Vision_Transformers">Vision Transformers</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Vision_transformer&amp;action=edit&amp;section=1" title="Edit section: Vision Transformers">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<div class="thumb tright"><div class="thumbinner" style="width:452px;"><a href="/wiki/File:Vision_Transformer.gif" class="image"><img alt="" src="//upload.wikimedia.org/wikipedia/commons/thumb/3/3e/Vision_Transformer.gif/450px-Vision_Transformer.gif" decoding="async" width="450" height="253" class="thumbimage" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/3/3e/Vision_Transformer.gif/675px-Vision_Transformer.gif 1.5x, //upload.wikimedia.org/wikipedia/commons/3/3e/Vision_Transformer.gif 2x" data-file-width="853" data-file-height="480" /></a>  <div class="thumbcaption"><div class="magnify"><a href="/wiki/File:Vision_Transformer.gif" class="internal" title="Enlarge"></a></div>Vision Transformer Architecture for Image Classification</div></div></div>
<p>Transformers found their initial applications in <a href="/wiki/Natural_language_processing" title="Natural language processing">natural language processing</a> (NLP) tasks, as demonstrated by <a href="/wiki/Language_models" class="mw-redirect" title="Language models">language models</a> such as <a href="/wiki/BERT_(language_model)" title="BERT (language model)">BERT</a> and <a href="/wiki/GPT-3" title="GPT-3">GPT-3</a>. By contrast the typical image processing system uses a <a href="/wiki/Convolutional_neural_network" title="Convolutional neural network">convolutional neural network</a> (CNN). Well-known projects include <a href="/wiki/Critical_Software" title="Critical Software">Xception</a>, <a href="/wiki/Residual_neural_network" title="Residual neural network">ResNet</a>, EfficientNet,<sup id="cite_ref-2" class="reference"><a href="#cite_note-2">&#91;2&#93;</a></sup> DenseNet,<sup id="cite_ref-3" class="reference"><a href="#cite_note-3">&#91;3&#93;</a></sup> and Inception.<sup id="cite_ref-:0_1-1" class="reference"><a href="#cite_note-:0-1">&#91;1&#93;</a></sup>
</p><p>Transformers measure the relationships between pairs of input tokens (words in the case of text strings), termed <a href="/wiki/Attention_(machine_learning)" title="Attention (machine learning)">attention</a>. The cost is exponential with the number of tokens. For images, the basic unit of analysis is the <a href="/wiki/Pixel" title="Pixel">pixel</a>. However, computing relationships for every pixel pair in a typical image is prohibitive in terms of memory and computation. Instead, ViT computes relationships among pixels in various small sections of the image (e.g., 16x16 pixels), at a drastically reduced cost. The sections (with positional embeddings) are placed in a sequence. The embeddings are learnable vectors. Each section is arranged into a linear sequence and multiplied by the embedding matrix. The result, with the position embedding is fed to the transformer.<sup id="cite_ref-:0_1-2" class="reference"><a href="#cite_note-:0-1">&#91;1&#93;</a></sup>
</p><p>As in the case of <a href="/wiki/BERT_(language_model)" title="BERT (language model)">BERT</a>, a fundamental role in classification tasks is played by the class token. A special token that is used as the only input of the final <a href="/wiki/Multilayer_perceptron" title="Multilayer perceptron">MLP</a> Head as it has been influenced by all the others.
</p><p>The architecture for image classification is the most common and uses only the Transformer Encoder in order to transform the various input tokens. However, there are also other applications in which the decoder part of the traditional Transformer Architecture is also used.
</p>
<h2><span class="mw-headline" id="History">History</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Vision_transformer&amp;action=edit&amp;section=2" title="Edit section: History">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>Transformers initially introduced in 2017 in the well-known paper "Attention is All You Need"<sup id="cite_ref-4" class="reference"><a href="#cite_note-4">&#91;4&#93;</a></sup> have spread widely in the field of Natural Language Processing soon becoming one of the most widely used and promising architectures in the field.
</p><p>In 2020 Vision Transformers were then adapted for tasks in Computer Vision with the paper "An image is worth 16x16 words".<sup id="cite_ref-5" class="reference"><a href="#cite_note-5">&#91;5&#93;</a></sup> The idea is basically to break down input images as a series of patches which, once transformed into vectors, are seen as words in a normal transformer.
</p><p>If in the field of Natural Language Processing the mechanism of attention of the Transformers tried to capture the relationships between different words of the text to be analysed, in Computer Vision the Vision Transformers try instead to capture the relationships between different portions of an image.
</p><p>In 2021 a pure transformer model demonstrated better performance and greater efficiency than CNNs on image classification.<sup id="cite_ref-:0_1-3" class="reference"><a href="#cite_note-:0-1">&#91;1&#93;</a></sup>
</p><p>A study in June 2021 added a transformer backend to Resnet, which dramatically reduced costs and increased accuracy.<sup id="cite_ref-:1_6-0" class="reference"><a href="#cite_note-:1-6">&#91;6&#93;</a></sup><sup id="cite_ref-7" class="reference"><a href="#cite_note-7">&#91;7&#93;</a></sup><sup id="cite_ref-:2_8-0" class="reference"><a href="#cite_note-:2-8">&#91;8&#93;</a></sup>
</p><p>In the same year, some important variants of the Vision Transformers were proposed. These variants are mainly intended to be more efficient, more accurate or better suited to a specific domain. Among the most relevant is the Swin Transformer,<sup id="cite_ref-9" class="reference"><a href="#cite_note-9">&#91;9&#93;</a></sup> which through some modifications to the attention mechanism and a multi-stage approach achieved state-of-the-art results on some object detection datasets such as <a href="/wiki/COCO_(dataset)" class="mw-redirect" title="COCO (dataset)">COCO</a>. Another interesting variant is the TimeSformer, designed for video understanding tasks and able to capture spatial and temporal information through the use of divided space-time attention.<sup id="cite_ref-10" class="reference"><a href="#cite_note-10">&#91;10&#93;</a></sup><sup id="cite_ref-11" class="reference"><a href="#cite_note-11">&#91;11&#93;</a></sup>
</p><p>Vision Transformers were also able to get out of the lab and into one of the most important fields of Computer Vision, <a href="/wiki/Autonomous_driving" class="mw-redirect" title="Autonomous driving">autonomous driving</a>. <a href="/wiki/Tesla,_Inc." title="Tesla, Inc.">Tesla</a>'s engineers showed during Tesla AI Day<sup id="cite_ref-12" class="reference"><a href="#cite_note-12">&#91;12&#93;</a></sup> that their <a href="/wiki/Tesla_Autopilot" title="Tesla Autopilot">Tesla Autopilot</a> uses a Transformer on the multi-camera system in cars.
</p>
<h2><span class="mw-headline" id="Comparison_with_Convolutional_Neural_Networks">Comparison with Convolutional Neural Networks</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Vision_transformer&amp;action=edit&amp;section=3" title="Edit section: Comparison with Convolutional Neural Networks">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>ViT performance depends on decisions including that of the optimizer, dataset-specific <a href="/wiki/Hyperparameter_(machine_learning)" title="Hyperparameter (machine learning)">hyperparameters</a>, and network depth. CNN are much easier to optimize.
</p><p>A variation on a pure transformer is to marry a transformer to a CNN stem/front end. A typical ViT stem uses a 16x16 convolution with a 16 stride. By contrast a 3x3 convolution with stride 2, increases stability and also improves accuracy.<sup id="cite_ref-:2_8-1" class="reference"><a href="#cite_note-:2-8">&#91;8&#93;</a></sup>
</p><p>The CNN translates from the basic pixel level to a feature map. A tokenizer translates the feature map into a series of tokens that are then fed into the transformer, which applies the attention mechanism to produce a series of output tokens. Finally, a projector reconnects the output tokens to the feature map. The latter allows the analysis to exploit potentially significant pixel-level details. This drastically reduces the number of tokens that need to be analyzed, reducing costs accordingly.<sup id="cite_ref-:1_6-1" class="reference"><a href="#cite_note-:1-6">&#91;6&#93;</a></sup>
</p><p>The differences between CNNs and Vision Transformers are many and lie mainly in their architectural differences.
</p><p>In fact, CNNs achieve excellent results even with training based on data volumes that are not as large as those required by Vision Transformers.
</p><p>This different behaviour seems to derive from the presence in the CNNs of some inductive biases that can be somehow exploited by these networks to grasp more quickly the particularities of the analysed images even if, on the other hand, they end up limiting them making it more complex to grasp global relations.<sup id="cite_ref-13" class="reference"><a href="#cite_note-13">&#91;13&#93;</a></sup><sup id="cite_ref-14" class="reference"><a href="#cite_note-14">&#91;14&#93;</a></sup>
</p><p>On the other hand, the Vision Transformers are free from these biases which leads them to be able to capture also global and wider range relations but at the cost of a more onerous training in terms of data.
</p><p>Vision Transformers also proved to be much more robust to input image distortions such as adversarial patches or permutations.<sup id="cite_ref-15" class="reference"><a href="#cite_note-15">&#91;15&#93;</a></sup>
</p><p>However, choosing one architecture over another is not always the wisest choice, and excellent results have been obtained in several Computer Vision tasks through hybrid architectures combining convolutional layers with Vision Transformers.<sup id="cite_ref-16" class="reference"><a href="#cite_note-16">&#91;16&#93;</a></sup><sup id="cite_ref-17" class="reference"><a href="#cite_note-17">&#91;17&#93;</a></sup><sup id="cite_ref-18" class="reference"><a href="#cite_note-18">&#91;18&#93;</a></sup>
</p>
<h2><span class="mw-headline" id="The_Role_of_Self-Supervised_Learning">The Role of Self-Supervised Learning</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Vision_transformer&amp;action=edit&amp;section=4" title="Edit section: The Role of Self-Supervised Learning">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>The considerable need for data during the training phase has made it essential to find alternative methods to train these models,<sup id="cite_ref-19" class="reference"><a href="#cite_note-19">&#91;19&#93;</a></sup> and a central role is now played by <a href="/wiki/Self-supervised_learning" title="Self-supervised learning">self-supervised methods</a>. Using these approaches, it is possible to train a neural network in an almost autonomous way, allowing it to deduce the peculiarities of a specific problem without having to build a large dataset or provide it with accurately assigned labels. Being able to train a Vision Transformer without having to have a huge vision dataset at its disposal could be the key to the widespread dissemination of this promising new architecture.
</p>
<h2><span class="mw-headline" id="Applications">Applications</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Vision_transformer&amp;action=edit&amp;section=5" title="Edit section: Applications">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>Vision Transformers have been used in many Computer Vision tasks with excellent results and in some cases even state-of-the-art.
</p><p>Among the most relevant areas of application are:
</p>
<ul><li><a href="/wiki/Image_classification" class="mw-redirect" title="Image classification">Image Classification</a></li>
<li><a href="/wiki/Object_detection" title="Object detection">Object Detection</a></li>
<li><a href="/wiki/Deepfake" title="Deepfake">Video Deepfake Detection</a></li>
<li><a href="/wiki/Image_segmentation" title="Image segmentation">Image segmentation</a></li>
<li><a href="/wiki/Anomaly_detection" title="Anomaly detection">Anomaly detection</a></li>
<li><a href="/wiki/Image_Synthesis" class="mw-redirect" title="Image Synthesis">Image Synthesis</a></li>
<li><a href="/wiki/Cluster_analysis" title="Cluster analysis">Cluster analysis</a></li>
<li><a href="/wiki/Autonomous_driving" class="mw-redirect" title="Autonomous driving">Autonomous Driving</a></li></ul>
<h2><span class="mw-headline" id="Implementations">Implementations</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Vision_transformer&amp;action=edit&amp;section=6" title="Edit section: Implementations">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>There are many implementations of Vision Transformers and its variants available in open source online. The main versions of this architecture have been implemented in <a href="/wiki/PyTorch" title="PyTorch">PyTorch</a><sup id="cite_ref-20" class="reference"><a href="#cite_note-20">&#91;20&#93;</a></sup> but implementations have also been made available for <a href="/wiki/TensorFlow" title="TensorFlow">TensorFlow</a>.<sup id="cite_ref-21" class="reference"><a href="#cite_note-21">&#91;21&#93;</a></sup>
</p>
<h2><span class="mw-headline" id="See_also">See also</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Vision_transformer&amp;action=edit&amp;section=7" title="Edit section: See also">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<ul><li><a href="/wiki/Transformer_(machine_learning_model)" title="Transformer (machine learning model)">Transformer (machine learning model)</a></li>
<li><a href="/wiki/Attention_(machine_learning)" title="Attention (machine learning)">Attention (machine learning)</a></li>
<li><a href="/wiki/Perceiver" title="Perceiver">Perceiver</a></li>
<li><a href="/wiki/Deep_learning" title="Deep learning">Deep learning</a></li>
<li><a href="/wiki/PyTorch" title="PyTorch">PyTorch</a></li>
<li><a href="/wiki/TensorFlow" title="TensorFlow">TensorFlow</a></li></ul>
<h2><span class="mw-headline" id="References">References</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Vision_transformer&amp;action=edit&amp;section=8" title="Edit section: References">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<style data-mw-deduplicate="TemplateStyles:r1011085734">.mw-parser-output .reflist{font-size:90%;margin-bottom:0.5em;list-style-type:decimal}.mw-parser-output .reflist .references{font-size:100%;margin-bottom:0;list-style-type:inherit}.mw-parser-output .reflist-columns-2{column-width:30em}.mw-parser-output .reflist-columns-3{column-width:25em}.mw-parser-output .reflist-columns{margin-top:0.3em}.mw-parser-output .reflist-columns ol{margin-top:0}.mw-parser-output .reflist-columns li{page-break-inside:avoid;break-inside:avoid-column}.mw-parser-output .reflist-upper-alpha{list-style-type:upper-alpha}.mw-parser-output .reflist-upper-roman{list-style-type:upper-roman}.mw-parser-output .reflist-lower-alpha{list-style-type:lower-alpha}.mw-parser-output .reflist-lower-greek{list-style-type:lower-greek}.mw-parser-output .reflist-lower-roman{list-style-type:lower-roman}</style><div class="reflist">
<div class="mw-references-wrap mw-references-columns"><ol class="references">
<li id="cite_note-:0-1"><span class="mw-cite-backlink">^ <a href="#cite_ref-:0_1-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-:0_1-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-:0_1-2"><sup><i><b>c</b></i></sup></a> <a href="#cite_ref-:0_1-3"><sup><i><b>d</b></i></sup></a></span> <span class="reference-text"><style data-mw-deduplicate="TemplateStyles:r1067248974">.mw-parser-output cite.citation{font-style:inherit;word-wrap:break-word}.mw-parser-output .citation q{quotes:"\"""\"""'""'"}.mw-parser-output .citation:target{background-color:rgba(0,127,255,0.133)}.mw-parser-output .id-lock-free a,.mw-parser-output .citation .cs1-lock-free a{background:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/6/65/Lock-green.svg")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-limited a,.mw-parser-output .id-lock-registration a,.mw-parser-output .citation .cs1-lock-limited a,.mw-parser-output .citation .cs1-lock-registration a{background:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/d/d6/Lock-gray-alt-2.svg")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-subscription a,.mw-parser-output .citation .cs1-lock-subscription a{background:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/a/aa/Lock-red-alt-2.svg")right 0.1em center/9px no-repeat}.mw-parser-output .cs1-ws-icon a{background:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/4/4c/Wikisource-logo.svg")right 0.1em center/12px no-repeat}.mw-parser-output .cs1-code{color:inherit;background:inherit;border:none;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;color:#d33}.mw-parser-output .cs1-visible-error{color:#d33}.mw-parser-output .cs1-maint{display:none;color:#3a3;margin-left:0.3em}.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right{padding-right:0.2em}.mw-parser-output .citation .mw-selflink{font-weight:inherit}</style><cite id="CITEREFSarkar2021" class="citation web cs1">Sarkar, Arjun (2021-05-20). <a rel="nofollow" class="external text" href="https://towardsdatascience.com/are-transformers-better-than-cnns-at-image-recognition-ced60ccc7c8">"Are Transformers better than CNN's at Image Recognition?"</a>. <i>Medium</i><span class="reference-accessdate">. Retrieved <span class="nowrap">2021-07-11</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Medium&amp;rft.atitle=Are+Transformers+better+than+CNN%27s+at+Image+Recognition%3F&amp;rft.date=2021-05-20&amp;rft.aulast=Sarkar&amp;rft.aufirst=Arjun&amp;rft_id=https%3A%2F%2Ftowardsdatascience.com%2Fare-transformers-better-than-cnns-at-image-recognition-ced60ccc7c8&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AVision+transformer" class="Z3988"></span></span>
</li>
<li id="cite_note-2"><span class="mw-cite-backlink"><b><a href="#cite_ref-2">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1067248974"/><cite id="CITEREFTanLe2021" class="citation arxiv cs1">Tan, Mingxing; Le, Quoc V. (23 June 2021). "EfficientNet V2: Smaller Models and Faster Training". <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="//arxiv.org/abs/2104.00298">2104.00298</a></span> [<a rel="nofollow" class="external text" href="//arxiv.org/archive/cs.CV">cs.CV</a>].</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=EfficientNet+V2%3A+Smaller+Models+and+Faster+Training&amp;rft.date=2021-06-23&amp;rft_id=info%3Aarxiv%2F2104.00298&amp;rft.aulast=Tan&amp;rft.aufirst=Mingxing&amp;rft.au=Le%2C+Quoc+V.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AVision+transformer" class="Z3988"></span></span>
</li>
<li id="cite_note-3"><span class="mw-cite-backlink"><b><a href="#cite_ref-3">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1067248974"/><cite id="CITEREFHuangLiuvan_der_MaatenQ._Weinberger2018" class="citation arxiv cs1">Huang, Gao; Liu, Zhuang; van der Maaten, Laurens; Q. Weinberger, Kilian (28 Jan 2018). "Densely Connected Convolutional Networks". <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="//arxiv.org/abs/1608.06993">1608.06993</a></span> [<a rel="nofollow" class="external text" href="//arxiv.org/archive/cs.CV">cs.CV</a>].</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=Densely+Connected+Convolutional+Networks&amp;rft.date=2018-01-28&amp;rft_id=info%3Aarxiv%2F1608.06993&amp;rft.aulast=Huang&amp;rft.aufirst=Gao&amp;rft.au=Liu%2C+Zhuang&amp;rft.au=van+der+Maaten%2C+Laurens&amp;rft.au=Q.+Weinberger%2C+Kilian&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AVision+transformer" class="Z3988"></span></span>
</li>
<li id="cite_note-4"><span class="mw-cite-backlink"><b><a href="#cite_ref-4">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1067248974"/><cite id="CITEREFVaswaniShazeerParmarUszkoreit2017" class="citation arxiv cs1">Vaswani, Ashish; Shazeer, Noam; Parmar, Niki; Uszkoreit, Jakob; Jones, Llion; Gomez, Aidan N.; Kaiser, Lukasz; Polosukhin, Illia (2017-12-05). "Attention Is All You Need". <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="//arxiv.org/abs/1706.03762">1706.03762</a></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=Attention+Is+All+You+Need&amp;rft.date=2017-12-05&amp;rft_id=info%3Aarxiv%2F1706.03762&amp;rft.aulast=Vaswani&amp;rft.aufirst=Ashish&amp;rft.au=Shazeer%2C+Noam&amp;rft.au=Parmar%2C+Niki&amp;rft.au=Uszkoreit%2C+Jakob&amp;rft.au=Jones%2C+Llion&amp;rft.au=Gomez%2C+Aidan+N.&amp;rft.au=Kaiser%2C+Lukasz&amp;rft.au=Polosukhin%2C+Illia&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AVision+transformer" class="Z3988"></span></span>
</li>
<li id="cite_note-5"><span class="mw-cite-backlink"><b><a href="#cite_ref-5">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1067248974"/><cite id="CITEREFDosovitskiyBeyerKolesnikovWeissenborn2021" class="citation arxiv cs1">Dosovitskiy, Alexey; Beyer, Lucas; Kolesnikov, Alexander; Weissenborn, Dirk; Zhai, Xiaohua; Unterthiner, Thomas; Dehghani, Mostafa; Minderer, Matthias; Heigold, Georg; Gelly, Sylvain; Uszkoreit, Jakob (2021-06-03). "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale". <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="//arxiv.org/abs/2010.11929">2010.11929</a></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=An+Image+is+Worth+16x16+Words%3A+Transformers+for+Image+Recognition+at+Scale&amp;rft.date=2021-06-03&amp;rft_id=info%3Aarxiv%2F2010.11929&amp;rft.aulast=Dosovitskiy&amp;rft.aufirst=Alexey&amp;rft.au=Beyer%2C+Lucas&amp;rft.au=Kolesnikov%2C+Alexander&amp;rft.au=Weissenborn%2C+Dirk&amp;rft.au=Zhai%2C+Xiaohua&amp;rft.au=Unterthiner%2C+Thomas&amp;rft.au=Dehghani%2C+Mostafa&amp;rft.au=Minderer%2C+Matthias&amp;rft.au=Heigold%2C+Georg&amp;rft.au=Gelly%2C+Sylvain&amp;rft.au=Uszkoreit%2C+Jakob&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AVision+transformer" class="Z3988"></span></span>
</li>
<li id="cite_note-:1-6"><span class="mw-cite-backlink">^ <a href="#cite_ref-:1_6-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-:1_6-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1067248974"/><cite id="CITEREFSynced2020" class="citation web cs1">Synced (2020-06-12). <a rel="nofollow" class="external text" href="https://medium.com/syncedreview/facebook-and-uc-berkeley-boost-cv-performance-and-lower-compute-cost-with-visual-transformers-c019823f0561">"Facebook and UC Berkeley Boost CV Performance and Lower Compute Cost With Visual Transformers"</a>. <i>Medium</i><span class="reference-accessdate">. Retrieved <span class="nowrap">2021-07-11</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Medium&amp;rft.atitle=Facebook+and+UC+Berkeley+Boost+CV+Performance+and+Lower+Compute+Cost+With+Visual+Transformers&amp;rft.date=2020-06-12&amp;rft.au=Synced&amp;rft_id=https%3A%2F%2Fmedium.com%2Fsyncedreview%2Ffacebook-and-uc-berkeley-boost-cv-performance-and-lower-compute-cost-with-visual-transformers-c019823f0561&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AVision+transformer" class="Z3988"></span></span>
</li>
<li id="cite_note-7"><span class="mw-cite-backlink"><b><a href="#cite_ref-7">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1067248974"/><cite id="CITEREFWuXuDaiWan2020" class="citation arxiv cs1">Wu, Bichen; Xu, Chenfeng; Dai, Xiaoliang; Wan, Alvin; Zhang, Peizhao; Yan, Zhicheng; Masayoshi, Tomizuka; Gonzalez, Joseph; Keutzer, Kurt; Vajda, Peter (2020). "Visual Transformers: Token-based Image Representation and Processing for Computer Vision". <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="//arxiv.org/abs/2006.03677">2006.03677</a></span> [<a rel="nofollow" class="external text" href="//arxiv.org/archive/cs.CV">cs.CV</a>].</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=Visual+Transformers%3A+Token-based+Image+Representation+and+Processing+for+Computer+Vision&amp;rft.date=2020&amp;rft_id=info%3Aarxiv%2F2006.03677&amp;rft.aulast=Wu&amp;rft.aufirst=Bichen&amp;rft.au=Xu%2C+Chenfeng&amp;rft.au=Dai%2C+Xiaoliang&amp;rft.au=Wan%2C+Alvin&amp;rft.au=Zhang%2C+Peizhao&amp;rft.au=Yan%2C+Zhicheng&amp;rft.au=Masayoshi%2C+Tomizuka&amp;rft.au=Gonzalez%2C+Joseph&amp;rft.au=Keutzer%2C+Kurt&amp;rft.au=Vajda%2C+Peter&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AVision+transformer" class="Z3988"></span></span>
</li>
<li id="cite_note-:2-8"><span class="mw-cite-backlink">^ <a href="#cite_ref-:2_8-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-:2_8-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1067248974"/><cite id="CITEREFXiaoSinghMintunDarrell2021" class="citation arxiv cs1">Xiao, Tete; Singh, Mannat; Mintun, Eric; Darrell, Trevor; Doll√°r, Piotr; Girshick, Ross (2021-06-28). "Early Convolutions Help Transformers See Better". <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="//arxiv.org/abs/2106.14881">2106.14881</a></span> [<a rel="nofollow" class="external text" href="//arxiv.org/archive/cs.CV">cs.CV</a>].</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=Early+Convolutions+Help+Transformers+See+Better&amp;rft.date=2021-06-28&amp;rft_id=info%3Aarxiv%2F2106.14881&amp;rft.aulast=Xiao&amp;rft.aufirst=Tete&amp;rft.au=Singh%2C+Mannat&amp;rft.au=Mintun%2C+Eric&amp;rft.au=Darrell%2C+Trevor&amp;rft.au=Doll%C3%A1r%2C+Piotr&amp;rft.au=Girshick%2C+Ross&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AVision+transformer" class="Z3988"></span></span>
</li>
<li id="cite_note-9"><span class="mw-cite-backlink"><b><a href="#cite_ref-9">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1067248974"/><cite id="CITEREFLiuLinCaoHu2021" class="citation arxiv cs1">Liu, Ze; Lin, Yutong; Cao, Yue; Hu, Han; Wei, Yixuan; Zhang, Zheng; Lin, Stephen; Guo, Baining (2021-03-25). "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows". <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="//arxiv.org/abs/2103.14030">2103.14030</a></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=Swin+Transformer%3A+Hierarchical+Vision+Transformer+using+Shifted+Windows&amp;rft.date=2021-03-25&amp;rft_id=info%3Aarxiv%2F2103.14030&amp;rft.aulast=Liu&amp;rft.aufirst=Ze&amp;rft.au=Lin%2C+Yutong&amp;rft.au=Cao%2C+Yue&amp;rft.au=Hu%2C+Han&amp;rft.au=Wei%2C+Yixuan&amp;rft.au=Zhang%2C+Zheng&amp;rft.au=Lin%2C+Stephen&amp;rft.au=Guo%2C+Baining&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AVision+transformer" class="Z3988"></span></span>
</li>
<li id="cite_note-10"><span class="mw-cite-backlink"><b><a href="#cite_ref-10">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1067248974"/><cite id="CITEREFBertasiusWangTorresani2021" class="citation arxiv cs1">Bertasius, Gedas; Wang, Heng; Torresani, Lorenzo (2021-02-09). "Is Space-Time Attention All You Need for Video Understanding?". <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="//arxiv.org/abs/2102.05095">2102.05095</a></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=Is+Space-Time+Attention+All+You+Need+for+Video+Understanding%3F&amp;rft.date=2021-02-09&amp;rft_id=info%3Aarxiv%2F2102.05095&amp;rft.aulast=Bertasius&amp;rft.aufirst=Gedas&amp;rft.au=Wang%2C+Heng&amp;rft.au=Torresani%2C+Lorenzo&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AVision+transformer" class="Z3988"></span></span>
</li>
<li id="cite_note-11"><span class="mw-cite-backlink"><b><a href="#cite_ref-11">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1067248974"/><cite id="CITEREFCoccomini2021" class="citation web cs1">Coccomini, Davide (2021-03-31). <span class="cs1-lock-subscription" title="Paid subscription required"><a rel="nofollow" class="external text" href="https://towardsdatascience.com/transformers-an-exciting-revolution-from-text-to-videos-dc70a15e617b">"On Transformers, TimeSformers, and Attention. An exciting revolution from text to videos"</a></span>. <i>Towards Data Science</i>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Towards+Data+Science&amp;rft.atitle=On+Transformers%2C+TimeSformers%2C+and+Attention.+An+exciting+revolution+from+text+to+videos&amp;rft.date=2021-03-31&amp;rft.aulast=Coccomini&amp;rft.aufirst=Davide&amp;rft_id=https%3A%2F%2Ftowardsdatascience.com%2Ftransformers-an-exciting-revolution-from-text-to-videos-dc70a15e617b&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AVision+transformer" class="Z3988"></span></span>
</li>
<li id="cite_note-12"><span class="mw-cite-backlink"><b><a href="#cite_ref-12">^</a></b></span> <span class="reference-text"><a rel="nofollow" class="external text" href="https://www.youtube.com/watch?v=j0z4FweCy4M"><span class="plainlinks">Tesla AI Day</span></a> on <a href="/wiki/YouTube" title="YouTube">YouTube</a></span>
</li>
<li id="cite_note-13"><span class="mw-cite-backlink"><b><a href="#cite_ref-13">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1067248974"/><cite id="CITEREFRaghuUnterthinerKornblithZhang2021" class="citation arxiv cs1">Raghu, Maithra; Unterthiner, Thomas; Kornblith, Simon; Zhang, Chiyuan; Dosovitskiy, Alexey (2021-08-19). "Do Vision Transformers See Like Convolutional Neural Networks?". <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="//arxiv.org/abs/2108.08810">2108.08810</a></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=Do+Vision+Transformers+See+Like+Convolutional+Neural+Networks%3F&amp;rft.date=2021-08-19&amp;rft_id=info%3Aarxiv%2F2108.08810&amp;rft.aulast=Raghu&amp;rft.aufirst=Maithra&amp;rft.au=Unterthiner%2C+Thomas&amp;rft.au=Kornblith%2C+Simon&amp;rft.au=Zhang%2C+Chiyuan&amp;rft.au=Dosovitskiy%2C+Alexey&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AVision+transformer" class="Z3988"></span></span>
</li>
<li id="cite_note-14"><span class="mw-cite-backlink"><b><a href="#cite_ref-14">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1067248974"/><cite id="CITEREFCoccomini2021" class="citation web cs1">Coccomini, Davide (2021-07-24). <span class="cs1-lock-subscription" title="Paid subscription required"><a rel="nofollow" class="external text" href="https://towardsdatascience.com/vision-transformers-or-convolutional-neural-networks-both-de1a2c3c62e4">"Vision Transformers or Convolutional Neural Networks? Both!"</a></span>. <i>Towards Data Science</i>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Towards+Data+Science&amp;rft.atitle=Vision+Transformers+or+Convolutional+Neural+Networks%3F+Both%21&amp;rft.date=2021-07-24&amp;rft.aulast=Coccomini&amp;rft.aufirst=Davide&amp;rft_id=https%3A%2F%2Ftowardsdatascience.com%2Fvision-transformers-or-convolutional-neural-networks-both-de1a2c3c62e4&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AVision+transformer" class="Z3988"></span></span>
</li>
<li id="cite_note-15"><span class="mw-cite-backlink"><b><a href="#cite_ref-15">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1067248974"/><cite id="CITEREFNaseerRanasingheKhanHayat2021" class="citation arxiv cs1">Naseer, Muzammal; Ranasinghe, Kanchana; Khan, Salman; Hayat, Munawar; Khan, Fahad Shahbaz; Yang, Ming-Hsuan (2021-05-21). "Intriguing Properties of Vision Transformers". <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="//arxiv.org/abs/2105.10497">2105.10497</a></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=Intriguing+Properties+of+Vision+Transformers&amp;rft.date=2021-05-21&amp;rft_id=info%3Aarxiv%2F2105.10497&amp;rft.aulast=Naseer&amp;rft.aufirst=Muzammal&amp;rft.au=Ranasinghe%2C+Kanchana&amp;rft.au=Khan%2C+Salman&amp;rft.au=Hayat%2C+Munawar&amp;rft.au=Khan%2C+Fahad+Shahbaz&amp;rft.au=Yang%2C+Ming-Hsuan&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AVision+transformer" class="Z3988"></span></span>
</li>
<li id="cite_note-16"><span class="mw-cite-backlink"><b><a href="#cite_ref-16">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1067248974"/><cite id="CITEREFDaiLiuLeTan2021" class="citation arxiv cs1">Dai, Zihang; Liu, Hanxiao; Le, Quoc V.; Tan, Mingxing (2021-06-09). "CoAtNet: Marrying Convolution and Attention for All Data Sizes". <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="//arxiv.org/abs/2106.04803">2106.04803</a></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=CoAtNet%3A+Marrying+Convolution+and+Attention+for+All+Data+Sizes&amp;rft.date=2021-06-09&amp;rft_id=info%3Aarxiv%2F2106.04803&amp;rft.aulast=Dai&amp;rft.aufirst=Zihang&amp;rft.au=Liu%2C+Hanxiao&amp;rft.au=Le%2C+Quoc+V.&amp;rft.au=Tan%2C+Mingxing&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AVision+transformer" class="Z3988"></span></span>
</li>
<li id="cite_note-17"><span class="mw-cite-backlink"><b><a href="#cite_ref-17">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1067248974"/><cite id="CITEREFWuXiaoCodellaLiu2021" class="citation arxiv cs1">Wu, Haiping; Xiao, Bin; Codella, Noel; Liu, Mengchen; Dai, Xiyang; Yuan, Lu; Zhang, Lei (2021-03-29). "CvT: Introducing Convolutions to Vision Transformers". <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="//arxiv.org/abs/2103.15808">2103.15808</a></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=CvT%3A+Introducing+Convolutions+to+Vision+Transformers&amp;rft.date=2021-03-29&amp;rft_id=info%3Aarxiv%2F2103.15808&amp;rft.aulast=Wu&amp;rft.aufirst=Haiping&amp;rft.au=Xiao%2C+Bin&amp;rft.au=Codella%2C+Noel&amp;rft.au=Liu%2C+Mengchen&amp;rft.au=Dai%2C+Xiyang&amp;rft.au=Yuan%2C+Lu&amp;rft.au=Zhang%2C+Lei&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AVision+transformer" class="Z3988"></span></span>
</li>
<li id="cite_note-18"><span class="mw-cite-backlink"><b><a href="#cite_ref-18">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1067248974"/><cite id="CITEREFCoccominiMessinaGennaroFalchi2021" class="citation arxiv cs1">Coccomini, Davide; Messina, Nicola; Gennaro, Claudio; Falchi, Fabrizio (2021-07-06). "Combining EfficientNet and Vision Transformers for Video Deepfake Detection". <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="//arxiv.org/abs/2107.02612">2107.02612</a></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=Combining+EfficientNet+and+Vision+Transformers+for+Video+Deepfake+Detection&amp;rft.date=2021-07-06&amp;rft_id=info%3Aarxiv%2F2107.02612&amp;rft.aulast=Coccomini&amp;rft.aufirst=Davide&amp;rft.au=Messina%2C+Nicola&amp;rft.au=Gennaro%2C+Claudio&amp;rft.au=Falchi%2C+Fabrizio&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AVision+transformer" class="Z3988"></span></span>
</li>
<li id="cite_note-19"><span class="mw-cite-backlink"><b><a href="#cite_ref-19">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1067248974"/><cite id="CITEREFCoccomini2021" class="citation web cs1">Coccomini, Davide (2021-07-24). <span class="cs1-lock-subscription" title="Paid subscription required"><a rel="nofollow" class="external text" href="https://towardsdatascience.com/self-supervised-learning-in-vision-transformers-30ff9be928c">"Self-Supervised Learning in Vision Transformers"</a></span>. <i>Towards Data Science</i>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Towards+Data+Science&amp;rft.atitle=Self-Supervised+Learning+in+Vision+Transformers&amp;rft.date=2021-07-24&amp;rft.aulast=Coccomini&amp;rft.aufirst=Davide&amp;rft_id=https%3A%2F%2Ftowardsdatascience.com%2Fself-supervised-learning-in-vision-transformers-30ff9be928c&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AVision+transformer" class="Z3988"></span></span>
</li>
<li id="cite_note-20"><span class="mw-cite-backlink"><b><a href="#cite_ref-20">^</a></b></span> <span class="reference-text"><a rel="nofollow" class="external text" href="https://github.com/lucidrains/vit-pytorch">vit-pytorch</a> on <a href="/wiki/GitHub" title="GitHub">GitHub</a></span>
</li>
<li id="cite_note-21"><span class="mw-cite-backlink"><b><a href="#cite_ref-21">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1067248974"/><cite id="CITEREFSalama2021" class="citation web cs1">Salama, Khalid (2021-01-18). <a rel="nofollow" class="external text" href="https://keras.io/examples/vision/image_classification_with_vision_transformer/">"Image classification with Vision Transformer"</a>. <i>keras.io</i>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=keras.io&amp;rft.atitle=Image+classification+with+Vision+Transformer&amp;rft.date=2021-01-18&amp;rft.aulast=Salama&amp;rft.aufirst=Khalid&amp;rft_id=https%3A%2F%2Fkeras.io%2Fexamples%2Fvision%2Fimage_classification_with_vision_transformer%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AVision+transformer" class="Z3988"></span></span>
</li>
</ol></div></div>
<h2><span class="mw-headline" id="External_links">External links</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Vision_transformer&amp;action=edit&amp;section=9" title="Edit section: External links">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<ul><li><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1067248974"/><cite id="CITEREFIgarashi2021" class="citation web cs1">Igarashi, Yoshiyuki (2021-02-04). <span class="cs1-lock-subscription" title="Paid subscription required"><a rel="nofollow" class="external text" href="https://towardsdatascience.com/are-you-ready-for-vision-transformer-vit-c9e11862c539">"Are You Ready for Vision Transformer (ViT)?"</a></span>. <i>Medium</i><span class="reference-accessdate">. Retrieved <span class="nowrap">2021-07-11</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Medium&amp;rft.atitle=Are+You+Ready+for+Vision+Transformer+%28ViT%29%3F&amp;rft.date=2021-02-04&amp;rft.aulast=Igarashi&amp;rft.aufirst=Yoshiyuki&amp;rft_id=https%3A%2F%2Ftowardsdatascience.com%2Fare-you-ready-for-vision-transformer-vit-c9e11862c539&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AVision+transformer" class="Z3988"></span></li>
<li><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1067248974"/><cite id="CITEREFCoccomini2021" class="citation web cs1">Coccomini, Davide (2021-05-03). <span class="cs1-lock-subscription" title="Paid subscription required"><a rel="nofollow" class="external text" href="https://towardsdatascience.com/on-dino-self-distillation-with-no-labels-c29e9365e382">"On DINO, Self-Distillation with no labels"</a></span>. <i>Towards Data Science</i><span class="reference-accessdate">. Retrieved <span class="nowrap">2021-10-03</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Towards+Data+Science&amp;rft.atitle=On+DINO%2C+Self-Distillation+with+no+labels&amp;rft.date=2021-05-03&amp;rft.aulast=Coccomini&amp;rft.aufirst=Davide&amp;rft_id=https%3A%2F%2Ftowardsdatascience.com%2Fon-dino-self-distillation-with-no-labels-c29e9365e382&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AVision+transformer" class="Z3988"></span></li></ul>
<!-- 
NewPP limit report
Parsed by mw1326
Cached time: 20220222171157
Cache expiry: 1814400
Reduced expiry: false
Complications: [vary‚Äêrevision‚Äêsha1]
CPU time usage: 0.462 seconds
Real time usage: 0.498 seconds
Preprocessor visited node count: 1578/1000000
Post‚Äêexpand include size: 51258/2097152 bytes
Template argument size: 1172/2097152 bytes
Highest expansion depth: 19/100
Expensive parser function count: 0/500
Unstrip recursion depth: 1/20
Unstrip post‚Äêexpand size: 63427/5000000 bytes
Lua time usage: 0.267/10.000 seconds
Lua memory usage: 5106632/52428800 bytes
Number of Wikibase entities loaded: 0/400
-->
<!--
Transclusion expansion time report (%,ms,calls,template)
100.00%  425.621      1 -total
 93.91%  399.720      1 Template:Reflist
 41.14%  175.084      8 Template:Cite_web
 30.89%  131.457     13 Template:Cite_arXiv
  8.22%   35.002      1 Template:YouTube
  4.79%   20.385      1 Template:Replace
  3.11%   13.223      1 Template:Delink
  2.53%   10.768      1 Template:GitHub
  1.49%    6.337      1 Template:If_empty
  0.68%    2.911      2 Template:Main_other
-->

<!-- Saved in parser cache with key enwiki:pcache:idhash:68212199-0!canonical and timestamp 20220222171156 and revision id 1073424596. Serialized with JSON.
 -->
</div><noscript><img src="//en.wikipedia.org/wiki/Special:CentralAutoLogin/start?type=1x1" alt="" title="" width="1" height="1" style="border: none; position: absolute;" /></noscript>
<div class="printfooter">Retrieved from "<a dir="ltr" href="https://en.wikipedia.org/w/index.php?title=Vision_transformer&amp;oldid=1073424596">https://en.wikipedia.org/w/index.php?title=Vision_transformer&amp;oldid=1073424596</a>"</div></div>
		<div id="catlinks" class="catlinks" data-mw="interface"><div id="mw-normal-catlinks" class="mw-normal-catlinks"><a href="/wiki/Help:Category" title="Help:Category">Categories</a>: <ul><li><a href="/wiki/Category:Neural_networks" title="Category:Neural networks">Neural networks</a></li><li><a href="/wiki/Category:Image_processing" title="Category:Image processing">Image processing</a></li></ul></div></div>
	</div>
</div>
<div id='mw-data-after-content'>
	<div class="read-more-container"></div>
</div>

<div id="mw-navigation">
	<h2>Navigation menu</h2>
	<div id="mw-head">
		
<nav id="p-personal" class="mw-portlet mw-portlet-personal vector-user-menu-legacy vector-menu" aria-labelledby="p-personal-label" role="navigation" 
	 >
	<label id="p-personal-label" aria-label="" class="vector-menu-heading">
		<span class="vector-menu-heading-label">Personal tools</span>
	</label>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list"><li id="pt-anonuserpage" class="mw-list-item"><span>Not logged in</span></li><li id="pt-anontalk" class="mw-list-item"><a href="/wiki/Special:MyTalk" title="Discussion about edits from this IP address [n]" accesskey="n"><span>Talk</span></a></li><li id="pt-anoncontribs" class="mw-list-item"><a href="/wiki/Special:MyContributions" title="A list of edits made from this IP address [y]" accesskey="y"><span>Contributions</span></a></li><li id="pt-createaccount" class="mw-list-item"><a href="/w/index.php?title=Special:CreateAccount&amp;returnto=Vision+transformer&amp;returntoquery=lxml%3D" title="You are encouraged to create an account and log in; however, it is not mandatory"><span>Create account</span></a></li><li id="pt-login" class="mw-list-item"><a href="/w/index.php?title=Special:UserLogin&amp;returnto=Vision+transformer&amp;returntoquery=lxml%3D" title="You&#039;re encouraged to log in; however, it&#039;s not mandatory. [o]" accesskey="o"><span>Log in</span></a></li></ul>
		
	</div>
</nav>

		<div id="left-navigation">
			
<nav id="p-namespaces" class="mw-portlet mw-portlet-namespaces vector-menu vector-menu-tabs" aria-labelledby="p-namespaces-label" role="navigation" 
	 >
	<label id="p-namespaces-label" aria-label="" class="vector-menu-heading">
		<span class="vector-menu-heading-label">Namespaces</span>
	</label>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list"><li id="ca-nstab-main" class="selected mw-list-item"><a href="/wiki/Vision_transformer" title="View the content page [c]" accesskey="c"><span>Article</span></a></li><li id="ca-talk" class="mw-list-item"><a href="/wiki/Talk:Vision_transformer" rel="discussion" title="Discuss improvements to the content page [t]" accesskey="t"><span>Talk</span></a></li></ul>
		
	</div>
</nav>

			
<nav id="p-variants" class="mw-portlet mw-portlet-variants emptyPortlet vector-menu-dropdown-noicon vector-menu vector-menu-dropdown" aria-labelledby="p-variants-label" role="navigation" 
	 >
	<input type="checkbox"
		id="p-variants-checkbox"
		role="button"
		aria-haspopup="true"
		data-event-name="ui.dropdown-p-variants"
		class="vector-menu-checkbox" aria-labelledby="p-variants-label" />
	<label id="p-variants-label" aria-label="Change language variant" class="vector-menu-heading">
		<span class="vector-menu-heading-label">English</span>
			<span class="vector-menu-checkbox-expanded">expanded</span>
			<span class="vector-menu-checkbox-collapsed">collapsed</span>
	</label>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list"></ul>
		
	</div>
</nav>

		</div>
		<div id="right-navigation">
			
<nav id="p-views" class="mw-portlet mw-portlet-views vector-menu vector-menu-tabs" aria-labelledby="p-views-label" role="navigation" 
	 >
	<label id="p-views-label" aria-label="" class="vector-menu-heading">
		<span class="vector-menu-heading-label">Views</span>
	</label>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list"><li id="ca-view" class="selected mw-list-item"><a href="/wiki/Vision_transformer"><span>Read</span></a></li><li id="ca-edit" class="mw-list-item"><a href="/w/index.php?title=Vision_transformer&amp;action=edit" title="Edit this page [e]" accesskey="e"><span>Edit</span></a></li><li id="ca-history" class="mw-list-item"><a href="/w/index.php?title=Vision_transformer&amp;action=history" title="Past revisions of this page [h]" accesskey="h"><span>View history</span></a></li></ul>
		
	</div>
</nav>

			
<nav id="p-cactions" class="mw-portlet mw-portlet-cactions emptyPortlet vector-menu-dropdown-noicon vector-menu vector-menu-dropdown" aria-labelledby="p-cactions-label" role="navigation"  title="More options"
	 >
	<input type="checkbox"
		id="p-cactions-checkbox"
		role="button"
		aria-haspopup="true"
		data-event-name="ui.dropdown-p-cactions"
		class="vector-menu-checkbox" aria-labelledby="p-cactions-label" />
	<label id="p-cactions-label" aria-label="" class="vector-menu-heading">
		<span class="vector-menu-heading-label">More</span>
			<span class="vector-menu-checkbox-expanded">expanded</span>
			<span class="vector-menu-checkbox-collapsed">collapsed</span>
	</label>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list"></ul>
		
	</div>
</nav>

			
<div id="p-search" role="search" class="vector-search-box-vue  vector-search-box-show-thumbnail vector-search-box-auto-expand-width vector-search-box">
	<div>
			<h3 >
				<label for="searchInput">Search</label>
			</h3>
		<form action="/w/index.php" id="searchform"
			class="vector-search-box-form">
			<div id="simpleSearch"
				class="vector-search-box-inner"
				 data-search-loc="header-navigation">
				<input class="vector-search-box-input"
					 type="search" name="search" placeholder="Search Wikipedia" aria-label="Search Wikipedia" autocapitalize="sentences" title="Search Wikipedia [f]" accesskey="f" id="searchInput"
				/>
				<input type="hidden" name="title" value="Special:Search"/>
				<input id="mw-searchButton"
					 class="searchButton mw-fallbackSearchButton" type="submit" name="fulltext" title="Search Wikipedia for this text" value="Search" />
				<input id="searchButton"
					 class="searchButton" type="submit" name="go" title="Go to a page with this exact name if it exists" value="Go" />
			</div>
		</form>
	</div>
</div>

		</div>
	</div>
	

<div id="mw-panel">
	<div id="p-logo" role="banner">
		<a class="mw-wiki-logo" href="/wiki/Main_Page"
			title="Visit the main page"></a>
	</div>
	
<nav id="p-navigation" class="mw-portlet mw-portlet-navigation vector-menu vector-menu-portal portal" aria-labelledby="p-navigation-label" role="navigation" 
	 >
	<label id="p-navigation-label" aria-label="" class="vector-menu-heading">
		<span class="vector-menu-heading-label">Navigation</span>
	</label>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list"><li id="n-mainpage-description" class="mw-list-item"><a href="/wiki/Main_Page" icon="home" title="Visit the main page [z]" accesskey="z"><span>Main page</span></a></li><li id="n-contents" class="mw-list-item"><a href="/wiki/Wikipedia:Contents" title="Guides to browsing Wikipedia"><span>Contents</span></a></li><li id="n-currentevents" class="mw-list-item"><a href="/wiki/Portal:Current_events" title="Articles related to current events"><span>Current events</span></a></li><li id="n-randompage" class="mw-list-item"><a href="/wiki/Special:Random" icon="die" title="Visit a randomly selected article [x]" accesskey="x"><span>Random article</span></a></li><li id="n-aboutsite" class="mw-list-item"><a href="/wiki/Wikipedia:About" title="Learn about Wikipedia and how it works"><span>About Wikipedia</span></a></li><li id="n-contactpage" class="mw-list-item"><a href="//en.wikipedia.org/wiki/Wikipedia:Contact_us" title="How to contact Wikipedia"><span>Contact us</span></a></li><li id="n-sitesupport" class="mw-list-item"><a href="https://donate.wikimedia.org/wiki/Special:FundraiserRedirector?utm_source=donate&amp;utm_medium=sidebar&amp;utm_campaign=C13_en.wikipedia.org&amp;uselang=en" title="Support us by donating to the Wikimedia Foundation"><span>Donate</span></a></li></ul>
		
	</div>
</nav>

	
<nav id="p-interaction" class="mw-portlet mw-portlet-interaction vector-menu vector-menu-portal portal" aria-labelledby="p-interaction-label" role="navigation" 
	 >
	<label id="p-interaction-label" aria-label="" class="vector-menu-heading">
		<span class="vector-menu-heading-label">Contribute</span>
	</label>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list"><li id="n-help" class="mw-list-item"><a href="/wiki/Help:Contents" icon="help" title="Guidance on how to use and edit Wikipedia"><span>Help</span></a></li><li id="n-introduction" class="mw-list-item"><a href="/wiki/Help:Introduction" title="Learn how to edit Wikipedia"><span>Learn to edit</span></a></li><li id="n-portal" class="mw-list-item"><a href="/wiki/Wikipedia:Community_portal" title="The hub for editors"><span>Community portal</span></a></li><li id="n-recentchanges" class="mw-list-item"><a href="/wiki/Special:RecentChanges" icon="recentChanges" title="A list of recent changes to Wikipedia [r]" accesskey="r"><span>Recent changes</span></a></li><li id="n-upload" class="mw-list-item"><a href="/wiki/Wikipedia:File_Upload_Wizard" title="Add images or other media for use on Wikipedia"><span>Upload file</span></a></li></ul>
		
	</div>
</nav>

<nav id="p-tb" class="mw-portlet mw-portlet-tb vector-menu vector-menu-portal portal" aria-labelledby="p-tb-label" role="navigation" 
	 >
	<label id="p-tb-label" aria-label="" class="vector-menu-heading">
		<span class="vector-menu-heading-label">Tools</span>
	</label>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list"><li id="t-whatlinkshere" class="mw-list-item"><a href="/wiki/Special:WhatLinksHere/Vision_transformer" title="List of all English Wikipedia pages containing links to this page [j]" accesskey="j"><span>What links here</span></a></li><li id="t-recentchangeslinked" class="mw-list-item"><a href="/wiki/Special:RecentChangesLinked/Vision_transformer" rel="nofollow" title="Recent changes in pages linked from this page [k]" accesskey="k"><span>Related changes</span></a></li><li id="t-upload" class="mw-list-item"><a href="/wiki/Wikipedia:File_Upload_Wizard" title="Upload files [u]" accesskey="u"><span>Upload file</span></a></li><li id="t-specialpages" class="mw-list-item"><a href="/wiki/Special:SpecialPages" title="A list of all special pages [q]" accesskey="q"><span>Special pages</span></a></li><li id="t-permalink" class="mw-list-item"><a href="/w/index.php?title=Vision_transformer&amp;oldid=1073424596" title="Permanent link to this revision of this page"><span>Permanent link</span></a></li><li id="t-info" class="mw-list-item"><a href="/w/index.php?title=Vision_transformer&amp;action=info" title="More information about this page"><span>Page information</span></a></li><li id="t-cite" class="mw-list-item"><a href="/w/index.php?title=Special:CiteThisPage&amp;page=Vision_transformer&amp;id=1073424596&amp;wpFormIdentifier=titleform" title="Information on how to cite this page"><span>Cite this page</span></a></li><li id="t-wikibase" class="mw-list-item"><a href="https://www.wikidata.org/wiki/Special:EntityPage/Q107675654" title="Structured data on this page hosted by Wikidata [g]" accesskey="g"><span>Wikidata item</span></a></li></ul>
		
	</div>
</nav>

<nav id="p-coll-print_export" class="mw-portlet mw-portlet-coll-print_export vector-menu vector-menu-portal portal" aria-labelledby="p-coll-print_export-label" role="navigation" 
	 >
	<label id="p-coll-print_export-label" aria-label="" class="vector-menu-heading">
		<span class="vector-menu-heading-label">Print/export</span>
	</label>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list"><li id="coll-download-as-rl" class="mw-list-item"><a href="/w/index.php?title=Special:DownloadAsPdf&amp;page=Vision_transformer&amp;action=show-download-screen" title="Download this page as a PDF file"><span>Download as PDF</span></a></li><li id="t-print" class="mw-list-item"><a href="/w/index.php?title=Vision_transformer&amp;printable=yes" title="Printable version of this page [p]" accesskey="p"><span>Printable version</span></a></li></ul>
		
	</div>
</nav>

	
<nav id="p-lang" class="mw-portlet mw-portlet-lang vector-menu vector-menu-portal portal" aria-labelledby="p-lang-label" role="navigation" 
	 >
	<label id="p-lang-label" aria-label="" class="vector-menu-heading">
		<span class="vector-menu-heading-label">Languages</span>
	</label>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list"><li class="interlanguage-link interwiki-ko mw-list-item"><a href="https://ko.wikipedia.org/wiki/%EB%B9%84%EC%A0%84_%EB%B3%80%ED%99%98%EA%B8%B0" title="ÎπÑÏ†Ñ Î≥ÄÌôòÍ∏∞ ‚Äì Korean" lang="ko" hreflang="ko" class="interlanguage-link-target"><span>ÌïúÍµ≠Ïñ¥</span></a></li><li class="interlanguage-link interwiki-uk mw-list-item"><a href="https://uk.wikipedia.org/wiki/%D0%97%D0%BE%D1%80%D0%BE%D0%B2%D0%B8%D0%B9_%D1%82%D1%80%D0%B0%D0%BD%D1%81%D1%84%D0%BE%D1%80%D0%BC%D0%B5%D1%80" title="–ó–æ—Ä–æ–≤–∏–π —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä ‚Äì Ukrainian" lang="uk" hreflang="uk" class="interlanguage-link-target"><span>–£–∫—Ä–∞—ó–Ω—Å—å–∫–∞</span></a></li></ul>
		<div class="after-portlet after-portlet-lang"><span class="wb-langlinks-edit wb-langlinks-link"><a href="https://www.wikidata.org/wiki/Special:EntityPage/Q107675654#sitelinks-wikipedia" title="Edit interlanguage links" class="wbc-editpage">Edit links</a></span></div>
	</div>
</nav>

</div>

</div>

<footer id="footer" class="mw-footer" role="contentinfo" >
	<ul id="footer-info">
	<li id="footer-info-lastmod"> This page was last edited on 22 February 2022, at 17:11<span class="anonymous-show">&#160;(UTC)</span>.</li>
	<li id="footer-info-copyright">Text is available under the <a rel="license" href="//en.wikipedia.org/wiki/Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License">Creative Commons Attribution-ShareAlike License 3.0</a><a rel="license" href="//creativecommons.org/licenses/by-sa/3.0/" style="display:none;"></a>;
additional terms may apply.  By using this site, you agree to the <a href="//foundation.wikimedia.org/wiki/Terms_of_Use">Terms of Use</a> and <a href="//foundation.wikimedia.org/wiki/Privacy_policy">Privacy Policy</a>. Wikipedia¬Æ is a registered trademark of the <a href="//www.wikimediafoundation.org/">Wikimedia Foundation, Inc.</a>, a non-profit organization.</li>
</ul>

	<ul id="footer-places">
	<li id="footer-places-privacy"><a href="https://foundation.wikimedia.org/wiki/Privacy_policy" class="extiw" title="wmf:Privacy policy">Privacy policy</a></li>
	<li id="footer-places-about"><a href="/wiki/Wikipedia:About" title="Wikipedia:About">About Wikipedia</a></li>
	<li id="footer-places-disclaimer"><a href="/wiki/Wikipedia:General_disclaimer" title="Wikipedia:General disclaimer">Disclaimers</a></li>
	<li id="footer-places-contact"><a href="//en.wikipedia.org/wiki/Wikipedia:Contact_us">Contact Wikipedia</a></li>
	<li id="footer-places-mobileview"><a href="//en.m.wikipedia.org/w/index.php?title=Vision_transformer&amp;lxml=&amp;mobileaction=toggle_view_mobile" class="noprint stopMobileRedirectToggle">Mobile view</a></li>
	<li id="footer-places-developers"><a href="https://www.mediawiki.org/wiki/Special:MyLanguage/How_to_contribute">Developers</a></li>
	<li id="footer-places-statslink"><a href="https://stats.wikimedia.org/#/en.wikipedia.org">Statistics</a></li>
	<li id="footer-places-cookiestatement"><a href="https://foundation.wikimedia.org/wiki/Cookie_statement">Cookie statement</a></li>
</ul>

	<ul id="footer-icons" class="noprint">
	<li id="footer-copyrightico"><a href="https://wikimediafoundation.org/"><img src="/static/images/footer/wikimedia-button.png" srcset="/static/images/footer/wikimedia-button-1.5x.png 1.5x, /static/images/footer/wikimedia-button-2x.png 2x" width="88" height="31" alt="Wikimedia Foundation" loading="lazy" /></a></li>
	<li id="footer-poweredbyico"><a href="https://www.mediawiki.org/"><img src="/static/images/footer/poweredby_mediawiki_88x31.png" alt="Powered by MediaWiki" srcset="/static/images/footer/poweredby_mediawiki_132x47.png 1.5x, /static/images/footer/poweredby_mediawiki_176x62.png 2x" width="88" height="31" loading="lazy"/></a></li>
</ul>

</footer>

<script>(RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgPageParseReport":{"limitreport":{"cputime":"0.462","walltime":"0.498","ppvisitednodes":{"value":1578,"limit":1000000},"postexpandincludesize":{"value":51258,"limit":2097152},"templateargumentsize":{"value":1172,"limit":2097152},"expansiondepth":{"value":19,"limit":100},"expensivefunctioncount":{"value":0,"limit":500},"unstrip-depth":{"value":1,"limit":20},"unstrip-size":{"value":63427,"limit":5000000},"entityaccesscount":{"value":0,"limit":400},"timingprofile":["100.00%  425.621      1 -total"," 93.91%  399.720      1 Template:Reflist"," 41.14%  175.084      8 Template:Cite_web"," 30.89%  131.457     13 Template:Cite_arXiv","  8.22%   35.002      1 Template:YouTube","  4.79%   20.385      1 Template:Replace","  3.11%   13.223      1 Template:Delink","  2.53%   10.768      1 Template:GitHub","  1.49%    6.337      1 Template:If_empty","  0.68%    2.911      2 Template:Main_other"]},"scribunto":{"limitreport-timeusage":{"value":"0.267","limit":"10.000"},"limitreport-memusage":{"value":5106632,"limit":52428800}},"cachereport":{"origin":"mw1326","timestamp":"20220222171157","ttl":1814400,"transientcontent":false}}});});</script>
<script type="application/ld+json">{"@context":"https:\/\/schema.org","@type":"Article","name":"Vision transformer","url":"https:\/\/en.wikipedia.org\/wiki\/Vision_transformer","sameAs":"http:\/\/www.wikidata.org\/entity\/Q107675654","mainEntity":"http:\/\/www.wikidata.org\/entity\/Q107675654","author":{"@type":"Organization","name":"Contributors to Wikimedia projects"},"publisher":{"@type":"Organization","name":"Wikimedia Foundation, Inc.","logo":{"@type":"ImageObject","url":"https:\/\/www.wikimedia.org\/static\/images\/wmf-hor-googpub.png"}},"datePublished":"2021-07-11T18:52:09Z","dateModified":"2022-02-22T17:11:56Z"}</script>
<script>(RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgBackendResponseTime":148,"wgHostname":"mw1320"});});</script>
</body>
</html>