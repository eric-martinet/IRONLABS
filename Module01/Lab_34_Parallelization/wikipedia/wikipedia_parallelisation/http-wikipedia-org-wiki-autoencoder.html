<!DOCTYPE html>
<html class="client-nojs" lang="en" dir="ltr">
<head>
<meta charset="UTF-8"/>
<title>Autoencoder - Wikipedia</title>
<script>document.documentElement.className="client-js";RLCONF={"wgBreakFrames":false,"wgSeparatorTransformTable":["",""],"wgDigitTransformTable":["",""],"wgDefaultDateFormat":"dmy","wgMonthNames":["","January","February","March","April","May","June","July","August","September","October","November","December"],"wgRequestId":"0f8e70ca-81c4-4334-86d0-ec691b23d6b0","wgCSPNonce":false,"wgCanonicalNamespace":"","wgCanonicalSpecialPageName":false,"wgNamespaceNumber":0,"wgPageName":"Autoencoder","wgTitle":"Autoencoder","wgCurRevisionId":1068573765,"wgRevisionId":1068573765,"wgArticleId":6836612,"wgIsArticle":true,"wgIsRedirect":false,"wgAction":"view","wgUserName":null,"wgUserGroups":["*"],"wgCategories":["CS1 errors: missing periodical","Articles with short description","Short description matches Wikidata","Use dmy dates from March 2020","Artificial neural networks","Unsupervised learning","Dimension reduction"],"wgPageContentLanguage":"en","wgPageContentModel":"wikitext","wgRelevantPageName":
"Autoencoder","wgRelevantArticleId":6836612,"wgIsProbablyEditable":true,"wgRelevantPageIsProbablyEditable":true,"wgRestrictionEdit":[],"wgRestrictionMove":[],"wgFlaggedRevsParams":{"tags":{"status":{"levels":-1}}},"wgVisualEditor":{"pageLanguageCode":"en","pageLanguageDir":"ltr","pageVariantFallbacks":"en"},"wgMFDisplayWikibaseDescriptions":{"search":true,"nearby":true,"watchlist":true,"tagline":false},"wgWMESchemaEditAttemptStepOversample":false,"wgWMEPageLength":40000,"wgNoticeProject":"wikipedia","wgMediaViewerOnClick":true,"wgMediaViewerEnabledByDefault":true,"wgPopupsFlags":10,"wgULSCurrentAutonym":"English","wgEditSubmitButtonLabelPublish":true,"wgCentralAuthMobileDomain":false,"wgULSPosition":"interlanguage","wgULSisCompactLinksEnabled":true,"wgWikibaseItemId":"Q786435","wgGENewcomerTasksGuidanceEnabled":true,"wgGEAskQuestionEnabled":false,"wgGELinkRecommendationsFrontendEnabled":false};RLSTATE={"ext.globalCssJs.user.styles":"ready","site.styles":"ready","user.styles":"ready",
"ext.globalCssJs.user":"ready","user":"ready","user.options":"loading","ext.cite.styles":"ready","ext.math.styles":"ready","skins.vector.styles.legacy":"ready","jquery.makeCollapsible.styles":"ready","ext.visualEditor.desktopArticleTarget.noscript":"ready","ext.wikimediaBadges":"ready","ext.uls.interlanguage":"ready","wikibase.client.init":"ready"};RLPAGEMODULES=["ext.cite.ux-enhancements","ext.math.scripts","site","mediawiki.page.ready","jquery.makeCollapsible","mediawiki.toc","skins.vector.legacy.js","ext.gadget.ReferenceTooltips","ext.gadget.charinsert","ext.gadget.extra-toolbar-buttons","ext.gadget.refToolbar","ext.gadget.switcher","mmv.head","mmv.bootstrap.autostart","ext.visualEditor.desktopArticleTarget.init","ext.visualEditor.targetLoader","ext.eventLogging","ext.wikimediaEvents","ext.navigationTiming","ext.cx.eventlogging.campaigns","ext.centralNotice.geoIP","ext.centralNotice.startUp","ext.centralauth.centralautologin","ext.popups","ext.uls.compactlinks","ext.uls.interface",
"ext.growthExperiments.SuggestedEditSession"];</script>
<script>(RLQ=window.RLQ||[]).push(function(){mw.loader.implement("user.options@1i9g4",function($,jQuery,require,module){mw.user.tokens.set({"patrolToken":"+\\","watchToken":"+\\","csrfToken":"+\\"});});});</script>
<link rel="stylesheet" href="/w/load.php?lang=en&amp;modules=ext.cite.styles%7Cext.math.styles%7Cext.uls.interlanguage%7Cext.visualEditor.desktopArticleTarget.noscript%7Cext.wikimediaBadges%7Cjquery.makeCollapsible.styles%7Cskins.vector.styles.legacy%7Cwikibase.client.init&amp;only=styles&amp;skin=vector"/>
<script async="" src="/w/load.php?lang=en&amp;modules=startup&amp;only=scripts&amp;raw=1&amp;skin=vector"></script>
<meta name="ResourceLoaderDynamicStyles" content=""/>
<link rel="stylesheet" href="/w/load.php?lang=en&amp;modules=site.styles&amp;only=styles&amp;skin=vector"/>
<meta name="generator" content="MediaWiki 1.38.0-wmf.25"/>
<meta name="referrer" content="origin"/>
<meta name="referrer" content="origin-when-crossorigin"/>
<meta name="referrer" content="origin-when-cross-origin"/>
<meta name="format-detection" content="telephone=no"/>
<meta property="og:image" content="https://upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Kernel_Machine.svg/1200px-Kernel_Machine.svg.png"/>
<meta property="og:image:width" content="1200"/>
<meta property="og:image:height" content="546"/>
<meta property="og:image" content="https://upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Kernel_Machine.svg/800px-Kernel_Machine.svg.png"/>
<meta property="og:image:width" content="800"/>
<meta property="og:image:height" content="364"/>
<meta property="og:image" content="https://upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Kernel_Machine.svg/640px-Kernel_Machine.svg.png"/>
<meta property="og:image:width" content="640"/>
<meta property="og:image:height" content="291"/>
<meta property="og:title" content="Autoencoder - Wikipedia"/>
<meta property="og:type" content="website"/>
<link rel="preconnect" href="//upload.wikimedia.org"/>
<link rel="alternate" media="only screen and (max-width: 720px)" href="//en.m.wikipedia.org/wiki/Autoencoder"/>
<link rel="alternate" type="application/x-wiki" title="Edit this page" href="/w/index.php?title=Autoencoder&amp;action=edit"/>
<link rel="apple-touch-icon" href="/static/apple-touch/wikipedia.png"/>
<link rel="shortcut icon" href="/static/favicon/wikipedia.ico"/>
<link rel="search" type="application/opensearchdescription+xml" href="/w/opensearch_desc.php" title="Wikipedia (en)"/>
<link rel="EditURI" type="application/rsd+xml" href="//en.wikipedia.org/w/api.php?action=rsd"/>
<link rel="license" href="https://creativecommons.org/licenses/by-sa/3.0/"/>
<link rel="canonical" href="https://en.wikipedia.org/wiki/Autoencoder"/>
<link rel="dns-prefetch" href="//meta.wikimedia.org" />
<link rel="dns-prefetch" href="//login.wikimedia.org"/>
</head>
<body class="mediawiki ltr sitedir-ltr mw-hide-empty-elt ns-0 ns-subject mw-editable page-Autoencoder rootpage-Autoencoder skin-vector action-view skin-vector-legacy"><div id="mw-page-base" class="noprint"></div>
<div id="mw-head-base" class="noprint"></div>
<div id="content" class="mw-body" role="main">
	<a id="top"></a>
	<div id="siteNotice"><!-- CentralNotice --></div>
	<div class="mw-indicators">
	</div>
	<h1 id="firstHeading" class="firstHeading mw-first-heading">Autoencoder</h1>
	<div id="bodyContent" class="vector-body">
		<div id="siteSub" class="noprint">From Wikipedia, the free encyclopedia</div>
		<div id="contentSub"></div>
		<div id="contentSub2"></div>
		
		<div id="jump-to-nav"></div>
		<a class="mw-jump-link" href="#mw-head">Jump to navigation</a>
		<a class="mw-jump-link" href="#searchInput">Jump to search</a>
		<div id="mw-content-text" class="mw-body-content mw-content-ltr" lang="en" dir="ltr"><div class="mw-parser-output"><div class="shortdescription nomobile noexcerpt noprint searchaux" style="display:none">Neural network that learns efficient data encoding in an unsupervised manner</div>
<style data-mw-deduplicate="TemplateStyles:r1033289096">.mw-parser-output .hatnote{font-style:italic}.mw-parser-output div.hatnote{padding-left:1.6em;margin-bottom:0.5em}.mw-parser-output .hatnote i{font-style:normal}.mw-parser-output .hatnote+link+.hatnote{margin-top:-0.5em}</style><div role="note" class="hatnote navigation-not-searchable">Not to be confused with <a href="/wiki/Autocoder" title="Autocoder">Autocoder</a> or <a href="/wiki/Autocode" title="Autocode">Autocode</a>.</div>
<p class="mw-empty-elt">
</p>
<style data-mw-deduplicate="TemplateStyles:r1045330069">.mw-parser-output .sidebar{width:22em;float:right;clear:right;margin:0.5em 0 1em 1em;background:#f8f9fa;border:1px solid #aaa;padding:0.2em;text-align:center;line-height:1.4em;font-size:88%;border-collapse:collapse;display:table}body.skin-minerva .mw-parser-output .sidebar{display:table!important;float:right!important;margin:0.5em 0 1em 1em!important}.mw-parser-output .sidebar-subgroup{width:100%;margin:0;border-spacing:0}.mw-parser-output .sidebar-left{float:left;clear:left;margin:0.5em 1em 1em 0}.mw-parser-output .sidebar-none{float:none;clear:both;margin:0.5em 1em 1em 0}.mw-parser-output .sidebar-outer-title{padding:0 0.4em 0.2em;font-size:125%;line-height:1.2em;font-weight:bold}.mw-parser-output .sidebar-top-image{padding:0.4em}.mw-parser-output .sidebar-top-caption,.mw-parser-output .sidebar-pretitle-with-top-image,.mw-parser-output .sidebar-caption{padding:0.2em 0.4em 0;line-height:1.2em}.mw-parser-output .sidebar-pretitle{padding:0.4em 0.4em 0;line-height:1.2em}.mw-parser-output .sidebar-title,.mw-parser-output .sidebar-title-with-pretitle{padding:0.2em 0.8em;font-size:145%;line-height:1.2em}.mw-parser-output .sidebar-title-with-pretitle{padding:0.1em 0.4em}.mw-parser-output .sidebar-image{padding:0.2em 0.4em 0.4em}.mw-parser-output .sidebar-heading{padding:0.1em 0.4em}.mw-parser-output .sidebar-content{padding:0 0.5em 0.4em}.mw-parser-output .sidebar-content-with-subgroup{padding:0.1em 0.4em 0.2em}.mw-parser-output .sidebar-above,.mw-parser-output .sidebar-below{padding:0.3em 0.8em;font-weight:bold}.mw-parser-output .sidebar-collapse .sidebar-above,.mw-parser-output .sidebar-collapse .sidebar-below{border-top:1px solid #aaa;border-bottom:1px solid #aaa}.mw-parser-output .sidebar-navbar{text-align:right;font-size:115%;padding:0 0.4em 0.4em}.mw-parser-output .sidebar-list-title{padding:0 0.4em;text-align:left;font-weight:bold;line-height:1.6em;font-size:105%}.mw-parser-output .sidebar-list-title-c{padding:0 0.4em;text-align:center;margin:0 3.3em}@media(max-width:720px){body.mediawiki .mw-parser-output .sidebar{width:100%!important;clear:both;float:none!important;margin-left:0!important;margin-right:0!important}}</style><table class="sidebar sidebar-collapse nomobile nowraplinks"><tbody><tr><td class="sidebar-pretitle">Part of a series on</td></tr><tr><th class="sidebar-title-with-pretitle"><a href="/wiki/Machine_learning" title="Machine learning">Machine learning</a><br />and <a href="/wiki/Data_mining" title="Data mining">data mining</a></th></tr><tr><td class="sidebar-image"><a href="/wiki/File:Kernel_Machine.svg" class="image" title="Scatterplot featuring a linear support vector machine&#39;s decision boundary (dashed line)"><img alt="Scatterplot featuring a linear support vector machine&#39;s decision boundary (dashed line)" src="//upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Kernel_Machine.svg/220px-Kernel_Machine.svg.png" decoding="async" width="220" height="100" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Kernel_Machine.svg/330px-Kernel_Machine.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Kernel_Machine.svg/440px-Kernel_Machine.svg.png 2x" data-file-width="512" data-file-height="233" /></a></td></tr><tr><td class="sidebar-content">
<div class="sidebar-list mw-collapsible mw-collapsed"><div class="sidebar-list-title" style="border-top:1px solid #aaa;text-align:center; background:#ddd;">Problems</div><div class="sidebar-list-content mw-collapsible-content hlist">
<ul><li><a href="/wiki/Statistical_classification" title="Statistical classification">Classification</a></li>
<li><a href="/wiki/Cluster_analysis" title="Cluster analysis">Clustering</a></li>
<li><a href="/wiki/Regression_analysis" title="Regression analysis">Regression</a></li>
<li><a href="/wiki/Anomaly_detection" title="Anomaly detection">Anomaly detection</a></li>
<li><a href="/wiki/Data_Cleaning" class="mw-redirect" title="Data Cleaning">Data Cleaning</a></li>
<li><a href="/wiki/Automated_machine_learning" title="Automated machine learning">AutoML</a></li>
<li><a href="/wiki/Association_rule_learning" title="Association rule learning">Association rules</a></li>
<li><a href="/wiki/Reinforcement_learning" title="Reinforcement learning">Reinforcement learning</a></li>
<li><a href="/wiki/Structured_prediction" title="Structured prediction">Structured prediction</a></li>
<li><a href="/wiki/Feature_engineering" title="Feature engineering">Feature engineering</a></li>
<li><a href="/wiki/Feature_learning" title="Feature learning">Feature learning</a></li>
<li><a href="/wiki/Online_machine_learning" title="Online machine learning">Online learning</a></li>
<li><a href="/wiki/Semi-supervised_learning" title="Semi-supervised learning">Semi-supervised learning</a></li>
<li><a href="/wiki/Unsupervised_learning" title="Unsupervised learning">Unsupervised learning</a></li>
<li><a href="/wiki/Learning_to_rank" title="Learning to rank">Learning to rank</a></li>
<li><a href="/wiki/Grammar_induction" title="Grammar induction">Grammar induction</a></li></ul></div></div></td>
</tr><tr><td class="sidebar-content">
<div class="sidebar-list mw-collapsible mw-collapsed"><div class="sidebar-list-title" style="border-top:1px solid #aaa;text-align:center; background:#ddd;"><div style="display:inline-block; padding:0.1em 0;line-height:1.2em;"><a href="/wiki/Supervised_learning" title="Supervised learning">Supervised learning</a><br /><style data-mw-deduplicate="TemplateStyles:r886047488">.mw-parser-output .nobold{font-weight:normal}</style><span class="nobold"><span style="font-size:85%;">(<b><a href="/wiki/Statistical_classification" title="Statistical classification">classification</a></b>&#160;&#8226;&#32;<b><a href="/wiki/Regression_analysis" title="Regression analysis">regression</a></b>)</span></span> </div></div><div class="sidebar-list-content mw-collapsible-content hlist">
<ul><li><a href="/wiki/Decision_tree_learning" title="Decision tree learning">Decision trees</a></li>
<li><a href="/wiki/Ensemble_learning" title="Ensemble learning">Ensembles</a>
<ul><li><a href="/wiki/Bootstrap_aggregating" title="Bootstrap aggregating">Bagging</a></li>
<li><a href="/wiki/Boosting_(machine_learning)" title="Boosting (machine learning)">Boosting</a></li>
<li><a href="/wiki/Random_forest" title="Random forest">Random forest</a></li></ul></li>
<li><a href="/wiki/K-nearest_neighbors_algorithm" title="K-nearest neighbors algorithm"><i>k</i>-NN</a></li>
<li><a href="/wiki/Linear_regression" title="Linear regression">Linear regression</a></li>
<li><a href="/wiki/Naive_Bayes_classifier" title="Naive Bayes classifier">Naive Bayes</a></li>
<li><a href="/wiki/Artificial_neural_network" title="Artificial neural network">Artificial neural networks</a></li>
<li><a href="/wiki/Logistic_regression" title="Logistic regression">Logistic regression</a></li>
<li><a href="/wiki/Perceptron" title="Perceptron">Perceptron</a></li>
<li><a href="/wiki/Relevance_vector_machine" title="Relevance vector machine">Relevance vector machine (RVM)</a></li>
<li><a href="/wiki/Support-vector_machine" title="Support-vector machine">Support vector machine (SVM)</a></li></ul></div></div></td>
</tr><tr><td class="sidebar-content">
<div class="sidebar-list mw-collapsible mw-collapsed"><div class="sidebar-list-title" style="border-top:1px solid #aaa;text-align:center; background:#ddd;"><a href="/wiki/Cluster_analysis" title="Cluster analysis">Clustering</a></div><div class="sidebar-list-content mw-collapsible-content hlist">
<ul><li><a href="/wiki/BIRCH" title="BIRCH">BIRCH</a></li>
<li><a href="/wiki/CURE_algorithm" title="CURE algorithm">CURE</a></li>
<li><a href="/wiki/Hierarchical_clustering" title="Hierarchical clustering">Hierarchical</a></li>
<li><a href="/wiki/K-means_clustering" title="K-means clustering"><i>k</i>-means</a></li>
<li><a href="/wiki/Expectation%E2%80%93maximization_algorithm" title="Expectation–maximization algorithm">Expectation–maximization (EM)</a></li>
<li><br /><a href="/wiki/DBSCAN" title="DBSCAN">DBSCAN</a></li>
<li><a href="/wiki/OPTICS_algorithm" title="OPTICS algorithm">OPTICS</a></li>
<li><a href="/wiki/Mean_shift" title="Mean shift">Mean shift</a></li></ul></div></div></td>
</tr><tr><td class="sidebar-content">
<div class="sidebar-list mw-collapsible mw-collapsed"><div class="sidebar-list-title" style="border-top:1px solid #aaa;text-align:center; background:#ddd;"><a href="/wiki/Dimensionality_reduction" title="Dimensionality reduction">Dimensionality reduction</a></div><div class="sidebar-list-content mw-collapsible-content hlist">
<ul><li><a href="/wiki/Factor_analysis" title="Factor analysis">Factor analysis</a></li>
<li><a href="/wiki/Canonical_correlation" title="Canonical correlation">CCA</a></li>
<li><a href="/wiki/Independent_component_analysis" title="Independent component analysis">ICA</a></li>
<li><a href="/wiki/Linear_discriminant_analysis" title="Linear discriminant analysis">LDA</a></li>
<li><a href="/wiki/Non-negative_matrix_factorization" title="Non-negative matrix factorization">NMF</a></li>
<li><a href="/wiki/Principal_component_analysis" title="Principal component analysis">PCA</a></li>
<li><a href="/wiki/Proper_generalized_decomposition" title="Proper generalized decomposition">PGD</a></li>
<li><a href="/wiki/T-distributed_stochastic_neighbor_embedding" title="T-distributed stochastic neighbor embedding">t-SNE</a></li></ul></div></div></td>
</tr><tr><td class="sidebar-content">
<div class="sidebar-list mw-collapsible mw-collapsed"><div class="sidebar-list-title" style="border-top:1px solid #aaa;text-align:center; background:#ddd;"><a href="/wiki/Structured_prediction" title="Structured prediction">Structured prediction</a></div><div class="sidebar-list-content mw-collapsible-content hlist">
<ul><li><a href="/wiki/Graphical_model" title="Graphical model">Graphical models</a>
<ul><li><a href="/wiki/Bayesian_network" title="Bayesian network">Bayes net</a></li>
<li><a href="/wiki/Conditional_random_field" title="Conditional random field">Conditional random field</a></li>
<li><a href="/wiki/Hidden_Markov_model" title="Hidden Markov model">Hidden Markov</a></li></ul></li></ul></div></div></td>
</tr><tr><td class="sidebar-content">
<div class="sidebar-list mw-collapsible mw-collapsed"><div class="sidebar-list-title" style="border-top:1px solid #aaa;text-align:center; background:#ddd;"><a href="/wiki/Anomaly_detection" title="Anomaly detection">Anomaly detection</a></div><div class="sidebar-list-content mw-collapsible-content hlist">
<ul><li><a href="/wiki/K-nearest_neighbors_algorithm" title="K-nearest neighbors algorithm"><i>k</i>-NN</a></li>
<li><a href="/wiki/Local_outlier_factor" title="Local outlier factor">Local outlier factor</a></li></ul></div></div></td>
</tr><tr><td class="sidebar-content">
<div class="sidebar-list mw-collapsible"><div class="sidebar-list-title" style="border-top:1px solid #aaa;text-align:center; background:#ddd;"><a href="/wiki/Artificial_neural_network" title="Artificial neural network">Artificial neural network</a></div><div class="sidebar-list-content mw-collapsible-content hlist">
<ul><li><a class="mw-selflink selflink">Autoencoder</a></li>
<li><a href="/wiki/Cognitive_computing" title="Cognitive computing">Cognitive computing</a></li>
<li><a href="/wiki/Deep_learning" title="Deep learning">Deep learning</a></li>
<li><a href="/wiki/DeepDream" title="DeepDream">DeepDream</a></li>
<li><a href="/wiki/Multilayer_perceptron" title="Multilayer perceptron">Multilayer perceptron</a></li>
<li><a href="/wiki/Recurrent_neural_network" title="Recurrent neural network">RNN</a>
<ul><li><a href="/wiki/Long_short-term_memory" title="Long short-term memory">LSTM</a></li>
<li><a href="/wiki/Gated_recurrent_unit" title="Gated recurrent unit">GRU</a></li>
<li><a href="/wiki/Echo_state_network" title="Echo state network">ESN</a></li></ul></li>
<li><a href="/wiki/Restricted_Boltzmann_machine" title="Restricted Boltzmann machine">Restricted Boltzmann machine</a></li>
<li><a href="/wiki/Generative_adversarial_network" title="Generative adversarial network">GAN</a></li>
<li><a href="/wiki/Self-organizing_map" title="Self-organizing map">SOM</a></li>
<li><a href="/wiki/Convolutional_neural_network" title="Convolutional neural network">Convolutional neural network</a>
<ul><li><a href="/wiki/U-Net" title="U-Net">U-Net</a></li></ul></li>
<li><a href="/wiki/Transformer_(machine_learning_model)" title="Transformer (machine learning model)">Transformer</a>
<ul><li><a href="/wiki/Vision_transformer" title="Vision transformer">Vision</a></li></ul></li>
<li><a href="/wiki/Spiking_neural_network" title="Spiking neural network">Spiking neural network</a></li>
<li><a href="/wiki/Memtransistor" title="Memtransistor">Memtransistor</a></li>
<li><a href="/wiki/Electrochemical_RAM" title="Electrochemical RAM">Electrochemical RAM</a> (ECRAM)</li></ul></div></div></td>
</tr><tr><td class="sidebar-content">
<div class="sidebar-list mw-collapsible mw-collapsed"><div class="sidebar-list-title" style="border-top:1px solid #aaa;text-align:center; background:#ddd;"><a href="/wiki/Reinforcement_learning" title="Reinforcement learning">Reinforcement learning</a></div><div class="sidebar-list-content mw-collapsible-content hlist">
<ul><li><a href="/wiki/Q-learning" title="Q-learning">Q-learning</a></li>
<li><a href="/wiki/State%E2%80%93action%E2%80%93reward%E2%80%93state%E2%80%93action" title="State–action–reward–state–action">SARSA</a></li>
<li><a href="/wiki/Temporal_difference_learning" title="Temporal difference learning">Temporal difference (TD)</a></li></ul></div></div></td>
</tr><tr><td class="sidebar-content">
<div class="sidebar-list mw-collapsible mw-collapsed"><div class="sidebar-list-title" style="border-top:1px solid #aaa;text-align:center; background:#ddd;">Theory</div><div class="sidebar-list-content mw-collapsible-content hlist">
<ul><li><a href="/wiki/Kernel_machines" class="mw-redirect" title="Kernel machines">Kernel machines</a></li>
<li><a href="/wiki/Bias%E2%80%93variance_tradeoff" title="Bias–variance tradeoff">Bias–variance tradeoff</a></li>
<li><a href="/wiki/Computational_learning_theory" title="Computational learning theory">Computational learning theory</a></li>
<li><a href="/wiki/Empirical_risk_minimization" title="Empirical risk minimization">Empirical risk minimization</a></li>
<li><a href="/wiki/Occam_learning" title="Occam learning">Occam learning</a></li>
<li><a href="/wiki/Probably_approximately_correct_learning" title="Probably approximately correct learning">PAC learning</a></li>
<li><a href="/wiki/Statistical_learning_theory" title="Statistical learning theory">Statistical learning</a></li>
<li><a href="/wiki/Vapnik%E2%80%93Chervonenkis_theory" title="Vapnik–Chervonenkis theory">VC theory</a></li></ul></div></div></td>
</tr><tr><td class="sidebar-content">
<div class="sidebar-list mw-collapsible mw-collapsed"><div class="sidebar-list-title" style="border-top:1px solid #aaa;text-align:center; background:#ddd;">Machine-learning venues</div><div class="sidebar-list-content mw-collapsible-content hlist">
<ul><li><a href="/wiki/Conference_on_Neural_Information_Processing_Systems" title="Conference on Neural Information Processing Systems">NeurIPS</a></li>
<li><a href="/wiki/International_Conference_on_Machine_Learning" title="International Conference on Machine Learning">ICML</a></li>
<li><a href="/wiki/Machine_Learning_(journal)" title="Machine Learning (journal)">ML</a></li>
<li><a href="/wiki/Journal_of_Machine_Learning_Research" title="Journal of Machine Learning Research">JMLR</a></li>
<li><a rel="nofollow" class="external text" href="https://arxiv.org/list/cs.LG/recent">ArXiv:cs.LG</a></li></ul></div></div></td>
</tr><tr><td class="sidebar-content">
<div class="sidebar-list mw-collapsible mw-collapsed"><div class="sidebar-list-title" style="border-top:1px solid #aaa;text-align:center; background:#ddd;">Related articles</div><div class="sidebar-list-content mw-collapsible-content hlist">
<ul><li><a href="/wiki/Glossary_of_artificial_intelligence" title="Glossary of artificial intelligence">Glossary of artificial intelligence</a></li>
<li><a href="/wiki/List_of_datasets_for_machine-learning_research" title="List of datasets for machine-learning research">List of datasets for machine-learning research</a></li>
<li><a href="/wiki/Outline_of_machine_learning" title="Outline of machine learning">Outline of machine learning</a></li></ul></div></div></td>
</tr><tr><td class="sidebar-navbar"><style data-mw-deduplicate="TemplateStyles:r1063604349">.mw-parser-output .navbar{display:inline;font-size:88%;font-weight:normal}.mw-parser-output .navbar-collapse{float:left;text-align:left}.mw-parser-output .navbar-boxtext{word-spacing:0}.mw-parser-output .navbar ul{display:inline-block;white-space:nowrap;line-height:inherit}.mw-parser-output .navbar-brackets::before{margin-right:-0.125em;content:"[ "}.mw-parser-output .navbar-brackets::after{margin-left:-0.125em;content:" ]"}.mw-parser-output .navbar li{word-spacing:-0.125em}.mw-parser-output .navbar a>span,.mw-parser-output .navbar a>abbr{text-decoration:inherit}.mw-parser-output .navbar-mini abbr{font-variant:small-caps;border-bottom:none;text-decoration:none;cursor:inherit}.mw-parser-output .navbar-ct-full{font-size:114%;margin:0 7em}.mw-parser-output .navbar-ct-mini{font-size:114%;margin:0 4em}</style><div class="navbar plainlinks hlist navbar-mini"><ul><li class="nv-view"><a href="/wiki/Template:Machine_learning" title="Template:Machine learning"><abbr title="View this template">v</abbr></a></li><li class="nv-talk"><a href="/wiki/Template_talk:Machine_learning" title="Template talk:Machine learning"><abbr title="Discuss this template">t</abbr></a></li><li class="nv-edit"><a class="external text" href="https://en.wikipedia.org/w/index.php?title=Template:Machine_learning&amp;action=edit"><abbr title="Edit this template">e</abbr></a></li></ul></div></td></tr></tbody></table>
<p>An <b>autoencoder</b> is a type of <a href="/wiki/Artificial_neural_network" title="Artificial neural network">artificial neural network</a> used to learn <a href="/wiki/Feature_learning" title="Feature learning">efficient codings</a> of unlabeled data (<a href="/wiki/Unsupervised_learning" title="Unsupervised learning">unsupervised learning</a>).<sup id="cite_ref-1" class="reference"><a href="#cite_note-1">&#91;1&#93;</a></sup> The encoding is validated and refined by attempting to regenerate the input from the encoding. The autoencoder learns a <a href="/wiki/Feature_learning" title="Feature learning">representation</a> (encoding) for a set of data, typically for <a href="/wiki/Dimensionality_reduction" title="Dimensionality reduction">dimensionality reduction</a>, by training the network to ignore insignificant data (“noise”). 
</p><p>Variants exist, aiming to force the learned representations to assume useful properties.<sup id="cite_ref-:0_2-0" class="reference"><a href="#cite_note-:0-2">&#91;2&#93;</a></sup> Examples are regularized autoencoders (<i>Sparse</i>, <i>Denoising</i> and <i>Contractive</i>), which are effective in learning representations for subsequent <a href="/wiki/Statistical_classification" title="Statistical classification">classification</a> tasks,<sup id="cite_ref-:4_3-0" class="reference"><a href="#cite_note-:4-3">&#91;3&#93;</a></sup> and <i>Variational</i> autoencoders, with applications as <a href="/wiki/Generative_model" title="Generative model">generative models</a>.<sup id="cite_ref-:11_4-0" class="reference"><a href="#cite_note-:11-4">&#91;4&#93;</a></sup> Autoencoders are applied to many problems, from <a href="/wiki/Face_recognition" class="mw-redirect" title="Face recognition">facial recognition</a>,<sup id="cite_ref-5" class="reference"><a href="#cite_note-5">&#91;5&#93;</a></sup> feature detection,<sup id="cite_ref-:2_6-0" class="reference"><a href="#cite_note-:2-6">&#91;6&#93;</a></sup> anomaly detection to acquiring the meaning of words.<sup id="cite_ref-7" class="reference"><a href="#cite_note-7">&#91;7&#93;</a></sup><sup id="cite_ref-8" class="reference"><a href="#cite_note-8">&#91;8&#93;</a></sup> Autoencoders are also generative models: they can randomly generate new data that is similar to the input data (training data).<sup id="cite_ref-:2_6-1" class="reference"><a href="#cite_note-:2-6">&#91;6&#93;</a></sup>
</p>
<style data-mw-deduplicate="TemplateStyles:r886046785">.mw-parser-output .toclimit-2 .toclevel-1 ul,.mw-parser-output .toclimit-3 .toclevel-2 ul,.mw-parser-output .toclimit-4 .toclevel-3 ul,.mw-parser-output .toclimit-5 .toclevel-4 ul,.mw-parser-output .toclimit-6 .toclevel-5 ul,.mw-parser-output .toclimit-7 .toclevel-6 ul{display:none}</style><div class="toclimit-3"><div id="toc" class="toc" role="navigation" aria-labelledby="mw-toc-heading"><input type="checkbox" role="button" id="toctogglecheckbox" class="toctogglecheckbox" style="display:none" /><div class="toctitle" lang="en" dir="ltr"><h2 id="mw-toc-heading">Contents</h2><span class="toctogglespan"><label class="toctogglelabel" for="toctogglecheckbox"></label></span></div>
<ul>
<li class="toclevel-1 tocsection-1"><a href="#Basic_architecture"><span class="tocnumber">1</span> <span class="toctext">Basic architecture</span></a></li>
<li class="toclevel-1 tocsection-2"><a href="#Variations"><span class="tocnumber">2</span> <span class="toctext">Variations</span></a>
<ul>
<li class="toclevel-2 tocsection-3"><a href="#Regularized_autoencoders"><span class="tocnumber">2.1</span> <span class="toctext">Regularized autoencoders</span></a>
<ul>
<li class="toclevel-3 tocsection-4"><a href="#Sparse_autoencoder_(SAE)"><span class="tocnumber">2.1.1</span> <span class="toctext">Sparse autoencoder (SAE)</span></a></li>
<li class="toclevel-3 tocsection-5"><a href="#Denoising_autoencoder_(DAE)"><span class="tocnumber">2.1.2</span> <span class="toctext">Denoising autoencoder (DAE)</span></a></li>
<li class="toclevel-3 tocsection-6"><a href="#Contractive_autoencoder_(CAE)"><span class="tocnumber">2.1.3</span> <span class="toctext">Contractive autoencoder (CAE)</span></a></li>
</ul>
</li>
<li class="toclevel-2 tocsection-7"><a href="#Concrete_autoencoder"><span class="tocnumber">2.2</span> <span class="toctext">Concrete autoencoder</span></a></li>
<li class="toclevel-2 tocsection-8"><a href="#Variational_autoencoder_(VAE)"><span class="tocnumber">2.3</span> <span class="toctext">Variational autoencoder (VAE)</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-9"><a href="#Advantages_of_depth"><span class="tocnumber">3</span> <span class="toctext">Advantages of depth</span></a>
<ul>
<li class="toclevel-2 tocsection-10"><a href="#Training"><span class="tocnumber">3.1</span> <span class="toctext">Training</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-11"><a href="#Applications"><span class="tocnumber">4</span> <span class="toctext">Applications</span></a>
<ul>
<li class="toclevel-2 tocsection-12"><a href="#Dimensionality_reduction"><span class="tocnumber">4.1</span> <span class="toctext">Dimensionality reduction</span></a>
<ul>
<li class="toclevel-3 tocsection-13"><a href="#Principal_component_analysis"><span class="tocnumber">4.1.1</span> <span class="toctext">Principal component analysis</span></a></li>
</ul>
</li>
<li class="toclevel-2 tocsection-14"><a href="#Information_retrieval"><span class="tocnumber">4.2</span> <span class="toctext">Information retrieval</span></a></li>
<li class="toclevel-2 tocsection-15"><a href="#Anomaly_detection"><span class="tocnumber">4.3</span> <span class="toctext">Anomaly detection</span></a></li>
<li class="toclevel-2 tocsection-16"><a href="#Image_processing"><span class="tocnumber">4.4</span> <span class="toctext">Image processing</span></a></li>
<li class="toclevel-2 tocsection-17"><a href="#Drug_discovery"><span class="tocnumber">4.5</span> <span class="toctext">Drug discovery</span></a></li>
<li class="toclevel-2 tocsection-18"><a href="#Popularity_prediction"><span class="tocnumber">4.6</span> <span class="toctext">Popularity prediction</span></a></li>
<li class="toclevel-2 tocsection-19"><a href="#Machine_translation"><span class="tocnumber">4.7</span> <span class="toctext">Machine translation</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-20"><a href="#See_also"><span class="tocnumber">5</span> <span class="toctext">See also</span></a></li>
<li class="toclevel-1 tocsection-21"><a href="#References"><span class="tocnumber">6</span> <span class="toctext">References</span></a></li>
</ul>
</div>
</div>
<h2><span class="mw-headline" id="Basic_architecture">Basic architecture</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Autoencoder&amp;action=edit&amp;section=1" title="Edit section: Basic architecture">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>An autoencoder has two main parts: an encoder that maps the input into the code, and a decoder that maps the code to a reconstruction of the input.
</p><p>The simplest way to perform the copying task perfectly would be to duplicate the signal. Instead, autoencoders are typically forced to reconstruct the input approximately, preserving only the most relevant aspects of the data in the copy.
</p><p>
The idea of autoencoders has been popular for decades. The first applications date to the 1980s.<sup id="cite_ref-:0_2-1" class="reference"><a href="#cite_note-:0-2">&#91;2&#93;</a></sup><sup id="cite_ref-9" class="reference"><a href="#cite_note-9">&#91;9&#93;</a></sup><sup id="cite_ref-10" class="reference"><a href="#cite_note-10">&#91;10&#93;</a></sup> Their most traditional application was <a href="/wiki/Dimensionality_reduction" title="Dimensionality reduction">dimensionality reduction</a> or <a href="/wiki/Feature_learning" title="Feature learning">feature learning</a>, but the concept became widely used for learning <a href="/wiki/Generative_model" title="Generative model">generative models</a> of data.<sup id="cite_ref-VAE_11-0" class="reference"><a href="#cite_note-VAE-11">&#91;11&#93;</a></sup><sup id="cite_ref-gan_faces_12-0" class="reference"><a href="#cite_note-gan_faces-12">&#91;12&#93;</a></sup> Some of the most powerful <a href="/wiki/Artificial_intelligence" title="Artificial intelligence">AIs</a> in the 2010s involved autoencoders stacked inside <a href="/wiki/Deep_learning" title="Deep learning">deep</a> neural networks.<sup id="cite_ref-domingos_13-0" class="reference"><a href="#cite_note-domingos-13">&#91;13&#93;</a></sup></p><div class="thumb tright"><div class="thumbinner" style="width:222px;"><a href="/wiki/File:Autoencoder_schema.png" class="image"><img alt="" src="//upload.wikimedia.org/wikipedia/commons/thumb/3/37/Autoencoder_schema.png/220px-Autoencoder_schema.png" decoding="async" width="220" height="200" class="thumbimage" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/3/37/Autoencoder_schema.png/330px-Autoencoder_schema.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/3/37/Autoencoder_schema.png/440px-Autoencoder_schema.png 2x" data-file-width="841" data-file-height="765" /></a>  <div class="thumbcaption"><div class="magnify"><a href="/wiki/File:Autoencoder_schema.png" class="internal" title="Enlarge"></a></div>Schema of a basic Autoencoder</div></div></div><p>The simplest form of an autoencoder is a <a href="/wiki/Feedforward_neural_network" title="Feedforward neural network">feedforward</a>, non-<a href="/wiki/Recurrent_neural_network" title="Recurrent neural network">recurrent neural network</a> similar to single layer <a href="/wiki/Perceptrons" class="mw-redirect" title="Perceptrons">perceptrons</a> that participate in <a href="/wiki/Multilayer_perceptron" title="Multilayer perceptron">multilayer perceptrons</a> (MLP) – employing an input layer and an output layer connected by one or more hidden layers. The output layer has the same number of nodes (neurons) as the input layer. Its purpose is to reconstruct its inputs (minimizing the difference between the input and the output) instead of predicting a target value <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle Y}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>Y</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle Y}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/961d67d6b454b4df2301ac571808a3538b3a6d3f" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.171ex; width:1.773ex; height:2.009ex;" alt="Y"/></span> given inputs <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle X}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>X</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle X}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/68baa052181f707c662844a465bfeeb135e82bab" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:1.98ex; height:2.176ex;" alt="X"/></span>. Therefore, autoencoders learn unsupervised.
</p><p>An autoencoder consists of two parts, the encoder and the decoder, which can be defined as transitions <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle \phi }">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>&#x03D5;<!-- ϕ --></mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \phi }</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/72b1f30316670aee6270a28334bdf4f5072cdde4" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.671ex; width:1.385ex; height:2.509ex;" alt="\phi "/></span> and <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle \psi ,}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>&#x03C8;<!-- ψ --></mi>
        <mo>,</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \psi ,}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/bfc4043b55bade492740e58cba74198873db1464" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.671ex; width:2.16ex; height:2.509ex;" alt="{\displaystyle \psi ,}"/></span> such that:
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle \phi :{\mathcal {X}}\rightarrow {\mathcal {F}}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>&#x03D5;<!-- ϕ --></mi>
        <mo>:</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mrow class="MJX-TeXAtom-ORD">
            <mi class="MJX-tex-caligraphic" mathvariant="script">X</mi>
          </mrow>
        </mrow>
        <mo stretchy="false">&#x2192;<!-- → --></mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mrow class="MJX-TeXAtom-ORD">
            <mi class="MJX-tex-caligraphic" mathvariant="script">F</mi>
          </mrow>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \phi :{\mathcal {X}}\rightarrow {\mathcal {F}}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/af15e24dd1ebaa221aa2be25d304f58de027135d" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.671ex; width:10.739ex; height:2.509ex;" alt="{\displaystyle \phi :{\mathcal {X}}\rightarrow {\mathcal {F}}}"/></span></dd>
<dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle \psi :{\mathcal {F}}\rightarrow {\mathcal {X}}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>&#x03C8;<!-- ψ --></mi>
        <mo>:</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mrow class="MJX-TeXAtom-ORD">
            <mi class="MJX-tex-caligraphic" mathvariant="script">F</mi>
          </mrow>
        </mrow>
        <mo stretchy="false">&#x2192;<!-- → --></mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mrow class="MJX-TeXAtom-ORD">
            <mi class="MJX-tex-caligraphic" mathvariant="script">X</mi>
          </mrow>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \psi :{\mathcal {F}}\rightarrow {\mathcal {X}}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/9b3812977cdf1ef5a7c522cae067fd186ac9cddd" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.671ex; width:10.866ex; height:2.509ex;" alt="{\displaystyle \psi :{\mathcal {F}}\rightarrow {\mathcal {X}}}"/></span></dd>
<dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle \phi ,\psi ={\underset {\phi ,\psi }{\operatorname {arg\,min} }}\,\|{\mathcal {X}}-(\psi \circ \phi ){\mathcal {X}}\|^{2}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>&#x03D5;<!-- ϕ --></mi>
        <mo>,</mo>
        <mi>&#x03C8;<!-- ψ --></mi>
        <mo>=</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <munder>
            <mrow class="MJX-TeXAtom-OP MJX-fixedlimits">
              <mi mathvariant="normal">a</mi>
              <mi mathvariant="normal">r</mi>
              <mi mathvariant="normal">g</mi>
              <mspace width="thinmathspace" />
              <mi mathvariant="normal">m</mi>
              <mi mathvariant="normal">i</mi>
              <mi mathvariant="normal">n</mi>
            </mrow>
            <mrow>
              <mi>&#x03D5;<!-- ϕ --></mi>
              <mo>,</mo>
              <mi>&#x03C8;<!-- ψ --></mi>
            </mrow>
          </munder>
        </mrow>
        <mspace width="thinmathspace" />
        <mo fence="false" stretchy="false">&#x2016;<!-- ‖ --></mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mrow class="MJX-TeXAtom-ORD">
            <mi class="MJX-tex-caligraphic" mathvariant="script">X</mi>
          </mrow>
        </mrow>
        <mo>&#x2212;<!-- − --></mo>
        <mo stretchy="false">(</mo>
        <mi>&#x03C8;<!-- ψ --></mi>
        <mo>&#x2218;<!-- ∘ --></mo>
        <mi>&#x03D5;<!-- ϕ --></mi>
        <mo stretchy="false">)</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mrow class="MJX-TeXAtom-ORD">
            <mi class="MJX-tex-caligraphic" mathvariant="script">X</mi>
          </mrow>
        </mrow>
        <msup>
          <mo fence="false" stretchy="false">&#x2016;<!-- ‖ --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>2</mn>
          </mrow>
        </msup>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \phi ,\psi ={\underset {\phi ,\psi }{\operatorname {arg\,min} }}\,\|{\mathcal {X}}-(\psi \circ \phi ){\mathcal {X}}\|^{2}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/1861570b77bf09c6bf845b33bb1b035959205905" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -2.838ex; width:31.79ex; height:5.176ex;" alt="{\displaystyle \phi ,\psi ={\underset {\phi ,\psi }{\operatorname {arg\,min} }}\,\|{\mathcal {X}}-(\psi \circ \phi ){\mathcal {X}}\|^{2}}"/></span></dd></dl>
<p>In the simplest case, given one hidden layer, the encoder stage of an autoencoder takes the input <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle \mathbf {x} \in \mathbb {R} ^{d}={\mathcal {X}}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="bold">x</mi>
        </mrow>
        <mo>&#x2208;<!-- ∈ --></mo>
        <msup>
          <mrow class="MJX-TeXAtom-ORD">
            <mi mathvariant="double-struck">R</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>d</mi>
          </mrow>
        </msup>
        <mo>=</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mrow class="MJX-TeXAtom-ORD">
            <mi class="MJX-tex-caligraphic" mathvariant="script">X</mi>
          </mrow>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \mathbf {x} \in \mathbb {R} ^{d}={\mathcal {X}}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/70a238822b434f8555744fcb5ce2b4e028e47003" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:11.996ex; height:2.676ex;" alt="{\displaystyle \mathbf {x} \in \mathbb {R} ^{d}={\mathcal {X}}}"/></span> and maps it to <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle \mathbf {h} \in \mathbb {R} ^{p}={\mathcal {F}}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="bold">h</mi>
        </mrow>
        <mo>&#x2208;<!-- ∈ --></mo>
        <msup>
          <mrow class="MJX-TeXAtom-ORD">
            <mi mathvariant="double-struck">R</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>p</mi>
          </mrow>
        </msup>
        <mo>=</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mrow class="MJX-TeXAtom-ORD">
            <mi class="MJX-tex-caligraphic" mathvariant="script">F</mi>
          </mrow>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \mathbf {h} \in \mathbb {R} ^{p}={\mathcal {F}}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/7a81ee279be26e05f46922c0cad8ab49630a78a3" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:12.088ex; height:2.343ex;" alt="{\displaystyle \mathbf {h} \in \mathbb {R} ^{p}={\mathcal {F}}}"/></span>:
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle \mathbf {h} =\sigma (\mathbf {Wx} +\mathbf {b} )}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="bold">h</mi>
        </mrow>
        <mo>=</mo>
        <mi>&#x03C3;<!-- σ --></mi>
        <mo stretchy="false">(</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="bold">W</mi>
          <mi mathvariant="bold">x</mi>
        </mrow>
        <mo>+</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="bold">b</mi>
        </mrow>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \mathbf {h} =\sigma (\mathbf {Wx} +\mathbf {b} )}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/d4783741ac8b0a41279ab2bd6575f736e73ec85f" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:16.222ex; height:2.843ex;" alt="{\displaystyle \mathbf {h} =\sigma (\mathbf {Wx} +\mathbf {b} )}"/></span></dd></dl>
<p>This image <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle \mathbf {h} }">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="bold">h</mi>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \mathbf {h} }</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/1fce1c8de3a01ae39379db83781850619c4c0987" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:1.485ex; height:2.176ex;" alt="{\mathbf  {h}}"/></span> is usually referred to as code, <a href="/wiki/Latent_variable" title="Latent variable">latent variables</a>, or a latent representation. <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle \sigma }">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>&#x03C3;<!-- σ --></mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \sigma }</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/59f59b7c3e6fdb1d0365a494b81fb9a696138c36" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:1.33ex; height:1.676ex;" alt="\sigma "/></span> is an element-wise <a href="/wiki/Activation_function" title="Activation function">activation function</a> such as a <a href="/wiki/Sigmoid_function" title="Sigmoid function">sigmoid function</a> or a <a href="/wiki/Rectified_linear_unit" class="mw-redirect" title="Rectified linear unit">rectified linear unit</a>.  <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle \mathbf {W} }">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="bold">W</mi>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \mathbf {W} }</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/04749f1e87cca59c094da23c79cc64b085b0df12" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:2.763ex; height:2.176ex;" alt="\mathbf {W} "/></span> is a weight matrix and <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle \mathbf {b} }">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="bold">b</mi>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \mathbf {b} }</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/13ebf4628a1adf07133a6009e4a78bdd990c6eb9" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:1.485ex; height:2.176ex;" alt="\mathbf {b} "/></span> is a bias vector. Weights and biases are usually initialized randomly, and then updated iteratively during training through <a href="/wiki/Backpropagation" title="Backpropagation">backpropagation</a>. After that, the decoder stage of the autoencoder maps <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle \mathbf {h} }">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="bold">h</mi>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \mathbf {h} }</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/1fce1c8de3a01ae39379db83781850619c4c0987" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:1.485ex; height:2.176ex;" alt="{\mathbf  {h}}"/></span> to the reconstruction <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle \mathbf {x'} }">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <msup>
            <mi mathvariant="bold">x</mi>
            <mo>&#x2032;</mo>
          </msup>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \mathbf {x'} }</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/7d14ab6186e99346cb608a30858c3e1580f760e6" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:2.096ex; height:2.509ex;" alt="\mathbf {x&#039;} "/></span> of the same shape as <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle \mathbf {x} }">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="bold">x</mi>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \mathbf {x} }</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/32adf004df5eb0a8c7fd8c0b6b7405183c5a5ef2" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:1.411ex; height:1.676ex;" alt="\mathbf {x} "/></span>:
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle \mathbf {x'} =\sigma '(\mathbf {W'h} +\mathbf {b'} )}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <msup>
            <mi mathvariant="bold">x</mi>
            <mo>&#x2032;</mo>
          </msup>
        </mrow>
        <mo>=</mo>
        <msup>
          <mi>&#x03C3;<!-- σ --></mi>
          <mo>&#x2032;</mo>
        </msup>
        <mo stretchy="false">(</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <msup>
            <mi mathvariant="bold">W</mi>
            <mo>&#x2032;</mo>
          </msup>
          <mi mathvariant="bold">h</mi>
        </mrow>
        <mo>+</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <msup>
            <mi mathvariant="bold">b</mi>
            <mo>&#x2032;</mo>
          </msup>
        </mrow>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \mathbf {x'} =\sigma '(\mathbf {W'h} +\mathbf {b'} )}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/7e42a81660c4283b01fc43f1a6c4a33dc1557d03" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:18.962ex; height:3.009ex;" alt="{\displaystyle \mathbf {x&#039;} =\sigma &#039;(\mathbf {W&#039;h} +\mathbf {b&#039;} )}"/></span></dd></dl>
<p>where <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle \mathbf {\sigma '} ,\mathbf {W'} ,{\text{ and }}\mathbf {b'} }">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <msup>
            <mi>&#x03C3;<!-- σ --></mi>
            <mo>&#x2032;</mo>
          </msup>
        </mrow>
        <mo>,</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <msup>
            <mi mathvariant="bold">W</mi>
            <mo>&#x2032;</mo>
          </msup>
        </mrow>
        <mo>,</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mtext>&#xA0;and&#xA0;</mtext>
        </mrow>
        <mrow class="MJX-TeXAtom-ORD">
          <msup>
            <mi mathvariant="bold">b</mi>
            <mo>&#x2032;</mo>
          </msup>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \mathbf {\sigma '} ,\mathbf {W'} ,{\text{ and }}\mathbf {b'} }</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/062a0023b06f960a4c9d005bbe283818d5946d33" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.671ex; width:14.609ex; height:2.843ex;" alt="{\displaystyle \mathbf {\sigma &#039;} ,\mathbf {W&#039;} ,{\text{ and }}\mathbf {b&#039;} }"/></span> for the decoder may be unrelated to the corresponding <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle \mathbf {\sigma } ,\mathbf {W} ,{\text{ and }}\mathbf {b} }">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mi>&#x03C3;<!-- σ --></mi>
        </mrow>
        <mo>,</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="bold">W</mi>
        </mrow>
        <mo>,</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mtext>&#xA0;and&#xA0;</mtext>
        </mrow>
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="bold">b</mi>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \mathbf {\sigma } ,\mathbf {W} ,{\text{ and }}\mathbf {b} }</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/2c5d41529f76819f274a8466c0fbfdb937649f31" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.671ex; width:12.554ex; height:2.509ex;" alt="{\displaystyle \mathbf {\sigma } ,\mathbf {W} ,{\text{ and }}\mathbf {b} }"/></span> for the encoder.
</p><p>Autoencoders are trained to minimise reconstruction errors (such as <a href="/wiki/Mean_squared_error" title="Mean squared error">squared errors</a>), often referred to as the "<a href="/wiki/Loss_function" title="Loss function">loss</a>":
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle {\mathcal {L}}(\mathbf {x} ,\mathbf {x'} )=\|\mathbf {x} -\mathbf {x'} \|^{2}=\|\mathbf {x} -\sigma '(\mathbf {W'} (\sigma (\mathbf {Wx} +\mathbf {b} ))+\mathbf {b'} )\|^{2}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mrow class="MJX-TeXAtom-ORD">
            <mi class="MJX-tex-caligraphic" mathvariant="script">L</mi>
          </mrow>
        </mrow>
        <mo stretchy="false">(</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="bold">x</mi>
        </mrow>
        <mo>,</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <msup>
            <mi mathvariant="bold">x</mi>
            <mo>&#x2032;</mo>
          </msup>
        </mrow>
        <mo stretchy="false">)</mo>
        <mo>=</mo>
        <mo fence="false" stretchy="false">&#x2016;<!-- ‖ --></mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="bold">x</mi>
        </mrow>
        <mo>&#x2212;<!-- − --></mo>
        <mrow class="MJX-TeXAtom-ORD">
          <msup>
            <mi mathvariant="bold">x</mi>
            <mo>&#x2032;</mo>
          </msup>
        </mrow>
        <msup>
          <mo fence="false" stretchy="false">&#x2016;<!-- ‖ --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>2</mn>
          </mrow>
        </msup>
        <mo>=</mo>
        <mo fence="false" stretchy="false">&#x2016;<!-- ‖ --></mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="bold">x</mi>
        </mrow>
        <mo>&#x2212;<!-- − --></mo>
        <msup>
          <mi>&#x03C3;<!-- σ --></mi>
          <mo>&#x2032;</mo>
        </msup>
        <mo stretchy="false">(</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <msup>
            <mi mathvariant="bold">W</mi>
            <mo>&#x2032;</mo>
          </msup>
        </mrow>
        <mo stretchy="false">(</mo>
        <mi>&#x03C3;<!-- σ --></mi>
        <mo stretchy="false">(</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="bold">W</mi>
          <mi mathvariant="bold">x</mi>
        </mrow>
        <mo>+</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="bold">b</mi>
        </mrow>
        <mo stretchy="false">)</mo>
        <mo stretchy="false">)</mo>
        <mo>+</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <msup>
            <mi mathvariant="bold">b</mi>
            <mo>&#x2032;</mo>
          </msup>
        </mrow>
        <mo stretchy="false">)</mo>
        <msup>
          <mo fence="false" stretchy="false">&#x2016;<!-- ‖ --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>2</mn>
          </mrow>
        </msup>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle {\mathcal {L}}(\mathbf {x} ,\mathbf {x'} )=\|\mathbf {x} -\mathbf {x'} \|^{2}=\|\mathbf {x} -\sigma '(\mathbf {W'} (\sigma (\mathbf {Wx} +\mathbf {b} ))+\mathbf {b'} )\|^{2}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/2c9d074db8e3d0e19f9f5128edcc26a2e7baad36" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:57.237ex; height:3.176ex;" alt="{\displaystyle {\mathcal {L}}(\mathbf {x} ,\mathbf {x&#039;} )=\|\mathbf {x} -\mathbf {x&#039;} \|^{2}=\|\mathbf {x} -\sigma &#039;(\mathbf {W&#039;} (\sigma (\mathbf {Wx} +\mathbf {b} ))+\mathbf {b&#039;} )\|^{2}}"/></span></dd></dl>
<p>where <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle \mathbf {x} }">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="bold">x</mi>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \mathbf {x} }</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/32adf004df5eb0a8c7fd8c0b6b7405183c5a5ef2" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:1.411ex; height:1.676ex;" alt="\mathbf {x} "/></span> is usually averaged over the training set.
</p><p>As mentioned before, autoencoder training is performed through backpropagation of the error, just like other <a href="/wiki/Feedforward_neural_network" title="Feedforward neural network">feedforward neural networks</a>.
</p><p>Should the <a href="/wiki/Feature_(machine_learning)" title="Feature (machine learning)">feature space</a> <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle {\mathcal {F}}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mrow class="MJX-TeXAtom-ORD">
            <mi class="MJX-tex-caligraphic" mathvariant="script">F</mi>
          </mrow>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle {\mathcal {F}}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/205d4b91000d9dcf1a5bbabdfa6a8395fa60b676" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:1.927ex; height:2.176ex;" alt="{\mathcal {F}}"/></span> have lower dimensionality than the input space <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle {\mathcal {X}}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mrow class="MJX-TeXAtom-ORD">
            <mi class="MJX-tex-caligraphic" mathvariant="script">X</mi>
          </mrow>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle {\mathcal {X}}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/8c7e5461c5286852df4ef652fca7e4b0b63030e9" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:1.875ex; height:2.176ex;" alt="{\mathcal {X}}"/></span>, the feature vector <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle \phi (x)}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>&#x03D5;<!-- ϕ --></mi>
        <mo stretchy="false">(</mo>
        <mi>x</mi>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \phi (x)}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/546b660b2f3cfb5f34be7b3ed8371d54f5c74227" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:4.524ex; height:2.843ex;" alt="\phi (x)"/></span> can be regarded as a <a href="/wiki/Data_compression" title="Data compression">compressed</a> representation of the input <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle x}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>x</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle x}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/87f9e315fd7e2ba406057a97300593c4802b53e4" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:1.33ex; height:1.676ex;" alt="x"/></span>. This is the case of <i>undercomplete</i> autoencoders. If the hidden layers are larger than (<i>overcomplete</i>), or equal to, the input layer, or the hidden units are given enough capacity, an autoencoder can potentially learn the <a href="/wiki/Identity_function" title="Identity function">identity function</a> and become useless. However, experimental results found that overcomplete autoencoders might still <a href="/wiki/Feature_learning" title="Feature learning">learn useful features</a>.<sup id="cite_ref-bengio_14-0" class="reference"><a href="#cite_note-bengio-14">&#91;14&#93;</a></sup> In the ideal setting, the code dimension and the model capacity could be set on the basis of the complexity of the data distribution to be modeled. One way to do so is to exploit the model variants known as Regularized Autoencoders.<sup id="cite_ref-:0_2-2" class="reference"><a href="#cite_note-:0-2">&#91;2&#93;</a></sup>
</p>
<h2><span class="mw-headline" id="Variations">Variations</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Autoencoder&amp;action=edit&amp;section=2" title="Edit section: Variations">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<h3><span class="mw-headline" id="Regularized_autoencoders">Regularized autoencoders</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Autoencoder&amp;action=edit&amp;section=3" title="Edit section: Regularized autoencoders">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Various techniques exist to prevent autoencoders from learning the <a href="/wiki/Identity_function" title="Identity function">identity function</a> and to improve their ability to capture important information and learn richer representations.
</p>
<h4><span id="Sparse_autoencoder_.28SAE.29"></span><span class="mw-headline" id="Sparse_autoencoder_(SAE)">Sparse autoencoder (SAE)</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Autoencoder&amp;action=edit&amp;section=4" title="Edit section: Sparse autoencoder (SAE)">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<div class="thumb tright"><div class="thumbinner" style="width:222px;"><a href="/wiki/File:Autoencoder_sparso.png" class="image"><img alt="" src="//upload.wikimedia.org/wikipedia/commons/thumb/8/83/Autoencoder_sparso.png/220px-Autoencoder_sparso.png" decoding="async" width="220" height="274" class="thumbimage" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/8/83/Autoencoder_sparso.png/330px-Autoencoder_sparso.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/8/83/Autoencoder_sparso.png/440px-Autoencoder_sparso.png 2x" data-file-width="442" data-file-height="550" /></a>  <div class="thumbcaption"><div class="magnify"><a href="/wiki/File:Autoencoder_sparso.png" class="internal" title="Enlarge"></a></div>Simple schema of a single-layer sparse autoencoder. The hidden nodes in bright yellow are activated, while the light yellow ones are inactive. The activation depends on the input.</div></div></div>
<p>Learning <a href="/wiki/Representation_learning" class="mw-redirect" title="Representation learning">representations</a>  in a way that encourages sparsity improves performance on classification tasks.<sup id="cite_ref-:5_15-0" class="reference"><a href="#cite_note-:5-15">&#91;15&#93;</a></sup> Sparse autoencoders may include more (rather than fewer) hidden units than inputs, but only a small number of the hidden units are allowed to be active at the same time (thus, sparse).<sup id="cite_ref-domingos_13-1" class="reference"><a href="#cite_note-domingos-13">&#91;13&#93;</a></sup> This constraint forces the model to respond to the unique statistical features of the training data.
</p><p>Specifically, a sparse autoencoder is an autoencoder whose training criterion involves a sparsity penalty <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle \Omega ({\boldsymbol {h}})}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi mathvariant="normal">&#x03A9;<!-- Ω --></mi>
        <mo stretchy="false">(</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="bold-italic">h</mi>
        </mrow>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \Omega ({\boldsymbol {h}})}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/661a44fc4576ee0c3a37f17fb19b7ed34b5b336e" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:5.04ex; height:2.843ex;" alt="{\displaystyle \Omega ({\boldsymbol {h}})}"/></span> on the code layer <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle {\boldsymbol {h}}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="bold-italic">h</mi>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle {\boldsymbol {h}}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/256342564b5fa949c4ec4c906b709422750982c8" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:1.553ex; height:2.176ex;" alt="{\displaystyle {\boldsymbol {h}}}"/></span>.
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle {\mathcal {L}}(\mathbf {x} ,\mathbf {x'} )+\Omega ({\boldsymbol {h}})}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mrow class="MJX-TeXAtom-ORD">
            <mi class="MJX-tex-caligraphic" mathvariant="script">L</mi>
          </mrow>
        </mrow>
        <mo stretchy="false">(</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="bold">x</mi>
        </mrow>
        <mo>,</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <msup>
            <mi mathvariant="bold">x</mi>
            <mo>&#x2032;</mo>
          </msup>
        </mrow>
        <mo stretchy="false">)</mo>
        <mo>+</mo>
        <mi mathvariant="normal">&#x03A9;<!-- Ω --></mi>
        <mo stretchy="false">(</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="bold-italic">h</mi>
        </mrow>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle {\mathcal {L}}(\mathbf {x} ,\mathbf {x'} )+\Omega ({\boldsymbol {h}})}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/fdaffbe9096d05eb9213604329dc2648fe482a0f" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:15.834ex; height:3.009ex;" alt="{\displaystyle {\mathcal {L}}(\mathbf {x} ,\mathbf {x&#039;} )+\Omega ({\boldsymbol {h}})}"/></span></dd></dl>
<p>Recalling that <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle {\boldsymbol {h}}=f({\boldsymbol {W}}{\boldsymbol {x}}+{\boldsymbol {b}})}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="bold-italic">h</mi>
        </mrow>
        <mo>=</mo>
        <mi>f</mi>
        <mo stretchy="false">(</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="bold-italic">W</mi>
        </mrow>
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="bold-italic">x</mi>
        </mrow>
        <mo>+</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="bold-italic">b</mi>
        </mrow>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle {\boldsymbol {h}}=f({\boldsymbol {W}}{\boldsymbol {x}}+{\boldsymbol {b}})}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/1a3766c038fda93dca0b38053100544f5c5a5587" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:16.127ex; height:2.843ex;" alt="{\displaystyle {\boldsymbol {h}}=f({\boldsymbol {W}}{\boldsymbol {x}}+{\boldsymbol {b}})}"/></span>, the penalty encourages the model to activate (i.e. output value close to 1) specific areas of the network on the basis of the input data, while inactivating all other neurons (i.e. to have an output value close to 0).<sup id="cite_ref-:6_16-0" class="reference"><a href="#cite_note-:6-16">&#91;16&#93;</a></sup>
</p><p>This sparsity can be achieved by formulating the penalty terms in different ways.
</p>
<ul><li>One way is to exploit the <a href="/wiki/Kullback%E2%80%93Leibler_divergence" title="Kullback–Leibler divergence">Kullback-Leibler (KL) divergence</a>.<sup id="cite_ref-:5_15-1" class="reference"><a href="#cite_note-:5-15">&#91;15&#93;</a></sup><sup id="cite_ref-:6_16-1" class="reference"><a href="#cite_note-:6-16">&#91;16&#93;</a></sup><sup id="cite_ref-17" class="reference"><a href="#cite_note-17">&#91;17&#93;</a></sup><sup id="cite_ref-18" class="reference"><a href="#cite_note-18">&#91;18&#93;</a></sup>  Let</li></ul>
<dl><dd><dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle {\hat {\rho _{j}}}={\frac {1}{m}}\sum _{i=1}^{m}[h_{j}(x_{i})]}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mrow class="MJX-TeXAtom-ORD">
            <mover>
              <msub>
                <mi>&#x03C1;<!-- ρ --></mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>j</mi>
                </mrow>
              </msub>
              <mo stretchy="false">&#x005E;<!-- ^ --></mo>
            </mover>
          </mrow>
        </mrow>
        <mo>=</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mfrac>
            <mn>1</mn>
            <mi>m</mi>
          </mfrac>
        </mrow>
        <munderover>
          <mo>&#x2211;<!-- ∑ --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
            <mo>=</mo>
            <mn>1</mn>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>m</mi>
          </mrow>
        </munderover>
        <mo stretchy="false">[</mo>
        <msub>
          <mi>h</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>j</mi>
          </mrow>
        </msub>
        <mo stretchy="false">(</mo>
        <msub>
          <mi>x</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
          </mrow>
        </msub>
        <mo stretchy="false">)</mo>
        <mo stretchy="false">]</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle {\hat {\rho _{j}}}={\frac {1}{m}}\sum _{i=1}^{m}[h_{j}(x_{i})]}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/582c2f9744cfcb64919ae703ac67aaed149972c4" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -3.005ex; width:19.31ex; height:6.843ex;" alt="{\displaystyle {\hat {\rho _{j}}}={\frac {1}{m}}\sum _{i=1}^{m}[h_{j}(x_{i})]}"/></span></dd></dl></dd></dl>
<dl><dd>be the average activation of the hidden unit <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle j}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>j</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle j}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/2f461e54f5c093e92a55547b9764291390f0b5d0" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.671ex; margin-left: -0.027ex; width:0.985ex; height:2.509ex;" alt="j"/></span> (averaged over the  <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle m}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>m</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle m}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/0a07d98bb302f3856cbabc47b2b9016692e3f7bc" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:2.04ex; height:1.676ex;" alt="m"/></span> training examples). The notation <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle h_{j}(x_{i})}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>h</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>j</mi>
          </mrow>
        </msub>
        <mo stretchy="false">(</mo>
        <msub>
          <mi>x</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
          </mrow>
        </msub>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle h_{j}(x_{i})}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/fba55a2af839ba242476fb89dd24989f391f01ad" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -1.005ex; width:6.187ex; height:3.009ex;" alt="{\displaystyle h_{j}(x_{i})}"/></span> identifies the input value that triggered the activation. To encourage most of the neurons to be inactive, <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle {\hat {\rho _{j}}}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mrow class="MJX-TeXAtom-ORD">
            <mover>
              <msub>
                <mi>&#x03C1;<!-- ρ --></mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>j</mi>
                </mrow>
              </msub>
              <mo stretchy="false">&#x005E;<!-- ^ --></mo>
            </mover>
          </mrow>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle {\hat {\rho _{j}}}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/1d476e1c65d425573658c5cd8dd92d7da854b236" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -1.005ex; width:2.112ex; height:2.843ex;" alt="{\displaystyle {\hat {\rho _{j}}}}"/></span> needs to be close to 0. Therefore, this method enforces the constraint <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle {\hat {\rho _{j}}}=\rho }">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mrow class="MJX-TeXAtom-ORD">
            <mover>
              <msub>
                <mi>&#x03C1;<!-- ρ --></mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>j</mi>
                </mrow>
              </msub>
              <mo stretchy="false">&#x005E;<!-- ^ --></mo>
            </mover>
          </mrow>
        </mrow>
        <mo>=</mo>
        <mi>&#x03C1;<!-- ρ --></mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle {\hat {\rho _{j}}}=\rho }</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/a3edbf61bbc005c4eef1b36d8398edd543e37d72" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -1.005ex; width:6.412ex; height:2.843ex;" alt="{\displaystyle {\hat {\rho _{j}}}=\rho }"/></span>  where <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle \rho }">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>&#x03C1;<!-- ρ --></mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \rho }</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/1f7d439671d1289b6a816e6af7a304be40608d64" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:1.202ex; height:2.176ex;" alt="\rho "/></span> is the sparsity parameter, a value close to zero. The penalty term <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle \Omega ({\boldsymbol {h}})}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi mathvariant="normal">&#x03A9;<!-- Ω --></mi>
        <mo stretchy="false">(</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="bold-italic">h</mi>
        </mrow>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \Omega ({\boldsymbol {h}})}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/661a44fc4576ee0c3a37f17fb19b7ed34b5b336e" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:5.04ex; height:2.843ex;" alt="{\displaystyle \Omega ({\boldsymbol {h}})}"/></span> takes a form that penalizes <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle {\hat {\rho _{j}}}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mrow class="MJX-TeXAtom-ORD">
            <mover>
              <msub>
                <mi>&#x03C1;<!-- ρ --></mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>j</mi>
                </mrow>
              </msub>
              <mo stretchy="false">&#x005E;<!-- ^ --></mo>
            </mover>
          </mrow>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle {\hat {\rho _{j}}}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/1d476e1c65d425573658c5cd8dd92d7da854b236" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -1.005ex; width:2.112ex; height:2.843ex;" alt="{\displaystyle {\hat {\rho _{j}}}}"/></span> for deviating significantly from <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle \rho }">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>&#x03C1;<!-- ρ --></mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \rho }</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/1f7d439671d1289b6a816e6af7a304be40608d64" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:1.202ex; height:2.176ex;" alt="\rho "/></span>, exploiting the KL divergence:</dd></dl>
<dl><dd><dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle \sum _{j=1}^{s}KL(\rho ||{\hat {\rho _{j}}})=\sum _{j=1}^{s}\left[\rho \log {\frac {\rho }{\hat {\rho _{j}}}}+(1-\rho )\log {\frac {1-\rho }{1-{\hat {\rho _{j}}}}}\right]}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <munderover>
          <mo>&#x2211;<!-- ∑ --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>j</mi>
            <mo>=</mo>
            <mn>1</mn>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>s</mi>
          </mrow>
        </munderover>
        <mi>K</mi>
        <mi>L</mi>
        <mo stretchy="false">(</mo>
        <mi>&#x03C1;<!-- ρ --></mi>
        <mrow class="MJX-TeXAtom-ORD">
          <mo stretchy="false">|</mo>
        </mrow>
        <mrow class="MJX-TeXAtom-ORD">
          <mo stretchy="false">|</mo>
        </mrow>
        <mrow class="MJX-TeXAtom-ORD">
          <mrow class="MJX-TeXAtom-ORD">
            <mover>
              <msub>
                <mi>&#x03C1;<!-- ρ --></mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>j</mi>
                </mrow>
              </msub>
              <mo stretchy="false">&#x005E;<!-- ^ --></mo>
            </mover>
          </mrow>
        </mrow>
        <mo stretchy="false">)</mo>
        <mo>=</mo>
        <munderover>
          <mo>&#x2211;<!-- ∑ --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>j</mi>
            <mo>=</mo>
            <mn>1</mn>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>s</mi>
          </mrow>
        </munderover>
        <mrow>
          <mo>[</mo>
          <mrow>
            <mi>&#x03C1;<!-- ρ --></mi>
            <mi>log</mi>
            <mo>&#x2061;<!-- ⁡ --></mo>
            <mrow class="MJX-TeXAtom-ORD">
              <mfrac>
                <mi>&#x03C1;<!-- ρ --></mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mover>
                    <msub>
                      <mi>&#x03C1;<!-- ρ --></mi>
                      <mrow class="MJX-TeXAtom-ORD">
                        <mi>j</mi>
                      </mrow>
                    </msub>
                    <mo stretchy="false">&#x005E;<!-- ^ --></mo>
                  </mover>
                </mrow>
              </mfrac>
            </mrow>
            <mo>+</mo>
            <mo stretchy="false">(</mo>
            <mn>1</mn>
            <mo>&#x2212;<!-- − --></mo>
            <mi>&#x03C1;<!-- ρ --></mi>
            <mo stretchy="false">)</mo>
            <mi>log</mi>
            <mo>&#x2061;<!-- ⁡ --></mo>
            <mrow class="MJX-TeXAtom-ORD">
              <mfrac>
                <mrow>
                  <mn>1</mn>
                  <mo>&#x2212;<!-- − --></mo>
                  <mi>&#x03C1;<!-- ρ --></mi>
                </mrow>
                <mrow>
                  <mn>1</mn>
                  <mo>&#x2212;<!-- − --></mo>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mrow class="MJX-TeXAtom-ORD">
                      <mover>
                        <msub>
                          <mi>&#x03C1;<!-- ρ --></mi>
                          <mrow class="MJX-TeXAtom-ORD">
                            <mi>j</mi>
                          </mrow>
                        </msub>
                        <mo stretchy="false">&#x005E;<!-- ^ --></mo>
                      </mover>
                    </mrow>
                  </mrow>
                </mrow>
              </mfrac>
            </mrow>
          </mrow>
          <mo>]</mo>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \sum _{j=1}^{s}KL(\rho ||{\hat {\rho _{j}}})=\sum _{j=1}^{s}\left[\rho \log {\frac {\rho }{\hat {\rho _{j}}}}+(1-\rho )\log {\frac {1-\rho }{1-{\hat {\rho _{j}}}}}\right]}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/93bdf538fae80657148ec8b5f919daf16d3ecb5b" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -3.338ex; width:51.55ex; height:7.176ex;" alt="{\displaystyle \sum _{j=1}^{s}KL(\rho ||{\hat {\rho _{j}}})=\sum _{j=1}^{s}\left[\rho \log {\frac {\rho }{\hat {\rho _{j}}}}+(1-\rho )\log {\frac {1-\rho }{1-{\hat {\rho _{j}}}}}\right]}"/></span></dd></dl></dd></dl>
<dl><dd>where <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle j}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>j</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle j}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/2f461e54f5c093e92a55547b9764291390f0b5d0" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.671ex; margin-left: -0.027ex; width:0.985ex; height:2.509ex;" alt="j"/></span> is summing over the <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle s}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>s</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle s}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/01d131dfd7673938b947072a13a9744fe997e632" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:1.09ex; height:1.676ex;" alt="s"/></span> hidden nodes in the hidden layer, and <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle KL(\rho ||{\hat {\rho _{j}}})}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>K</mi>
        <mi>L</mi>
        <mo stretchy="false">(</mo>
        <mi>&#x03C1;<!-- ρ --></mi>
        <mrow class="MJX-TeXAtom-ORD">
          <mo stretchy="false">|</mo>
        </mrow>
        <mrow class="MJX-TeXAtom-ORD">
          <mo stretchy="false">|</mo>
        </mrow>
        <mrow class="MJX-TeXAtom-ORD">
          <mrow class="MJX-TeXAtom-ORD">
            <mover>
              <msub>
                <mi>&#x03C1;<!-- ρ --></mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>j</mi>
                </mrow>
              </msub>
              <mo stretchy="false">&#x005E;<!-- ^ --></mo>
            </mover>
          </mrow>
        </mrow>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle KL(\rho ||{\hat {\rho _{j}}})}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/250518385ae70f6a35100c7fb7fcc8e004ba1068" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -1.005ex; width:10.065ex; height:3.009ex;" alt="{\displaystyle KL(\rho ||{\hat {\rho _{j}}})}"/></span> is the KL-divergence between a <a href="/wiki/Bernoulli_distribution" title="Bernoulli distribution">Bernoulli random variable</a> with mean <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle \rho }">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>&#x03C1;<!-- ρ --></mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \rho }</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/1f7d439671d1289b6a816e6af7a304be40608d64" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:1.202ex; height:2.176ex;" alt="\rho "/></span> and a Bernoulli random variable with mean <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle {\hat {\rho _{j}}}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mrow class="MJX-TeXAtom-ORD">
            <mover>
              <msub>
                <mi>&#x03C1;<!-- ρ --></mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>j</mi>
                </mrow>
              </msub>
              <mo stretchy="false">&#x005E;<!-- ^ --></mo>
            </mover>
          </mrow>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle {\hat {\rho _{j}}}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/1d476e1c65d425573658c5cd8dd92d7da854b236" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -1.005ex; width:2.112ex; height:2.843ex;" alt="{\displaystyle {\hat {\rho _{j}}}}"/></span>.<sup id="cite_ref-:6_16-2" class="reference"><a href="#cite_note-:6-16">&#91;16&#93;</a></sup></dd></dl>
<ul><li>Another way to achieve sparsity is by applying L1 or L2 regularization terms on the activation, scaled by a certain parameter <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle \lambda }">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>&#x03BB;<!-- λ --></mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \lambda }</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/b43d0ea3c9c025af1be9128e62a18fa74bedda2a" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:1.355ex; height:2.176ex;" alt="\lambda "/></span>.<sup id="cite_ref-19" class="reference"><a href="#cite_note-19">&#91;19&#93;</a></sup> For instance, in the case of L1 the <a href="/wiki/Loss_function" title="Loss function">loss function</a> becomes</li></ul>
<dl><dd><dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle {\mathcal {L}}(\mathbf {x} ,\mathbf {x'} )+\lambda \sum _{i}|h_{i}|}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mrow class="MJX-TeXAtom-ORD">
            <mi class="MJX-tex-caligraphic" mathvariant="script">L</mi>
          </mrow>
        </mrow>
        <mo stretchy="false">(</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="bold">x</mi>
        </mrow>
        <mo>,</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <msup>
            <mi mathvariant="bold">x</mi>
            <mo>&#x2032;</mo>
          </msup>
        </mrow>
        <mo stretchy="false">)</mo>
        <mo>+</mo>
        <mi>&#x03BB;<!-- λ --></mi>
        <munder>
          <mo>&#x2211;<!-- ∑ --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
          </mrow>
        </munder>
        <mrow class="MJX-TeXAtom-ORD">
          <mo stretchy="false">|</mo>
        </mrow>
        <msub>
          <mi>h</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
          </mrow>
        </msub>
        <mrow class="MJX-TeXAtom-ORD">
          <mo stretchy="false">|</mo>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle {\mathcal {L}}(\mathbf {x} ,\mathbf {x'} )+\lambda \sum _{i}|h_{i}|}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/0c10c34c80609ffb3969034231bcec57657fbf3f" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -3.005ex; width:19.711ex; height:5.509ex;" alt="{\displaystyle {\mathcal {L}}(\mathbf {x} ,\mathbf {x&#039;} )+\lambda \sum _{i}|h_{i}|}"/></span></dd></dl></dd></dl>
<ul><li>A further proposed strategy to force sparsity is to manually zero all but the strongest hidden unit activations (<i>k-sparse autoencoder</i>).<sup id="cite_ref-:1_20-0" class="reference"><a href="#cite_note-:1-20">&#91;20&#93;</a></sup> The k-sparse autoencoder is based on a linear autoencoder (i.e. with linear activation function) and tied weights. The identification of the strongest activations can be achieved by sorting the activities and keeping only the first <i>k</i> values, or by using <a href="/wiki/Rectifier_(neural_networks)" title="Rectifier (neural networks)">ReLU</a> hidden units with thresholds that are adaptively adjusted until the k largest activities are identified. This selection acts like the previously mentioned regularization terms in that it prevents the model from reconstructing the input using too many neurons.<sup id="cite_ref-:1_20-1" class="reference"><a href="#cite_note-:1-20">&#91;20&#93;</a></sup></li></ul>
<h4><span id="Denoising_autoencoder_.28DAE.29"></span><span class="mw-headline" id="Denoising_autoencoder_(DAE)">Denoising autoencoder (DAE)</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Autoencoder&amp;action=edit&amp;section=5" title="Edit section: Denoising autoencoder (DAE)">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<p>Denoising autoencoders (DAE) try to achieve a <i>good</i> representation by changing the <i>reconstruction criterion</i>.<sup id="cite_ref-:0_2-3" class="reference"><a href="#cite_note-:0-2">&#91;2&#93;</a></sup>
</p><p>Indeed, DAEs take a partially corrupted input and are trained to recover the original <i>undistorted input</i>.  In practice, the objective of denoising autoencoders is that of cleaning the corrupted input, or <i>denoising.</i> Two assumptions are inherent to this approach:
</p>
<ul><li>Higher level representations are relatively stable and robust to the corruption of the input;</li>
<li>To perform denoising well, the model needs to <a href="/wiki/Feature_extraction" title="Feature extraction">extract features</a> that capture useful structure in the input distribution.<sup id="cite_ref-:4_3-1" class="reference"><a href="#cite_note-:4-3">&#91;3&#93;</a></sup></li></ul>
<p>In other words, denoising is advocated as a training criterion for learning to extract useful features that will constitute better higher level representations of the input.<sup id="cite_ref-:4_3-2" class="reference"><a href="#cite_note-:4-3">&#91;3&#93;</a></sup>
</p><p>The training process of a DAE works as follows:
</p>
<ul><li>The initial input <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle x}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>x</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle x}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/87f9e315fd7e2ba406057a97300593c4802b53e4" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:1.33ex; height:1.676ex;" alt="x"/></span> is corrupted into <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle {\boldsymbol {\tilde {x}}}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mrow class="MJX-TeXAtom-ORD">
            <mover>
              <mi mathvariant="bold-italic">x</mi>
              <mo mathvariant="bold" stretchy="false">&#x007E;<!-- ~ --></mo>
            </mover>
          </mrow>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle {\boldsymbol {\tilde {x}}}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/ccbe6c6a519b23654d7b4c3807282b4287bbdd47" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:1.532ex; height:2.176ex;" alt="{\displaystyle {\boldsymbol {\tilde {x}}}}"/></span> through stochastic mapping <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle {\boldsymbol {\tilde {x}}}\thicksim q_{D}({\boldsymbol {\tilde {x}}}|{\boldsymbol {x}})}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mrow class="MJX-TeXAtom-ORD">
            <mover>
              <mi mathvariant="bold-italic">x</mi>
              <mo mathvariant="bold" stretchy="false">&#x007E;<!-- ~ --></mo>
            </mover>
          </mrow>
        </mrow>
        <mo class="MJX-variant">&#x223C;<!-- ∼ --></mo>
        <msub>
          <mi>q</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>D</mi>
          </mrow>
        </msub>
        <mo stretchy="false">(</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mrow class="MJX-TeXAtom-ORD">
            <mover>
              <mi mathvariant="bold-italic">x</mi>
              <mo mathvariant="bold" stretchy="false">&#x007E;<!-- ~ --></mo>
            </mover>
          </mrow>
        </mrow>
        <mrow class="MJX-TeXAtom-ORD">
          <mo stretchy="false">|</mo>
        </mrow>
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="bold-italic">x</mi>
        </mrow>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle {\boldsymbol {\tilde {x}}}\thicksim q_{D}({\boldsymbol {\tilde {x}}}|{\boldsymbol {x}})}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/142363ae58513427315bebcdeab55a1f5e0bf629" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:12.78ex; height:2.843ex;" alt="{\displaystyle {\boldsymbol {\tilde {x}}}\thicksim q_{D}({\boldsymbol {\tilde {x}}}|{\boldsymbol {x}})}"/></span>.</li>
<li>The corrupted input <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle {\boldsymbol {\tilde {x}}}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mrow class="MJX-TeXAtom-ORD">
            <mover>
              <mi mathvariant="bold-italic">x</mi>
              <mo mathvariant="bold" stretchy="false">&#x007E;<!-- ~ --></mo>
            </mover>
          </mrow>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle {\boldsymbol {\tilde {x}}}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/ccbe6c6a519b23654d7b4c3807282b4287bbdd47" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:1.532ex; height:2.176ex;" alt="{\displaystyle {\boldsymbol {\tilde {x}}}}"/></span> is then mapped to a hidden representation with the same process of the standard autoencoder, <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle {\boldsymbol {h}}=f_{\theta }({\boldsymbol {\tilde {x}}})=s({\boldsymbol {W}}{\boldsymbol {\tilde {x}}}+{\boldsymbol {b}})}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="bold-italic">h</mi>
        </mrow>
        <mo>=</mo>
        <msub>
          <mi>f</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>&#x03B8;<!-- θ --></mi>
          </mrow>
        </msub>
        <mo stretchy="false">(</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mrow class="MJX-TeXAtom-ORD">
            <mover>
              <mi mathvariant="bold-italic">x</mi>
              <mo mathvariant="bold" stretchy="false">&#x007E;<!-- ~ --></mo>
            </mover>
          </mrow>
        </mrow>
        <mo stretchy="false">)</mo>
        <mo>=</mo>
        <mi>s</mi>
        <mo stretchy="false">(</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="bold-italic">W</mi>
        </mrow>
        <mrow class="MJX-TeXAtom-ORD">
          <mrow class="MJX-TeXAtom-ORD">
            <mover>
              <mi mathvariant="bold-italic">x</mi>
              <mo mathvariant="bold" stretchy="false">&#x007E;<!-- ~ --></mo>
            </mover>
          </mrow>
        </mrow>
        <mo>+</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="bold-italic">b</mi>
        </mrow>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle {\boldsymbol {h}}=f_{\theta }({\boldsymbol {\tilde {x}}})=s({\boldsymbol {W}}{\boldsymbol {\tilde {x}}}+{\boldsymbol {b}})}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/856b6f6efecae1718023ec6ac467c574309ba9fc" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:24.521ex; height:2.843ex;" alt="{\displaystyle {\boldsymbol {h}}=f_{\theta }({\boldsymbol {\tilde {x}}})=s({\boldsymbol {W}}{\boldsymbol {\tilde {x}}}+{\boldsymbol {b}})}"/></span>.</li>
<li>From the hidden representation the model reconstructs <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle {\boldsymbol {z}}=g_{\theta '}({\boldsymbol {h}})}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="bold-italic">z</mi>
        </mrow>
        <mo>=</mo>
        <msub>
          <mi>g</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <msup>
              <mi>&#x03B8;<!-- θ --></mi>
              <mo>&#x2032;</mo>
            </msup>
          </mrow>
        </msub>
        <mo stretchy="false">(</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="bold-italic">h</mi>
        </mrow>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle {\boldsymbol {z}}=g_{\theta '}({\boldsymbol {h}})}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/1d9d76c0ea4a82fc202091f0480565cee17d189b" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:10.395ex; height:2.843ex;" alt="{\displaystyle {\boldsymbol {z}}=g_{\theta &#039;}({\boldsymbol {h}})}"/></span>.<sup id="cite_ref-:4_3-3" class="reference"><a href="#cite_note-:4-3">&#91;3&#93;</a></sup></li></ul>
<p>The model's parameters <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle \theta }">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>&#x03B8;<!-- θ --></mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \theta }</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/6e5ab2664b422d53eb0c7df3b87e1360d75ad9af" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:1.09ex; height:2.176ex;" alt="\theta "/></span> and <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle \theta '}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msup>
          <mi>&#x03B8;<!-- θ --></mi>
          <mo>&#x2032;</mo>
        </msup>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \theta '}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/c02a68dba2972e9cf4faf9656f58da0df8be6ae9" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:1.775ex; height:2.509ex;" alt="\theta &#039;"/></span> are trained to minimize the average reconstruction error over the training data, specifically, minimizing the difference between <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle {\boldsymbol {z}}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="bold-italic">z</mi>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle {\boldsymbol {z}}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/bfd80493f0fd8b383e270b7b5297fc11ef2459c0" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:1.29ex; height:1.676ex;" alt="{\displaystyle {\boldsymbol {z}}}"/></span> and the original uncorrupted input  <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle {\boldsymbol {x}}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="bold-italic">x</mi>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle {\boldsymbol {x}}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/606b7680d510560a505937143775ea80fa958051" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:1.532ex; height:1.676ex;" alt="{\boldsymbol {x}}"/></span>.<sup id="cite_ref-:4_3-4" class="reference"><a href="#cite_note-:4-3">&#91;3&#93;</a></sup> Note that each time a random example <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle {\boldsymbol {x}}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="bold-italic">x</mi>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle {\boldsymbol {x}}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/606b7680d510560a505937143775ea80fa958051" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:1.532ex; height:1.676ex;" alt="{\boldsymbol {x}}"/></span> is presented to the model, a new corrupted version is generated stochastically on the basis of <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle q_{D}({\boldsymbol {\tilde {x}}}|{\boldsymbol {x}})}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>q</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>D</mi>
          </mrow>
        </msub>
        <mo stretchy="false">(</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mrow class="MJX-TeXAtom-ORD">
            <mover>
              <mi mathvariant="bold-italic">x</mi>
              <mo mathvariant="bold" stretchy="false">&#x007E;<!-- ~ --></mo>
            </mover>
          </mrow>
        </mrow>
        <mrow class="MJX-TeXAtom-ORD">
          <mo stretchy="false">|</mo>
        </mrow>
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="bold-italic">x</mi>
        </mrow>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle q_{D}({\boldsymbol {\tilde {x}}}|{\boldsymbol {x}})}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/c602ee45112476b6e9aba3c8be437e5298b82bad" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:8.15ex; height:2.843ex;" alt="{\displaystyle q_{D}({\boldsymbol {\tilde {x}}}|{\boldsymbol {x}})}"/></span>.
</p><p>The above-mentioned training process could be applied with any kind of corruption process. Some examples might be additive isotropic <a href="/wiki/Additive_white_Gaussian_noise" title="Additive white Gaussian noise">Gaussian noise</a>, masking noise (a fraction of the input chosen at random for each example is forced to 0) or salt-and-pepper noise (a fraction of the input chosen at random for each example is set to its minimum or maximum value with uniform probability).<sup id="cite_ref-:4_3-5" class="reference"><a href="#cite_note-:4-3">&#91;3&#93;</a></sup>
</p><p>The corruption of the input is performed only during training. After training, no corruption is added.
</p>
<h4><span id="Contractive_autoencoder_.28CAE.29"></span><span class="mw-headline" id="Contractive_autoencoder_(CAE)">Contractive autoencoder (CAE)</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Autoencoder&amp;action=edit&amp;section=6" title="Edit section: Contractive autoencoder (CAE)">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<p>A contractive autoencoder adds an explicit regularizer in its objective function that forces the model to learn an encoding robust to slight variations of input values. This regularizer corresponds to the <a href="/wiki/Frobenius_norm" class="mw-redirect" title="Frobenius norm">Frobenius norm</a> of the <a href="/wiki/Jacobian_matrix_and_determinant" title="Jacobian matrix and determinant">Jacobian matrix</a> of the encoder activations with respect to the input. Since the penalty is applied to training examples only, this term forces the model to learn useful information about the training distribution. The final objective function has the following form:
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle {\mathcal {L}}(\mathbf {x} ,\mathbf {x'} )+\lambda \sum _{i}||\nabla _{x}h_{i}||^{2}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mrow class="MJX-TeXAtom-ORD">
            <mi class="MJX-tex-caligraphic" mathvariant="script">L</mi>
          </mrow>
        </mrow>
        <mo stretchy="false">(</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="bold">x</mi>
        </mrow>
        <mo>,</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <msup>
            <mi mathvariant="bold">x</mi>
            <mo>&#x2032;</mo>
          </msup>
        </mrow>
        <mo stretchy="false">)</mo>
        <mo>+</mo>
        <mi>&#x03BB;<!-- λ --></mi>
        <munder>
          <mo>&#x2211;<!-- ∑ --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
          </mrow>
        </munder>
        <mrow class="MJX-TeXAtom-ORD">
          <mo stretchy="false">|</mo>
        </mrow>
        <mrow class="MJX-TeXAtom-ORD">
          <mo stretchy="false">|</mo>
        </mrow>
        <msub>
          <mi mathvariant="normal">&#x2207;<!-- ∇ --></mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>x</mi>
          </mrow>
        </msub>
        <msub>
          <mi>h</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
          </mrow>
        </msub>
        <mrow class="MJX-TeXAtom-ORD">
          <mo stretchy="false">|</mo>
        </mrow>
        <msup>
          <mrow class="MJX-TeXAtom-ORD">
            <mo stretchy="false">|</mo>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>2</mn>
          </mrow>
        </msup>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle {\mathcal {L}}(\mathbf {x} ,\mathbf {x'} )+\lambda \sum _{i}||\nabla _{x}h_{i}||^{2}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/8c969944dbfe7ccfb757e5430f98f1492d101282" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -3.005ex; width:25.167ex; height:5.509ex;" alt="{\displaystyle {\mathcal {L}}(\mathbf {x} ,\mathbf {x&#039;} )+\lambda \sum _{i}||\nabla _{x}h_{i}||^{2}}"/></span></dd></dl>
<p>The autoencoder is termed contractive because it is encouraged to map a neighborhood of input points to a smaller neighborhood of output points.<sup id="cite_ref-:0_2-4" class="reference"><a href="#cite_note-:0-2">&#91;2&#93;</a></sup>
</p><p>DAE is connected to CAE: in the limit of small Gaussian input noise, DAEs make the reconstruction function resist small but finite-sized input perturbations, while CAEs make the extracted features resist infinitesimal input perturbations.
</p>
<h3><span class="mw-headline" id="Concrete_autoencoder">Concrete autoencoder</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Autoencoder&amp;action=edit&amp;section=7" title="Edit section: Concrete autoencoder">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>The concrete autoencoder is designed for discrete feature selection.<sup id="cite_ref-21" class="reference"><a href="#cite_note-21">&#91;21&#93;</a></sup> A concrete autoencoder forces the latent space to consist only of a user-specified number of features. The concrete autoencoder uses a continuous <a href="/wiki/Relaxation_(approximation)" title="Relaxation (approximation)">relaxation</a> of the <a href="/wiki/Categorical_distribution" title="Categorical distribution">categorical distribution</a> to allow gradients to pass through the feature selector layer, which makes it possible to use standard <a href="/wiki/Backpropagation" title="Backpropagation">backpropagation</a> to learn an optimal subset of input features that minimize reconstruction loss.
</p>
<h3><span id="Variational_autoencoder_.28VAE.29"></span><span class="mw-headline" id="Variational_autoencoder_(VAE)">Variational autoencoder (VAE)</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Autoencoder&amp;action=edit&amp;section=8" title="Edit section: Variational autoencoder (VAE)">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1033289096"/><div role="note" class="hatnote navigation-not-searchable">Main article: <a href="/wiki/Variational_autoencoder" title="Variational autoencoder">Variational autoencoder</a></div>
<p><a href="/wiki/Variational_autoencoder" title="Variational autoencoder">Variational autoencoders</a> (VAEs) belong to the families of <a href="/wiki/Variational_Bayesian_methods" title="Variational Bayesian methods">variational Bayesian methods</a>. Despite the architectural similarities with basic autoencoders, VAEs are architecture with different goals and with a completely different mathematical formulation. The latent space is in this case composed by a mixture of distributions instead of a fixed vector.
</p><p>Given an input dataset <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle \mathbf {x} }">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="bold">x</mi>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \mathbf {x} }</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/32adf004df5eb0a8c7fd8c0b6b7405183c5a5ef2" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:1.411ex; height:1.676ex;" alt="\mathbf {x} "/></span> characterized by an unknown probability function <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle P(\mathbf {x} )}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>P</mi>
        <mo stretchy="false">(</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="bold">x</mi>
        </mrow>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle P(\mathbf {x} )}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/970ea0b151cace7b43ce326d5615aca33bc93072" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:4.966ex; height:2.843ex;" alt="P({\mathbf  {x}})"/></span> and a multivariate latent encoding vector <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle \mathbf {z} }">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="bold">z</mi>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \mathbf {z} }</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/82eca5d0928078d5a61b9e7e98cc73db31070909" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:1.188ex; height:1.676ex;" alt="\mathbf {z} "/></span>, the objective is to model the data as a distribution <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle p_{\theta }(\mathbf {x} )}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>p</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>&#x03B8;<!-- θ --></mi>
          </mrow>
        </msub>
        <mo stretchy="false">(</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="bold">x</mi>
        </mrow>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle p_{\theta }(\mathbf {x} )}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/fab1dbee282696393c63af6ff526c6448eed2e28" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; margin-left: -0.089ex; width:5.482ex; height:2.843ex;" alt="{\displaystyle p_{\theta }(\mathbf {x} )}"/></span>, with <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle \theta }">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>&#x03B8;<!-- θ --></mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \theta }</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/6e5ab2664b422d53eb0c7df3b87e1360d75ad9af" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:1.09ex; height:2.176ex;" alt="\theta "/></span> defined as the set of the network parameters so that <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle p_{\theta }(\mathbf {x} )=\int _{\mathbf {z} }p_{\theta }(\mathbf {x,z} )d\mathbf {z} }">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>p</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>&#x03B8;<!-- θ --></mi>
          </mrow>
        </msub>
        <mo stretchy="false">(</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="bold">x</mi>
        </mrow>
        <mo stretchy="false">)</mo>
        <mo>=</mo>
        <msub>
          <mo>&#x222B;<!-- ∫ --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mrow class="MJX-TeXAtom-ORD">
              <mi mathvariant="bold">z</mi>
            </mrow>
          </mrow>
        </msub>
        <msub>
          <mi>p</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>&#x03B8;<!-- θ --></mi>
          </mrow>
        </msub>
        <mo stretchy="false">(</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="bold">x</mi>
          <mo mathvariant="bold">,</mo>
          <mi mathvariant="bold">z</mi>
        </mrow>
        <mo stretchy="false">)</mo>
        <mi>d</mi>
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="bold">z</mi>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle p_{\theta }(\mathbf {x} )=\int _{\mathbf {z} }p_{\theta }(\mathbf {x,z} )d\mathbf {z} }</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/554f5e711d902684f904d42801967f72fcdd5dba" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -2.338ex; margin-left: -0.089ex; width:21.447ex; height:5.676ex;" alt="{\displaystyle p_{\theta }(\mathbf {x} )=\int _{\mathbf {z} }p_{\theta }(\mathbf {x,z} )d\mathbf {z} }"/></span>.
</p>
<h2><span class="mw-headline" id="Advantages_of_depth">Advantages of depth</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Autoencoder&amp;action=edit&amp;section=9" title="Edit section: Advantages of depth">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<div class="thumb tright"><div class="thumbinner" style="width:352px;"><a href="/wiki/File:Autoencoder_structure.png" class="image"><img alt="" src="//upload.wikimedia.org/wikipedia/commons/thumb/2/28/Autoencoder_structure.png/350px-Autoencoder_structure.png" decoding="async" width="350" height="262" class="thumbimage" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/2/28/Autoencoder_structure.png/525px-Autoencoder_structure.png 1.5x, //upload.wikimedia.org/wikipedia/commons/2/28/Autoencoder_structure.png 2x" data-file-width="677" data-file-height="506" /></a>  <div class="thumbcaption"><div class="magnify"><a href="/wiki/File:Autoencoder_structure.png" class="internal" title="Enlarge"></a></div>Schematic structure of an autoencoder with 3 fully connected hidden layers. The code (z, or h for reference in the text) is the most internal layer.</div></div></div>
<p>Autoencoders are often trained with a single layer encoder and a single layer decoder, but using many-layered (deep) encoders and decoders offers many advantages.<sup id="cite_ref-:0_2-5" class="reference"><a href="#cite_note-:0-2">&#91;2&#93;</a></sup>
</p>
<ul><li>Depth can exponentially reduce the computational cost of representing some functions.<sup id="cite_ref-:0_2-6" class="reference"><a href="#cite_note-:0-2">&#91;2&#93;</a></sup></li>
<li>Depth can exponentially decrease the amount of training data needed to learn some functions.<sup id="cite_ref-:0_2-7" class="reference"><a href="#cite_note-:0-2">&#91;2&#93;</a></sup></li>
<li>Experimentally, deep autoencoders yield better compression compared to shallow or linear autoencoders.<sup id="cite_ref-:7_22-0" class="reference"><a href="#cite_note-:7-22">&#91;22&#93;</a></sup></li></ul>
<h3><span class="mw-headline" id="Training">Training</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Autoencoder&amp;action=edit&amp;section=10" title="Edit section: Training">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p><a href="/wiki/Geoffrey_Hinton" title="Geoffrey Hinton">Geoffrey Hinton</a> developed the <a href="/wiki/Deep_belief_network" title="Deep belief network">deep belief network</a> technique for training many-layered deep autoencoders. His method involves treating each neighbouring set of two layers as a <a href="/wiki/Restricted_Boltzmann_machine" title="Restricted Boltzmann machine">restricted Boltzmann machine</a> so that pretraining approximates a good solution, then using backpropagation to fine-tune the results.<sup id="cite_ref-:7_22-1" class="reference"><a href="#cite_note-:7-22">&#91;22&#93;</a></sup> 
</p><p>Researchers have debated whether joint training (i.e. training the whole architecture together with a single global reconstruction objective to optimize) would be better for deep auto-encoders.<sup id="cite_ref-:9_23-0" class="reference"><a href="#cite_note-:9-23">&#91;23&#93;</a></sup> A 2015 study showed that joint training learns better data models along with more representative features for classification as compared to the layerwise method.<sup id="cite_ref-:9_23-1" class="reference"><a href="#cite_note-:9-23">&#91;23&#93;</a></sup> However, their experiments showed that the success of joint training depends heavily on the regularization strategies adopted.<sup id="cite_ref-:9_23-2" class="reference"><a href="#cite_note-:9-23">&#91;23&#93;</a></sup><sup id="cite_ref-24" class="reference"><a href="#cite_note-24">&#91;24&#93;</a></sup>
</p>
<h2><span class="mw-headline" id="Applications">Applications</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Autoencoder&amp;action=edit&amp;section=11" title="Edit section: Applications">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>The two main applications of autoencoders are dimensionality reduction and information retrieval,<sup id="cite_ref-:0_2-8" class="reference"><a href="#cite_note-:0-2">&#91;2&#93;</a></sup> but modern variations have been applied to other tasks.
</p>
<h3><span class="mw-headline" id="Dimensionality_reduction">Dimensionality reduction</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Autoencoder&amp;action=edit&amp;section=12" title="Edit section: Dimensionality reduction">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<div class="thumb tright"><div class="thumbinner" style="width:222px;"><a href="/wiki/File:PCA_vs_Linear_Autoencoder.png" class="image"><img alt="" src="//upload.wikimedia.org/wikipedia/commons/thumb/0/0b/PCA_vs_Linear_Autoencoder.png/220px-PCA_vs_Linear_Autoencoder.png" decoding="async" width="220" height="110" class="thumbimage" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/0/0b/PCA_vs_Linear_Autoencoder.png/330px-PCA_vs_Linear_Autoencoder.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/0/0b/PCA_vs_Linear_Autoencoder.png/440px-PCA_vs_Linear_Autoencoder.png 2x" data-file-width="576" data-file-height="288" /></a>  <div class="thumbcaption"><div class="magnify"><a href="/wiki/File:PCA_vs_Linear_Autoencoder.png" class="internal" title="Enlarge"></a></div>Plot of the first two Principal Components (left) and a two-dimension hidden layer of a Linear Autoencoder (Right) applied to the <a href="/w/index.php?title=Fashion_MNIST_dataset&amp;action=edit&amp;redlink=1" class="new" title="Fashion MNIST dataset (page does not exist)">Fashion MNIST dataset</a>.<sup id="cite_ref-:10_25-0" class="reference"><a href="#cite_note-:10-25">&#91;25&#93;</a></sup> The two models being both linear learn to span the same subspace. The projection of the data points is indeed identical, apart from rotation of the subspace - to which PCA is invariant.</div></div></div><p><a href="/wiki/Dimensionality_reduction" title="Dimensionality reduction">Dimensionality reduction</a> was one of the first <a href="/wiki/Deep_learning" title="Deep learning">deep learning</a> applications.<sup id="cite_ref-:0_2-9" class="reference"><a href="#cite_note-:0-2">&#91;2&#93;</a></sup>
</p><p>For Hinton's 2006 study,<sup id="cite_ref-:7_22-2" class="reference"><a href="#cite_note-:7-22">&#91;22&#93;</a></sup> he pretrained a multi-layer autoencoder with a stack of <a href="/wiki/Restricted_Boltzmann_machine" title="Restricted Boltzmann machine">RBMs</a> and then used their weights to initialize a deep autoencoder with gradually smaller hidden layers until hitting a bottleneck of 30 neurons. The resulting 30 dimensions of the code yielded a smaller reconstruction error compared to the first 30 components of a principal component analysis (PCA), and learned a representation that was qualitatively easier to interpret, clearly separating data clusters.<sup id="cite_ref-:0_2-10" class="reference"><a href="#cite_note-:0-2">&#91;2&#93;</a></sup><sup id="cite_ref-:7_22-3" class="reference"><a href="#cite_note-:7-22">&#91;22&#93;</a></sup>
</p><p>Representing dimensions can improve performance on tasks such as classification.<sup id="cite_ref-:0_2-11" class="reference"><a href="#cite_note-:0-2">&#91;2&#93;</a></sup> Indeed, the hallmark of dimensionality reduction is to place semantically related examples near each other.<sup id="cite_ref-:3_26-0" class="reference"><a href="#cite_note-:3-26">&#91;26&#93;</a></sup>
</p>
<h4><span class="mw-headline" id="Principal_component_analysis">Principal component analysis</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Autoencoder&amp;action=edit&amp;section=13" title="Edit section: Principal component analysis">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<div class="thumb tright"><div class="thumbinner" style="width:222px;"><a href="/wiki/File:Reconstruction_autoencoders_vs_PCA.png" class="image"><img alt="" src="//upload.wikimedia.org/wikipedia/commons/thumb/e/ea/Reconstruction_autoencoders_vs_PCA.png/220px-Reconstruction_autoencoders_vs_PCA.png" decoding="async" width="220" height="48" class="thumbimage" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/e/ea/Reconstruction_autoencoders_vs_PCA.png/330px-Reconstruction_autoencoders_vs_PCA.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/e/ea/Reconstruction_autoencoders_vs_PCA.png/440px-Reconstruction_autoencoders_vs_PCA.png 2x" data-file-width="2008" data-file-height="441" /></a>  <div class="thumbcaption"><div class="magnify"><a href="/wiki/File:Reconstruction_autoencoders_vs_PCA.png" class="internal" title="Enlarge"></a></div>Reconstruction of 28x28pixel images by an Autoencoder with a code size of two (two-units hidden layer) and the reconstruction from the first two Principal Components of PCA. Images come from the <a href="/w/index.php?title=Fashion_MNIST_dataset&amp;action=edit&amp;redlink=1" class="new" title="Fashion MNIST dataset (page does not exist)">Fashion MNIST dataset</a>.<sup id="cite_ref-:10_25-1" class="reference"><a href="#cite_note-:10-25">&#91;25&#93;</a></sup></div></div></div>
<p>If linear activations are used, or only a single sigmoid hidden layer, then the optimal solution to an autoencoder is strongly related to <a href="/wiki/Principal_component_analysis" title="Principal component analysis">principal component analysis</a> (PCA).<sup id="cite_ref-27" class="reference"><a href="#cite_note-27">&#91;27&#93;</a></sup><sup id="cite_ref-28" class="reference"><a href="#cite_note-28">&#91;28&#93;</a></sup> The weights of an autoencoder with a single hidden layer of size <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle p}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>p</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle p}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/81eac1e205430d1f40810df36a0edffdc367af36" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.671ex; margin-left: -0.089ex; width:1.259ex; height:2.009ex;" alt="p"/></span> (where <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle p}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>p</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle p}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/81eac1e205430d1f40810df36a0edffdc367af36" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.671ex; margin-left: -0.089ex; width:1.259ex; height:2.009ex;" alt="p"/></span> is less than the size of the input) span the same vector subspace as the one spanned by the first <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\displaystyle p}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>p</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle p}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/81eac1e205430d1f40810df36a0edffdc367af36" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.671ex; margin-left: -0.089ex; width:1.259ex; height:2.009ex;" alt="p"/></span> principal components, and the output of the autoencoder is an orthogonal projection onto this subspace. The autoencoder weights are not equal to the principal components, and are generally not orthogonal, yet the principal components may be recovered from them using the <a href="/wiki/Singular_value_decomposition" title="Singular value decomposition">singular value decomposition</a>.<sup id="cite_ref-29" class="reference"><a href="#cite_note-29">&#91;29&#93;</a></sup>
</p><p>However, the potential of autoencoders resides in their non-linearity, allowing the model to learn more powerful generalizations compared to PCA, and to reconstruct the input with significantly lower information loss.<sup id="cite_ref-:7_22-4" class="reference"><a href="#cite_note-:7-22">&#91;22&#93;</a></sup>
</p>
<h3><span class="mw-headline" id="Information_retrieval">Information retrieval</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Autoencoder&amp;action=edit&amp;section=14" title="Edit section: Information retrieval">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p><a href="/wiki/Information_retrieval" title="Information retrieval">Information retrieval</a> benefits particularly from <a href="/wiki/Dimensionality_reduction" title="Dimensionality reduction">dimensionality reduction</a> in that search can become more efficient in certain kinds of low dimensional spaces. Autoencoders were indeed applied to semantic hashing, proposed by <a href="/wiki/Russ_Salakhutdinov" title="Russ Salakhutdinov">Salakhutdinov</a> and Hinton in 2007.<sup id="cite_ref-:3_26-1" class="reference"><a href="#cite_note-:3-26">&#91;26&#93;</a></sup> By training the algorithm to produce a low-dimensional binary code, all database entries could be stored in a <a href="/wiki/Hash_table" title="Hash table">hash table</a> mapping binary code vectors to entries. This table would then support information retrieval by returning all entries with the same binary code as the query, or slightly less similar entries by flipping some bits from the query encoding.
</p>
<h3><span class="mw-headline" id="Anomaly_detection">Anomaly detection</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Autoencoder&amp;action=edit&amp;section=15" title="Edit section: Anomaly detection">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Another application for autoencoders is <a href="/wiki/Anomaly_detection" title="Anomaly detection">anomaly detection</a>.<sup id="cite_ref-30" class="reference"><a href="#cite_note-30">&#91;30&#93;</a></sup> <sup id="cite_ref-31" class="reference"><a href="#cite_note-31">&#91;31&#93;</a></sup><sup id="cite_ref-:8_32-0" class="reference"><a href="#cite_note-:8-32">&#91;32&#93;</a></sup><sup id="cite_ref-33" class="reference"><a href="#cite_note-33">&#91;33&#93;</a></sup><sup id="cite_ref-34" class="reference"><a href="#cite_note-34">&#91;34&#93;</a></sup> By learning to replicate the most salient features in the training data under some of the constraints described previously, the model is encouraged to learn to precisely reproduce the most frequently observed characteristics. When facing anomalies, the model should worsen its reconstruction performance. In most cases, only data with normal instances are used to train the autoencoder; in others, the frequency of anomalies is small compared to the observation set so that its contribution to the learned representation could be ignored. After training, the autoencoder will accurately reconstruct "normal" data, while failing to do so with unfamiliar anomalous data.<sup id="cite_ref-:8_32-1" class="reference"><a href="#cite_note-:8-32">&#91;32&#93;</a></sup> Reconstruction error (the error between the original data and its low dimensional reconstruction) is used as an anomaly score to detect anomalies.<sup id="cite_ref-:8_32-2" class="reference"><a href="#cite_note-:8-32">&#91;32&#93;</a></sup>
</p><p>Recent literature has however shown that certain autoencoding models can, counterintuitively, be very good at reconstructing anomalous examples and consequently not able to reliably perform anomaly detection.<sup id="cite_ref-35" class="reference"><a href="#cite_note-35">&#91;35&#93;</a></sup><sup id="cite_ref-36" class="reference"><a href="#cite_note-36">&#91;36&#93;</a></sup>
</p>
<h3><span class="mw-headline" id="Image_processing">Image processing</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Autoencoder&amp;action=edit&amp;section=16" title="Edit section: Image processing">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>The characteristics of autoencoders are useful in image processing.
</p><p>One example can be found in lossy <a href="/wiki/Image_compression" title="Image compression">image compression</a>, where autoencoders outperformed other approaches and proved competitive against <a href="/wiki/JPEG_2000" title="JPEG 2000">JPEG 2000</a>.<sup id="cite_ref-37" class="reference"><a href="#cite_note-37">&#91;37&#93;</a></sup><sup id="cite_ref-38" class="reference"><a href="#cite_note-38">&#91;38&#93;</a></sup>
</p><p>Another useful application of autoencoders in image preprocessing is <a href="/wiki/Image_denoising" class="mw-redirect" title="Image denoising">image denoising</a>.<sup id="cite_ref-39" class="reference"><a href="#cite_note-39">&#91;39&#93;</a></sup><sup id="cite_ref-40" class="reference"><a href="#cite_note-40">&#91;40&#93;</a></sup><sup id="cite_ref-41" class="reference"><a href="#cite_note-41">&#91;41&#93;</a></sup>
</p><p>Autoencoders found use in more demanding contexts such as <a href="/wiki/Medical_imaging" title="Medical imaging">medical imaging</a> where they have been used for <a href="/wiki/Image_denoising" class="mw-redirect" title="Image denoising">image denoising</a><sup id="cite_ref-42" class="reference"><a href="#cite_note-42">&#91;42&#93;</a></sup> as well as <a href="/wiki/Super-resolution" class="mw-redirect" title="Super-resolution">super-resolution</a>.<sup id="cite_ref-43" class="reference"><a href="#cite_note-43">&#91;43&#93;</a></sup><sup id="cite_ref-44" class="reference"><a href="#cite_note-44">&#91;44&#93;</a></sup> In image-assisted diagnosis, experiments have applied autoencoders for <a href="/wiki/Breast_cancer" title="Breast cancer">breast cancer</a> detection<sup id="cite_ref-45" class="reference"><a href="#cite_note-45">&#91;45&#93;</a></sup> and for modelling the relation between the cognitive decline of <a href="/wiki/Alzheimer%27s_disease" title="Alzheimer&#39;s disease">Alzheimer's disease</a> and the latent features of an autoencoder trained with <a href="/wiki/MRI" class="mw-redirect" title="MRI">MRI</a>.<sup id="cite_ref-46" class="reference"><a href="#cite_note-46">&#91;46&#93;</a></sup>
</p>
<h3><span class="mw-headline" id="Drug_discovery">Drug discovery</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Autoencoder&amp;action=edit&amp;section=17" title="Edit section: Drug discovery">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>In 2019 molecules generated with variational autoencoders were validated experimentally in mice.<sup id="cite_ref-47" class="reference"><a href="#cite_note-47">&#91;47&#93;</a></sup><sup id="cite_ref-48" class="reference"><a href="#cite_note-48">&#91;48&#93;</a></sup>
</p>
<h3><span class="mw-headline" id="Popularity_prediction">Popularity prediction</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Autoencoder&amp;action=edit&amp;section=18" title="Edit section: Popularity prediction">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Recently, a stacked autoencoder framework produced promising results in predicting popularity of social media posts,<sup id="cite_ref-49" class="reference"><a href="#cite_note-49">&#91;49&#93;</a></sup> which is helpful for online advertising strategies.
</p>
<h3><span class="mw-headline" id="Machine_translation">Machine translation</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Autoencoder&amp;action=edit&amp;section=19" title="Edit section: Machine translation">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Autoencoders have been applied to <a href="/wiki/Machine_translation" title="Machine translation">machine translation</a>, which is usually referred to as <a href="/wiki/Neural_machine_translation" title="Neural machine translation">neural machine translation</a> (NMT).<sup id="cite_ref-50" class="reference"><a href="#cite_note-50">&#91;50&#93;</a></sup><sup id="cite_ref-51" class="reference"><a href="#cite_note-51">&#91;51&#93;</a></sup> Unlike traditional autoencoders, the output does not match the input - it is in another language. In NMT, texts are treated as sequences to be encoded into the learning procedure, while on the decoder side sequences in the target language(s) are generated. <a href="/wiki/Language" title="Language">Language</a>-specific autoencoders incorporate further <a href="/wiki/Linguistic" class="mw-redirect" title="Linguistic">linguistic</a> features into the learning procedure, such as Chinese decomposition features.<sup id="cite_ref-52" class="reference"><a href="#cite_note-52">&#91;52&#93;</a></sup>
</p>
<h2><span class="mw-headline" id="See_also">See also</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Autoencoder&amp;action=edit&amp;section=20" title="Edit section: See also">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<ul><li><a href="/wiki/Representation_learning" class="mw-redirect" title="Representation learning">Representation learning</a></li>
<li><a href="/wiki/Sparse_dictionary_learning" title="Sparse dictionary learning">Sparse dictionary learning</a></li>
<li><a href="/wiki/Deep_learning" title="Deep learning">Deep learning</a></li></ul>
<h2><span class="mw-headline" id="References">References</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Autoencoder&amp;action=edit&amp;section=21" title="Edit section: References">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<style data-mw-deduplicate="TemplateStyles:r1011085734">.mw-parser-output .reflist{font-size:90%;margin-bottom:0.5em;list-style-type:decimal}.mw-parser-output .reflist .references{font-size:100%;margin-bottom:0;list-style-type:inherit}.mw-parser-output .reflist-columns-2{column-width:30em}.mw-parser-output .reflist-columns-3{column-width:25em}.mw-parser-output .reflist-columns{margin-top:0.3em}.mw-parser-output .reflist-columns ol{margin-top:0}.mw-parser-output .reflist-columns li{page-break-inside:avoid;break-inside:avoid-column}.mw-parser-output .reflist-upper-alpha{list-style-type:upper-alpha}.mw-parser-output .reflist-upper-roman{list-style-type:upper-roman}.mw-parser-output .reflist-lower-alpha{list-style-type:lower-alpha}.mw-parser-output .reflist-lower-greek{list-style-type:lower-greek}.mw-parser-output .reflist-lower-roman{list-style-type:lower-roman}</style><div class="reflist reflist-columns references-column-width" style="column-width: 30em;">
<ol class="references">
<li id="cite_note-1"><span class="mw-cite-backlink"><b><a href="#cite_ref-1">^</a></b></span> <span class="reference-text"><style data-mw-deduplicate="TemplateStyles:r1067248974">.mw-parser-output cite.citation{font-style:inherit;word-wrap:break-word}.mw-parser-output .citation q{quotes:"\"""\"""'""'"}.mw-parser-output .citation:target{background-color:rgba(0,127,255,0.133)}.mw-parser-output .id-lock-free a,.mw-parser-output .citation .cs1-lock-free a{background:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/6/65/Lock-green.svg")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-limited a,.mw-parser-output .id-lock-registration a,.mw-parser-output .citation .cs1-lock-limited a,.mw-parser-output .citation .cs1-lock-registration a{background:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/d/d6/Lock-gray-alt-2.svg")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-subscription a,.mw-parser-output .citation .cs1-lock-subscription a{background:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/a/aa/Lock-red-alt-2.svg")right 0.1em center/9px no-repeat}.mw-parser-output .cs1-ws-icon a{background:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/4/4c/Wikisource-logo.svg")right 0.1em center/12px no-repeat}.mw-parser-output .cs1-code{color:inherit;background:inherit;border:none;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;color:#d33}.mw-parser-output .cs1-visible-error{color:#d33}.mw-parser-output .cs1-maint{display:none;color:#3a3;margin-left:0.3em}.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right{padding-right:0.2em}.mw-parser-output .citation .mw-selflink{font-weight:inherit}</style><cite id="CITEREFKramer1991" class="citation journal cs1">Kramer, Mark A. (1991). <a rel="nofollow" class="external text" href="https://www.researchgate.net/profile/Abir_Alobaid/post/To_learn_a_probability_density_function_by_using_neural_network_can_we_first_estimate_density_using_nonparametric_methods_then_train_the_network/attachment/59d6450279197b80779a031e/AS:451263696510979@1484601057779/download/NL+PCA+by+using+ANN.pdf">"Nonlinear principal component analysis using autoassociative neural networks"</a> <span class="cs1-format">(PDF)</span>. <i>AIChE Journal</i>. <b>37</b> (2): 233–243. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1002%2Faic.690370209">10.1002/aic.690370209</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=AIChE+Journal&amp;rft.atitle=Nonlinear+principal+component+analysis+using+autoassociative+neural+networks&amp;rft.volume=37&amp;rft.issue=2&amp;rft.pages=233-243&amp;rft.date=1991&amp;rft_id=info%3Adoi%2F10.1002%2Faic.690370209&amp;rft.aulast=Kramer&amp;rft.aufirst=Mark+A.&amp;rft_id=https%3A%2F%2Fwww.researchgate.net%2Fprofile%2FAbir_Alobaid%2Fpost%2FTo_learn_a_probability_density_function_by_using_neural_network_can_we_first_estimate_density_using_nonparametric_methods_then_train_the_network%2Fattachment%2F59d6450279197b80779a031e%2FAS%3A451263696510979%401484601057779%2Fdownload%2FNL%2BPCA%2Bby%2Busing%2BANN.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutoencoder" class="Z3988"></span></span>
</li>
<li id="cite_note-:0-2"><span class="mw-cite-backlink">^ <a href="#cite_ref-:0_2-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-:0_2-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-:0_2-2"><sup><i><b>c</b></i></sup></a> <a href="#cite_ref-:0_2-3"><sup><i><b>d</b></i></sup></a> <a href="#cite_ref-:0_2-4"><sup><i><b>e</b></i></sup></a> <a href="#cite_ref-:0_2-5"><sup><i><b>f</b></i></sup></a> <a href="#cite_ref-:0_2-6"><sup><i><b>g</b></i></sup></a> <a href="#cite_ref-:0_2-7"><sup><i><b>h</b></i></sup></a> <a href="#cite_ref-:0_2-8"><sup><i><b>i</b></i></sup></a> <a href="#cite_ref-:0_2-9"><sup><i><b>j</b></i></sup></a> <a href="#cite_ref-:0_2-10"><sup><i><b>k</b></i></sup></a> <a href="#cite_ref-:0_2-11"><sup><i><b>l</b></i></sup></a></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1067248974"/><cite id="CITEREFGoodfellowBengioCourville2016" class="citation book cs1">Goodfellow, Ian; Bengio, Yoshua; Courville, Aaron (2016). <a rel="nofollow" class="external text" href="http://www.deeplearningbook.org"><i>Deep Learning</i></a>. MIT Press. <a href="/wiki/ISBN_(identifier)" class="mw-redirect" title="ISBN (identifier)">ISBN</a>&#160;<a href="/wiki/Special:BookSources/978-0262035613" title="Special:BookSources/978-0262035613"><bdi>978-0262035613</bdi></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Deep+Learning&amp;rft.pub=MIT+Press&amp;rft.date=2016&amp;rft.isbn=978-0262035613&amp;rft.aulast=Goodfellow&amp;rft.aufirst=Ian&amp;rft.au=Bengio%2C+Yoshua&amp;rft.au=Courville%2C+Aaron&amp;rft_id=http%3A%2F%2Fwww.deeplearningbook.org&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutoencoder" class="Z3988"></span></span>
</li>
<li id="cite_note-:4-3"><span class="mw-cite-backlink">^ <a href="#cite_ref-:4_3-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-:4_3-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-:4_3-2"><sup><i><b>c</b></i></sup></a> <a href="#cite_ref-:4_3-3"><sup><i><b>d</b></i></sup></a> <a href="#cite_ref-:4_3-4"><sup><i><b>e</b></i></sup></a> <a href="#cite_ref-:4_3-5"><sup><i><b>f</b></i></sup></a></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1067248974"/><cite id="CITEREFVincentLarochelle2010" class="citation journal cs1">Vincent, Pascal; Larochelle, Hugo (2010). "Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion". <i>Journal of Machine Learning Research</i>. <b>11</b>: 3371–3408.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Journal+of+Machine+Learning+Research&amp;rft.atitle=Stacked+Denoising+Autoencoders%3A+Learning+Useful+Representations+in+a+Deep+Network+with+a+Local+Denoising+Criterion&amp;rft.volume=11&amp;rft.pages=3371-3408&amp;rft.date=2010&amp;rft.aulast=Vincent&amp;rft.aufirst=Pascal&amp;rft.au=Larochelle%2C+Hugo&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutoencoder" class="Z3988"></span></span>
</li>
<li id="cite_note-:11-4"><span class="mw-cite-backlink"><b><a href="#cite_ref-:11_4-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1067248974"/><cite id="CITEREFWellingKingma2019" class="citation journal cs1">Welling, Max; Kingma, Diederik P. (2019). "An Introduction to Variational Autoencoders". <i>Foundations and Trends in Machine Learning</i>. <b>12</b> (4): 307–392. <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="//arxiv.org/abs/1906.02691">1906.02691</a></span>. <a href="/wiki/Bibcode_(identifier)" class="mw-redirect" title="Bibcode (identifier)">Bibcode</a>:<a rel="nofollow" class="external text" href="https://ui.adsabs.harvard.edu/abs/2019arXiv190602691K">2019arXiv190602691K</a>. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1561%2F2200000056">10.1561/2200000056</a>. <a href="/wiki/S2CID_(identifier)" class="mw-redirect" title="S2CID (identifier)">S2CID</a>&#160;<a rel="nofollow" class="external text" href="https://api.semanticscholar.org/CorpusID:174802445">174802445</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Foundations+and+Trends+in+Machine+Learning&amp;rft.atitle=An+Introduction+to+Variational+Autoencoders&amp;rft.volume=12&amp;rft.issue=4&amp;rft.pages=307-392&amp;rft.date=2019&amp;rft_id=info%3Aarxiv%2F1906.02691&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A174802445%23id-name%3DS2CID&amp;rft_id=info%3Adoi%2F10.1561%2F2200000056&amp;rft_id=info%3Abibcode%2F2019arXiv190602691K&amp;rft.aulast=Welling&amp;rft.aufirst=Max&amp;rft.au=Kingma%2C+Diederik+P.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutoencoder" class="Z3988"></span></span>
</li>
<li id="cite_note-5"><span class="mw-cite-backlink"><b><a href="#cite_ref-5">^</a></b></span> <span class="reference-text">Hinton GE, Krizhevsky A, Wang SD. <a rel="nofollow" class="external text" href="http://www.cs.toronto.edu/~fritz/absps/transauto6.pdf">Transforming auto-encoders.</a> In International Conference on Artificial Neural Networks 2011 Jun 14 (pp. 44-51). Springer, Berlin, Heidelberg.</span>
</li>
<li id="cite_note-:2-6"><span class="mw-cite-backlink">^ <a href="#cite_ref-:2_6-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-:2_6-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1067248974"/><cite id="CITEREFGéron2019" class="citation book cs1">Géron, Aurélien (2019). <i>Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow</i>. Canada: O’Reilly Media, Inc. pp.&#160;739–740.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Hands-On+Machine+Learning+with+Scikit-Learn%2C+Keras%2C+and+TensorFlow&amp;rft.place=Canada&amp;rft.pages=739-740&amp;rft.pub=O%E2%80%99Reilly+Media%2C+Inc.&amp;rft.date=2019&amp;rft.aulast=G%C3%A9ron&amp;rft.aufirst=Aur%C3%A9lien&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutoencoder" class="Z3988"></span></span>
</li>
<li id="cite_note-7"><span class="mw-cite-backlink"><b><a href="#cite_ref-7">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1067248974"/><cite id="CITEREFLiouHuangYang2008" class="citation journal cs1">Liou, Cheng-Yuan; Huang, Jau-Chi; Yang, Wen-Chie (2008). "Modeling word perception using the Elman network". <i>Neurocomputing</i>. <b>71</b> (16–18): 3150. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1016%2Fj.neucom.2008.04.030">10.1016/j.neucom.2008.04.030</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Neurocomputing&amp;rft.atitle=Modeling+word+perception+using+the+Elman+network&amp;rft.volume=71&amp;rft.issue=16%E2%80%9318&amp;rft.pages=3150&amp;rft.date=2008&amp;rft_id=info%3Adoi%2F10.1016%2Fj.neucom.2008.04.030&amp;rft.aulast=Liou&amp;rft.aufirst=Cheng-Yuan&amp;rft.au=Huang%2C+Jau-Chi&amp;rft.au=Yang%2C+Wen-Chie&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutoencoder" class="Z3988"></span></span>
</li>
<li id="cite_note-8"><span class="mw-cite-backlink"><b><a href="#cite_ref-8">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1067248974"/><cite id="CITEREFLiouChengLiouLiou2014" class="citation journal cs1">Liou, Cheng-Yuan; Cheng, Wei-Chen; Liou, Jiun-Wei; Liou, Daw-Ran (2014). "Autoencoder for words". <i>Neurocomputing</i>. <b>139</b>: 84–96. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1016%2Fj.neucom.2013.09.055">10.1016/j.neucom.2013.09.055</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Neurocomputing&amp;rft.atitle=Autoencoder+for+words&amp;rft.volume=139&amp;rft.pages=84-96&amp;rft.date=2014&amp;rft_id=info%3Adoi%2F10.1016%2Fj.neucom.2013.09.055&amp;rft.aulast=Liou&amp;rft.aufirst=Cheng-Yuan&amp;rft.au=Cheng%2C+Wei-Chen&amp;rft.au=Liou%2C+Jiun-Wei&amp;rft.au=Liou%2C+Daw-Ran&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutoencoder" class="Z3988"></span></span>
</li>
<li id="cite_note-9"><span class="mw-cite-backlink"><b><a href="#cite_ref-9">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1067248974"/><cite id="CITEREFSchmidhuber2015" class="citation journal cs1">Schmidhuber, Jürgen (January 2015). "Deep learning in neural networks: An overview". <i>Neural Networks</i>. <b>61</b>: 85–117. <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="//arxiv.org/abs/1404.7828">1404.7828</a></span>. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1016%2Fj.neunet.2014.09.003">10.1016/j.neunet.2014.09.003</a>. <a href="/wiki/PMID_(identifier)" class="mw-redirect" title="PMID (identifier)">PMID</a>&#160;<a rel="nofollow" class="external text" href="//pubmed.ncbi.nlm.nih.gov/25462637">25462637</a>. <a href="/wiki/S2CID_(identifier)" class="mw-redirect" title="S2CID (identifier)">S2CID</a>&#160;<a rel="nofollow" class="external text" href="https://api.semanticscholar.org/CorpusID:11715509">11715509</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Neural+Networks&amp;rft.atitle=Deep+learning+in+neural+networks%3A+An+overview&amp;rft.volume=61&amp;rft.pages=85-117&amp;rft.date=2015-01&amp;rft_id=info%3Aarxiv%2F1404.7828&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A11715509%23id-name%3DS2CID&amp;rft_id=info%3Apmid%2F25462637&amp;rft_id=info%3Adoi%2F10.1016%2Fj.neunet.2014.09.003&amp;rft.aulast=Schmidhuber&amp;rft.aufirst=J%C3%BCrgen&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutoencoder" class="Z3988"></span></span>
</li>
<li id="cite_note-10"><span class="mw-cite-backlink"><b><a href="#cite_ref-10">^</a></b></span> <span class="reference-text">Hinton, G. E., &amp; Zemel, R. S. (1994). Autoencoders, minimum description length and Helmholtz free energy. In <i>Advances in neural information processing systems 6</i> (pp. 3-10).</span>
</li>
<li id="cite_note-VAE-11"><span class="mw-cite-backlink"><b><a href="#cite_ref-VAE_11-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1067248974"/><cite id="CITEREFDiederik_P_KingmaWelling2013" class="citation arxiv cs1">Diederik P Kingma; Welling, Max (2013). "Auto-Encoding Variational Bayes". <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="//arxiv.org/abs/1312.6114">1312.6114</a></span> [<a rel="nofollow" class="external text" href="//arxiv.org/archive/stat.ML">stat.ML</a>].</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=Auto-Encoding+Variational+Bayes&amp;rft.date=2013&amp;rft_id=info%3Aarxiv%2F1312.6114&amp;rft.au=Diederik+P+Kingma&amp;rft.au=Welling%2C+Max&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutoencoder" class="Z3988"></span></span>
</li>
<li id="cite_note-gan_faces-12"><span class="mw-cite-backlink"><b><a href="#cite_ref-gan_faces_12-0">^</a></b></span> <span class="reference-text">Generating Faces with Torch, Boesen A., Larsen L. and Sonderby S.K., 2015 <span class="url"><a rel="nofollow" class="external text" href="http://torch.ch/blog/2015/11/13/gan.html">torch<wbr />.ch<wbr />/blog<wbr />/2015<wbr />/11<wbr />/13<wbr />/gan<wbr />.html</a></span></span>
</li>
<li id="cite_note-domingos-13"><span class="mw-cite-backlink">^ <a href="#cite_ref-domingos_13-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-domingos_13-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1067248974"/><cite id="CITEREFDomingos2015" class="citation book cs1"><a href="/wiki/Pedro_Domingos" title="Pedro Domingos">Domingos, Pedro</a> (2015). "4". <a href="/wiki/The_Master_Algorithm" title="The Master Algorithm"><i>The Master Algorithm: How the Quest for the Ultimate Learning Machine Will Remake Our World</i></a>. Basic Books. "Deeper into the Brain" subsection. <a href="/wiki/ISBN_(identifier)" class="mw-redirect" title="ISBN (identifier)">ISBN</a>&#160;<a href="/wiki/Special:BookSources/978-046506192-1" title="Special:BookSources/978-046506192-1"><bdi>978-046506192-1</bdi></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=4&amp;rft.btitle=The+Master+Algorithm%3A+How+the+Quest+for+the+Ultimate+Learning+Machine+Will+Remake+Our+World&amp;rft.pages=%22Deeper+into+the+Brain%22+subsection&amp;rft.pub=Basic+Books&amp;rft.date=2015&amp;rft.isbn=978-046506192-1&amp;rft.aulast=Domingos&amp;rft.aufirst=Pedro&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutoencoder" class="Z3988"></span></span>
</li>
<li id="cite_note-bengio-14"><span class="mw-cite-backlink"><b><a href="#cite_ref-bengio_14-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1067248974"/><cite id="CITEREFBengio2009" class="citation journal cs1">Bengio, Y. (2009). <a rel="nofollow" class="external text" href="http://www.iro.umontreal.ca/~lisa/pointeurs/TR1312.pdf">"Learning Deep Architectures for AI"</a> <span class="cs1-format">(PDF)</span>. <i>Foundations and Trends in Machine Learning</i>. <b>2</b> (8): 1795–7. <a href="/wiki/CiteSeerX_(identifier)" class="mw-redirect" title="CiteSeerX (identifier)">CiteSeerX</a>&#160;<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="//citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.701.9550">10.1.1.701.9550</a></span>. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1561%2F2200000006">10.1561/2200000006</a>. <a href="/wiki/PMID_(identifier)" class="mw-redirect" title="PMID (identifier)">PMID</a>&#160;<a rel="nofollow" class="external text" href="//pubmed.ncbi.nlm.nih.gov/23946944">23946944</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Foundations+and+Trends+in+Machine+Learning&amp;rft.atitle=Learning+Deep+Architectures+for+AI&amp;rft.volume=2&amp;rft.issue=8&amp;rft.pages=1795-7&amp;rft.date=2009&amp;rft_id=%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fsummary%3Fdoi%3D10.1.1.701.9550%23id-name%3DCiteSeerX&amp;rft_id=info%3Apmid%2F23946944&amp;rft_id=info%3Adoi%2F10.1561%2F2200000006&amp;rft.aulast=Bengio&amp;rft.aufirst=Y.&amp;rft_id=http%3A%2F%2Fwww.iro.umontreal.ca%2F~lisa%2Fpointeurs%2FTR1312.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutoencoder" class="Z3988"></span></span>
</li>
<li id="cite_note-:5-15"><span class="mw-cite-backlink">^ <a href="#cite_ref-:5_15-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-:5_15-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1067248974"/><cite id="CITEREFFreyMakhzani2013" class="citation journal cs1">Frey, Brendan; Makhzani, Alireza (2013-12-19). "k-Sparse Autoencoders". <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="//arxiv.org/abs/1312.5663">1312.5663</a></span>. <a href="/wiki/Bibcode_(identifier)" class="mw-redirect" title="Bibcode (identifier)">Bibcode</a>:<a rel="nofollow" class="external text" href="https://ui.adsabs.harvard.edu/abs/2013arXiv1312.5663M">2013arXiv1312.5663M</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=k-Sparse+Autoencoders&amp;rft.date=2013-12-19&amp;rft_id=info%3Aarxiv%2F1312.5663&amp;rft_id=info%3Abibcode%2F2013arXiv1312.5663M&amp;rft.aulast=Frey&amp;rft.aufirst=Brendan&amp;rft.au=Makhzani%2C+Alireza&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutoencoder" class="Z3988"></span> <span class="cs1-hidden-error citation-comment"><code class="cs1-code">{{<a href="/wiki/Template:Cite_journal" title="Template:Cite journal">cite journal</a>}}</code>: </span><span class="cs1-hidden-error citation-comment">Cite journal requires <code class="cs1-code">&#124;journal=</code> (<a href="/wiki/Help:CS1_errors#missing_periodical" title="Help:CS1 errors">help</a>)</span></span>
</li>
<li id="cite_note-:6-16"><span class="mw-cite-backlink">^ <a href="#cite_ref-:6_16-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-:6_16-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-:6_16-2"><sup><i><b>c</b></i></sup></a></span> <span class="reference-text">Ng, A. (2011). <a rel="nofollow" class="external text" href="https://web.stanford.edu/class/cs294a/sparseAutoencoder_2011new.pdf">Sparse autoencoder</a>. <i>CS294A Lecture notes</i>, <i>72</i>(2011), 1-19.</span>
</li>
<li id="cite_note-17"><span class="mw-cite-backlink"><b><a href="#cite_ref-17">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1067248974"/><cite id="CITEREFNairHinton2009" class="citation journal cs1">Nair, Vinod; Hinton, Geoffrey E. (2009). <a rel="nofollow" class="external text" href="http://dl.acm.org/citation.cfm?id=2984093.2984244">"3D Object Recognition with Deep Belief Nets"</a>. <i>Proceedings of the 22Nd International Conference on Neural Information Processing Systems</i>. NIPS'09. USA: Curran Associates Inc.: 1339–1347. <a href="/wiki/ISBN_(identifier)" class="mw-redirect" title="ISBN (identifier)">ISBN</a>&#160;<a href="/wiki/Special:BookSources/9781615679119" title="Special:BookSources/9781615679119"><bdi>9781615679119</bdi></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Proceedings+of+the+22Nd+International+Conference+on+Neural+Information+Processing+Systems&amp;rft.atitle=3D+Object+Recognition+with+Deep+Belief+Nets&amp;rft.pages=1339-1347&amp;rft.date=2009&amp;rft.isbn=9781615679119&amp;rft.aulast=Nair&amp;rft.aufirst=Vinod&amp;rft.au=Hinton%2C+Geoffrey+E.&amp;rft_id=http%3A%2F%2Fdl.acm.org%2Fcitation.cfm%3Fid%3D2984093.2984244&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutoencoder" class="Z3988"></span></span>
</li>
<li id="cite_note-18"><span class="mw-cite-backlink"><b><a href="#cite_ref-18">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1067248974"/><cite id="CITEREFZengZhangSongLiu2018" class="citation journal cs1">Zeng, Nianyin; Zhang, Hong; Song, Baoye; Liu, Weibo; Li, Yurong; Dobaie, Abdullah M. (2018-01-17). "Facial expression recognition via learning deep sparse autoencoders". <i>Neurocomputing</i>. <b>273</b>: 643–649. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1016%2Fj.neucom.2017.08.043">10.1016/j.neucom.2017.08.043</a>. <a href="/wiki/ISSN_(identifier)" class="mw-redirect" title="ISSN (identifier)">ISSN</a>&#160;<a rel="nofollow" class="external text" href="//www.worldcat.org/issn/0925-2312">0925-2312</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Neurocomputing&amp;rft.atitle=Facial+expression+recognition+via+learning+deep+sparse+autoencoders&amp;rft.volume=273&amp;rft.pages=643-649&amp;rft.date=2018-01-17&amp;rft_id=info%3Adoi%2F10.1016%2Fj.neucom.2017.08.043&amp;rft.issn=0925-2312&amp;rft.aulast=Zeng&amp;rft.aufirst=Nianyin&amp;rft.au=Zhang%2C+Hong&amp;rft.au=Song%2C+Baoye&amp;rft.au=Liu%2C+Weibo&amp;rft.au=Li%2C+Yurong&amp;rft.au=Dobaie%2C+Abdullah+M.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutoencoder" class="Z3988"></span></span>
</li>
<li id="cite_note-19"><span class="mw-cite-backlink"><b><a href="#cite_ref-19">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1067248974"/><cite id="CITEREFArpitZhouNgoGovindaraju2015" class="citation arxiv cs1">Arpit, Devansh; Zhou, Yingbo; Ngo, Hung; Govindaraju, Venu (2015). "Why Regularized Auto-Encoders learn Sparse Representation?". <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="//arxiv.org/abs/1505.05561">1505.05561</a></span> [<a rel="nofollow" class="external text" href="//arxiv.org/archive/stat.ML">stat.ML</a>].</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=Why+Regularized+Auto-Encoders+learn+Sparse+Representation%3F&amp;rft.date=2015&amp;rft_id=info%3Aarxiv%2F1505.05561&amp;rft.aulast=Arpit&amp;rft.aufirst=Devansh&amp;rft.au=Zhou%2C+Yingbo&amp;rft.au=Ngo%2C+Hung&amp;rft.au=Govindaraju%2C+Venu&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutoencoder" class="Z3988"></span></span>
</li>
<li id="cite_note-:1-20"><span class="mw-cite-backlink">^ <a href="#cite_ref-:1_20-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-:1_20-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1067248974"/><cite id="CITEREFMakhzaniFrey2013" class="citation arxiv cs1">Makhzani, Alireza; Frey, Brendan (2013). "K-Sparse Autoencoders". <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="//arxiv.org/abs/1312.5663">1312.5663</a></span> [<a rel="nofollow" class="external text" href="//arxiv.org/archive/cs.LG">cs.LG</a>].</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=K-Sparse+Autoencoders&amp;rft.date=2013&amp;rft_id=info%3Aarxiv%2F1312.5663&amp;rft.aulast=Makhzani&amp;rft.aufirst=Alireza&amp;rft.au=Frey%2C+Brendan&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutoencoder" class="Z3988"></span></span>
</li>
<li id="cite_note-21"><span class="mw-cite-backlink"><b><a href="#cite_ref-21">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1067248974"/><cite id="CITEREFAbidBalinZou2019" class="citation arxiv cs1">Abid, Abubakar; Balin, Muhammad Fatih; Zou, James (2019-01-27). "Concrete Autoencoders for Differentiable Feature Selection and Reconstruction". <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="//arxiv.org/abs/1901.09346">1901.09346</a></span> [<a rel="nofollow" class="external text" href="//arxiv.org/archive/cs.LG">cs.LG</a>].</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=Concrete+Autoencoders+for+Differentiable+Feature+Selection+and+Reconstruction&amp;rft.date=2019-01-27&amp;rft_id=info%3Aarxiv%2F1901.09346&amp;rft.aulast=Abid&amp;rft.aufirst=Abubakar&amp;rft.au=Balin%2C+Muhammad+Fatih&amp;rft.au=Zou%2C+James&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutoencoder" class="Z3988"></span></span>
</li>
<li id="cite_note-:7-22"><span class="mw-cite-backlink">^ <a href="#cite_ref-:7_22-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-:7_22-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-:7_22-2"><sup><i><b>c</b></i></sup></a> <a href="#cite_ref-:7_22-3"><sup><i><b>d</b></i></sup></a> <a href="#cite_ref-:7_22-4"><sup><i><b>e</b></i></sup></a></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1067248974"/><cite id="CITEREFHintonSalakhutdinov2006" class="citation journal cs1">Hinton, G. E.; Salakhutdinov, R.R. (2006-07-28). "Reducing the Dimensionality of Data with Neural Networks". <i>Science</i>. <b>313</b> (5786): 504–507. <a href="/wiki/Bibcode_(identifier)" class="mw-redirect" title="Bibcode (identifier)">Bibcode</a>:<a rel="nofollow" class="external text" href="https://ui.adsabs.harvard.edu/abs/2006Sci...313..504H">2006Sci...313..504H</a>. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1126%2Fscience.1127647">10.1126/science.1127647</a>. <a href="/wiki/PMID_(identifier)" class="mw-redirect" title="PMID (identifier)">PMID</a>&#160;<a rel="nofollow" class="external text" href="//pubmed.ncbi.nlm.nih.gov/16873662">16873662</a>. <a href="/wiki/S2CID_(identifier)" class="mw-redirect" title="S2CID (identifier)">S2CID</a>&#160;<a rel="nofollow" class="external text" href="https://api.semanticscholar.org/CorpusID:1658773">1658773</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Science&amp;rft.atitle=Reducing+the+Dimensionality+of+Data+with+Neural+Networks&amp;rft.volume=313&amp;rft.issue=5786&amp;rft.pages=504-507&amp;rft.date=2006-07-28&amp;rft_id=info%3Adoi%2F10.1126%2Fscience.1127647&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A1658773%23id-name%3DS2CID&amp;rft_id=info%3Apmid%2F16873662&amp;rft_id=info%3Abibcode%2F2006Sci...313..504H&amp;rft.aulast=Hinton&amp;rft.aufirst=G.+E.&amp;rft.au=Salakhutdinov%2C+R.R.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutoencoder" class="Z3988"></span></span>
</li>
<li id="cite_note-:9-23"><span class="mw-cite-backlink">^ <a href="#cite_ref-:9_23-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-:9_23-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-:9_23-2"><sup><i><b>c</b></i></sup></a></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1067248974"/><cite id="CITEREFZhouArpitNwoguGovindaraju2014" class="citation arxiv cs1">Zhou, Yingbo; Arpit, Devansh; Nwogu, Ifeoma; Govindaraju, Venu (2014). "Is Joint Training Better for Deep Auto-Encoders?". <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="//arxiv.org/abs/1405.1380">1405.1380</a></span> [<a rel="nofollow" class="external text" href="//arxiv.org/archive/stat.ML">stat.ML</a>].</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=Is+Joint+Training+Better+for+Deep+Auto-Encoders%3F&amp;rft.date=2014&amp;rft_id=info%3Aarxiv%2F1405.1380&amp;rft.aulast=Zhou&amp;rft.aufirst=Yingbo&amp;rft.au=Arpit%2C+Devansh&amp;rft.au=Nwogu%2C+Ifeoma&amp;rft.au=Govindaraju%2C+Venu&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutoencoder" class="Z3988"></span></span>
</li>
<li id="cite_note-24"><span class="mw-cite-backlink"><b><a href="#cite_ref-24">^</a></b></span> <span class="reference-text">R. Salakhutdinov and G. E. Hinton, “Deep boltzmann machines,” in AISTATS, 2009, pp. 448–455.</span>
</li>
<li id="cite_note-:10-25"><span class="mw-cite-backlink">^ <a href="#cite_ref-:10_25-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-:10_25-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1067248974"/><cite class="citation web cs1"><a rel="nofollow" class="external text" href="https://github.com/zalandoresearch/fashion-mnist">"Fashion MNIST"</a>. <i><a href="/wiki/GitHub" title="GitHub">GitHub</a></i>. 2019-07-12.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=GitHub&amp;rft.atitle=Fashion+MNIST&amp;rft.date=2019-07-12&amp;rft_id=https%3A%2F%2Fgithub.com%2Fzalandoresearch%2Ffashion-mnist&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutoencoder" class="Z3988"></span></span>
</li>
<li id="cite_note-:3-26"><span class="mw-cite-backlink">^ <a href="#cite_ref-:3_26-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-:3_26-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1067248974"/><cite id="CITEREFSalakhutdinovHinton2009" class="citation journal cs1">Salakhutdinov, Ruslan; Hinton, Geoffrey (2009-07-01). <a rel="nofollow" class="external text" href="https://doi.org/10.1016%2Fj.ijar.2008.11.006">"Semantic hashing"</a>. <i>International Journal of Approximate Reasoning</i>. Special Section on Graphical Models and Information Retrieval. <b>50</b> (7): 969–978. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://doi.org/10.1016%2Fj.ijar.2008.11.006">10.1016/j.ijar.2008.11.006</a></span>. <a href="/wiki/ISSN_(identifier)" class="mw-redirect" title="ISSN (identifier)">ISSN</a>&#160;<a rel="nofollow" class="external text" href="//www.worldcat.org/issn/0888-613X">0888-613X</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=International+Journal+of+Approximate+Reasoning&amp;rft.atitle=Semantic+hashing&amp;rft.volume=50&amp;rft.issue=7&amp;rft.pages=969-978&amp;rft.date=2009-07-01&amp;rft_id=info%3Adoi%2F10.1016%2Fj.ijar.2008.11.006&amp;rft.issn=0888-613X&amp;rft.aulast=Salakhutdinov&amp;rft.aufirst=Ruslan&amp;rft.au=Hinton%2C+Geoffrey&amp;rft_id=%2F%2Fdoi.org%2F10.1016%252Fj.ijar.2008.11.006&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutoencoder" class="Z3988"></span></span>
</li>
<li id="cite_note-27"><span class="mw-cite-backlink"><b><a href="#cite_ref-27">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1067248974"/><cite id="CITEREFBourlardKamp1988" class="citation journal cs1">Bourlard, H.; Kamp, Y. (1988). <a rel="nofollow" class="external text" href="http://infoscience.epfl.ch/record/82601">"Auto-association by multilayer perceptrons and singular value decomposition"</a>. <i>Biological Cybernetics</i>. <b>59</b> (4–5): 291–294. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1007%2FBF00332918">10.1007/BF00332918</a>. <a href="/wiki/PMID_(identifier)" class="mw-redirect" title="PMID (identifier)">PMID</a>&#160;<a rel="nofollow" class="external text" href="//pubmed.ncbi.nlm.nih.gov/3196773">3196773</a>. <a href="/wiki/S2CID_(identifier)" class="mw-redirect" title="S2CID (identifier)">S2CID</a>&#160;<a rel="nofollow" class="external text" href="https://api.semanticscholar.org/CorpusID:206775335">206775335</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Biological+Cybernetics&amp;rft.atitle=Auto-association+by+multilayer+perceptrons+and+singular+value+decomposition&amp;rft.volume=59&amp;rft.issue=4%E2%80%935&amp;rft.pages=291-294&amp;rft.date=1988&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A206775335%23id-name%3DS2CID&amp;rft_id=info%3Apmid%2F3196773&amp;rft_id=info%3Adoi%2F10.1007%2FBF00332918&amp;rft.aulast=Bourlard&amp;rft.aufirst=H.&amp;rft.au=Kamp%2C+Y.&amp;rft_id=http%3A%2F%2Finfoscience.epfl.ch%2Frecord%2F82601&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutoencoder" class="Z3988"></span></span>
</li>
<li id="cite_note-28"><span class="mw-cite-backlink"><b><a href="#cite_ref-28">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1067248974"/><cite id="CITEREFChiccoSadowskiBaldi2014" class="citation book cs1">Chicco, Davide; Sadowski, Peter; Baldi, Pierre (2014). "Deep autoencoder neural networks for gene ontology annotation predictions". <a rel="nofollow" class="external text" href="http://dl.acm.org/citation.cfm?id=2649442"><i>Proceedings of the 5th ACM Conference on Bioinformatics, Computational Biology, and Health Informatics - BCB '14</i></a>. p.&#160;533. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1145%2F2649387.2649442">10.1145/2649387.2649442</a>. <a href="/wiki/Hdl_(identifier)" class="mw-redirect" title="Hdl (identifier)">hdl</a>:<a rel="nofollow" class="external text" href="//hdl.handle.net/11311%2F964622">11311/964622</a>. <a href="/wiki/ISBN_(identifier)" class="mw-redirect" title="ISBN (identifier)">ISBN</a>&#160;<a href="/wiki/Special:BookSources/9781450328944" title="Special:BookSources/9781450328944"><bdi>9781450328944</bdi></a>. <a href="/wiki/S2CID_(identifier)" class="mw-redirect" title="S2CID (identifier)">S2CID</a>&#160;<a rel="nofollow" class="external text" href="https://api.semanticscholar.org/CorpusID:207217210">207217210</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Deep+autoencoder+neural+networks+for+gene+ontology+annotation+predictions&amp;rft.btitle=Proceedings+of+the+5th+ACM+Conference+on+Bioinformatics%2C+Computational+Biology%2C+and+Health+Informatics+-+BCB+%2714&amp;rft.pages=533&amp;rft.date=2014&amp;rft_id=info%3Ahdl%2F11311%2F964622&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A207217210%23id-name%3DS2CID&amp;rft_id=info%3Adoi%2F10.1145%2F2649387.2649442&amp;rft.isbn=9781450328944&amp;rft.aulast=Chicco&amp;rft.aufirst=Davide&amp;rft.au=Sadowski%2C+Peter&amp;rft.au=Baldi%2C+Pierre&amp;rft_id=http%3A%2F%2Fdl.acm.org%2Fcitation.cfm%3Fid%3D2649442&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutoencoder" class="Z3988"></span></span>
</li>
<li id="cite_note-29"><span class="mw-cite-backlink"><b><a href="#cite_ref-29">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1067248974"/><cite id="CITEREFPlaut2018" class="citation arxiv cs1">Plaut, E (2018). "From Principal Subspaces to Principal Components with Linear Autoencoders". <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="//arxiv.org/abs/1804.10253">1804.10253</a></span> [<a rel="nofollow" class="external text" href="//arxiv.org/archive/stat.ML">stat.ML</a>].</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=From+Principal+Subspaces+to+Principal+Components+with+Linear+Autoencoders&amp;rft.date=2018&amp;rft_id=info%3Aarxiv%2F1804.10253&amp;rft.aulast=Plaut&amp;rft.aufirst=E&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutoencoder" class="Z3988"></span></span>
</li>
<li id="cite_note-30"><span class="mw-cite-backlink"><b><a href="#cite_ref-30">^</a></b></span> <span class="reference-text"> Morales-Forero, A., &amp; Bassetto, S. (2019, December). Case Study: A Semi-Supervised Methodology for Anomaly Detection and Diagnosis. In <i>2019 IEEE International Conference on Industrial Engineering and Engineering Management (IEEM)</i> (p. 4) (pp. 1031-1037). IEEE.</span>
</li>
<li id="cite_note-31"><span class="mw-cite-backlink"><b><a href="#cite_ref-31">^</a></b></span> <span class="reference-text">Sakurada, M., &amp; Yairi, T. (2014, December). Anomaly detection using autoencoders with nonlinear dimensionality reduction. In <i>Proceedings of the MLSDA 2014 2nd Workshop on Machine Learning for Sensory Data Analysis</i> (p. 4). ACM.</span>
</li>
<li id="cite_note-:8-32"><span class="mw-cite-backlink">^ <a href="#cite_ref-:8_32-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-:8_32-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-:8_32-2"><sup><i><b>c</b></i></sup></a></span> <span class="reference-text">An, J., &amp; Cho, S. (2015). Variational autoencoder based anomaly detection using reconstruction probability. <i>Special Lecture on IE</i>, <i>2</i>, 1-18.</span>
</li>
<li id="cite_note-33"><span class="mw-cite-backlink"><b><a href="#cite_ref-33">^</a></b></span> <span class="reference-text">Zhou, C., &amp; Paffenroth, R. C. (2017, August). Anomaly detection with robust deep autoencoders. In <i>Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</i> (pp. 665-674). ACM.</span>
</li>
<li id="cite_note-34"><span class="mw-cite-backlink"><b><a href="#cite_ref-34">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1067248974"/><cite id="CITEREFRibeiroLazzarettiLopes2018" class="citation journal cs1">Ribeiro, Manassés; Lazzaretti, André Eugênio; Lopes, Heitor Silvério (2018). "A study of deep convolutional auto-encoders for anomaly detection in videos". <i>Pattern Recognition Letters</i>. <b>105</b>: 13–22. <a href="/wiki/Bibcode_(identifier)" class="mw-redirect" title="Bibcode (identifier)">Bibcode</a>:<a rel="nofollow" class="external text" href="https://ui.adsabs.harvard.edu/abs/2018PaReL.105...13R">2018PaReL.105...13R</a>. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1016%2Fj.patrec.2017.07.016">10.1016/j.patrec.2017.07.016</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Pattern+Recognition+Letters&amp;rft.atitle=A+study+of+deep+convolutional+auto-encoders+for+anomaly+detection+in+videos&amp;rft.volume=105&amp;rft.pages=13-22&amp;rft.date=2018&amp;rft_id=info%3Adoi%2F10.1016%2Fj.patrec.2017.07.016&amp;rft_id=info%3Abibcode%2F2018PaReL.105...13R&amp;rft.aulast=Ribeiro&amp;rft.aufirst=Manass%C3%A9s&amp;rft.au=Lazzaretti%2C+Andr%C3%A9+Eug%C3%AAnio&amp;rft.au=Lopes%2C+Heitor+Silv%C3%A9rio&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutoencoder" class="Z3988"></span></span>
</li>
<li id="cite_note-35"><span class="mw-cite-backlink"><b><a href="#cite_ref-35">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1067248974"/><cite id="CITEREFNalisnickMatsukawaTehGorur2019" class="citation arxiv cs1">Nalisnick, Eric; Matsukawa, Akihiro; Teh, Yee Whye; Gorur, Dilan; Lakshminarayanan, Balaji (2019-02-24). "Do Deep Generative Models Know What They Don't Know?". <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="//arxiv.org/abs/1810.09136">1810.09136</a></span> [<a rel="nofollow" class="external text" href="//arxiv.org/archive/stat.ML">stat.ML</a>].</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=Do+Deep+Generative+Models+Know+What+They+Don%27t+Know%3F&amp;rft.date=2019-02-24&amp;rft_id=info%3Aarxiv%2F1810.09136&amp;rft.aulast=Nalisnick&amp;rft.aufirst=Eric&amp;rft.au=Matsukawa%2C+Akihiro&amp;rft.au=Teh%2C+Yee+Whye&amp;rft.au=Gorur%2C+Dilan&amp;rft.au=Lakshminarayanan%2C+Balaji&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutoencoder" class="Z3988"></span></span>
</li>
<li id="cite_note-36"><span class="mw-cite-backlink"><b><a href="#cite_ref-36">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1067248974"/><cite id="CITEREFXiaoYanAmit2020" class="citation journal cs1">Xiao, Zhisheng; Yan, Qing; Amit, Yali (2020). <a rel="nofollow" class="external text" href="https://proceedings.neurips.cc/paper/2020/hash/eddea82ad2755b24c4e168c5fc2ebd40-Abstract.html">"Likelihood Regret: An Out-of-Distribution Detection Score For Variational Auto-encoder"</a>. <i>Advances in Neural Information Processing Systems</i>. <b>33</b>. <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="//arxiv.org/abs/2003.02977">2003.02977</a></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Advances+in+Neural+Information+Processing+Systems&amp;rft.atitle=Likelihood+Regret%3A+An+Out-of-Distribution+Detection+Score+For+Variational+Auto-encoder&amp;rft.volume=33&amp;rft.date=2020&amp;rft_id=info%3Aarxiv%2F2003.02977&amp;rft.aulast=Xiao&amp;rft.aufirst=Zhisheng&amp;rft.au=Yan%2C+Qing&amp;rft.au=Amit%2C+Yali&amp;rft_id=https%3A%2F%2Fproceedings.neurips.cc%2Fpaper%2F2020%2Fhash%2Feddea82ad2755b24c4e168c5fc2ebd40-Abstract.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutoencoder" class="Z3988"></span></span>
</li>
<li id="cite_note-37"><span class="mw-cite-backlink"><b><a href="#cite_ref-37">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1067248974"/><cite id="CITEREFTheisShiCunninghamHuszár2017" class="citation arxiv cs1">Theis, Lucas; Shi, Wenzhe; Cunningham, Andrew; Huszár, Ferenc (2017). "Lossy Image Compression with Compressive Autoencoders". <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="//arxiv.org/abs/1703.00395">1703.00395</a></span> [<a rel="nofollow" class="external text" href="//arxiv.org/archive/stat.ML">stat.ML</a>].</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=Lossy+Image+Compression+with+Compressive+Autoencoders&amp;rft.date=2017&amp;rft_id=info%3Aarxiv%2F1703.00395&amp;rft.aulast=Theis&amp;rft.aufirst=Lucas&amp;rft.au=Shi%2C+Wenzhe&amp;rft.au=Cunningham%2C+Andrew&amp;rft.au=Husz%C3%A1r%2C+Ferenc&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutoencoder" class="Z3988"></span></span>
</li>
<li id="cite_note-38"><span class="mw-cite-backlink"><b><a href="#cite_ref-38">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1067248974"/><cite id="CITEREFBalleLaparraSimoncelli2017" class="citation book cs1">Balle, J; Laparra, V; Simoncelli, EP (April 2017). "End-to-end optimized image compression". <i>International Conference on Learning Representations</i>. <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="//arxiv.org/abs/1611.01704">1611.01704</a></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=End-to-end+optimized+image+compression&amp;rft.btitle=International+Conference+on+Learning+Representations&amp;rft.date=2017-04&amp;rft_id=info%3Aarxiv%2F1611.01704&amp;rft.aulast=Balle&amp;rft.aufirst=J&amp;rft.au=Laparra%2C+V&amp;rft.au=Simoncelli%2C+EP&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutoencoder" class="Z3988"></span></span>
</li>
<li id="cite_note-39"><span class="mw-cite-backlink"><b><a href="#cite_ref-39">^</a></b></span> <span class="reference-text">Cho, K. (2013, February). Simple sparsification improves sparse denoising autoencoders in denoising highly corrupted images. In <i>International Conference on Machine Learning</i> (pp. 432-440).</span>
</li>
<li id="cite_note-40"><span class="mw-cite-backlink"><b><a href="#cite_ref-40">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1067248974"/><cite id="CITEREFCho2013" class="citation arxiv cs1">Cho, Kyunghyun (2013). "Boltzmann Machines and Denoising Autoencoders for Image Denoising". <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="//arxiv.org/abs/1301.3468">1301.3468</a></span> [<a rel="nofollow" class="external text" href="//arxiv.org/archive/stat.ML">stat.ML</a>].</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=Boltzmann+Machines+and+Denoising+Autoencoders+for+Image+Denoising&amp;rft.date=2013&amp;rft_id=info%3Aarxiv%2F1301.3468&amp;rft.aulast=Cho&amp;rft.aufirst=Kyunghyun&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutoencoder" class="Z3988"></span></span>
</li>
<li id="cite_note-41"><span class="mw-cite-backlink"><b><a href="#cite_ref-41">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1067248974"/><cite id="CITEREFBuadesCollMorel2005" class="citation journal cs1">Buades, A.; Coll, B.; Morel, J. M. (2005). <a rel="nofollow" class="external text" href="https://hal.archives-ouvertes.fr/hal-00271141">"A Review of Image Denoising Algorithms, with a New One"</a>. <i>Multiscale Modeling &amp; Simulation</i>. <b>4</b> (2): 490–530. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1137%2F040616024">10.1137/040616024</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Multiscale+Modeling+%26+Simulation&amp;rft.atitle=A+Review+of+Image+Denoising+Algorithms%2C+with+a+New+One&amp;rft.volume=4&amp;rft.issue=2&amp;rft.pages=490-530&amp;rft.date=2005&amp;rft_id=info%3Adoi%2F10.1137%2F040616024&amp;rft.aulast=Buades&amp;rft.aufirst=A.&amp;rft.au=Coll%2C+B.&amp;rft.au=Morel%2C+J.+M.&amp;rft_id=https%3A%2F%2Fhal.archives-ouvertes.fr%2Fhal-00271141&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutoencoder" class="Z3988"></span></span>
</li>
<li id="cite_note-42"><span class="mw-cite-backlink"><b><a href="#cite_ref-42">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1067248974"/><cite id="CITEREFGondara2016" class="citation journal cs1">Gondara, Lovedeep (December 2016). "Medical Image Denoising Using Convolutional Denoising Autoencoders". <i>2016 IEEE 16th International Conference on Data Mining Workshops (ICDMW)</i>. Barcelona, Spain: IEEE: 241–246. <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="//arxiv.org/abs/1608.04667">1608.04667</a></span>. <a href="/wiki/Bibcode_(identifier)" class="mw-redirect" title="Bibcode (identifier)">Bibcode</a>:<a rel="nofollow" class="external text" href="https://ui.adsabs.harvard.edu/abs/2016arXiv160804667G">2016arXiv160804667G</a>. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1109%2FICDMW.2016.0041">10.1109/ICDMW.2016.0041</a>. <a href="/wiki/ISBN_(identifier)" class="mw-redirect" title="ISBN (identifier)">ISBN</a>&#160;<a href="/wiki/Special:BookSources/9781509059102" title="Special:BookSources/9781509059102"><bdi>9781509059102</bdi></a>. <a href="/wiki/S2CID_(identifier)" class="mw-redirect" title="S2CID (identifier)">S2CID</a>&#160;<a rel="nofollow" class="external text" href="https://api.semanticscholar.org/CorpusID:14354973">14354973</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=2016+IEEE+16th+International+Conference+on+Data+Mining+Workshops+%28ICDMW%29&amp;rft.atitle=Medical+Image+Denoising+Using+Convolutional+Denoising+Autoencoders&amp;rft.pages=241-246&amp;rft.date=2016-12&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A14354973%23id-name%3DS2CID&amp;rft_id=info%3Abibcode%2F2016arXiv160804667G&amp;rft_id=info%3Aarxiv%2F1608.04667&amp;rft_id=info%3Adoi%2F10.1109%2FICDMW.2016.0041&amp;rft.isbn=9781509059102&amp;rft.aulast=Gondara&amp;rft.aufirst=Lovedeep&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutoencoder" class="Z3988"></span></span>
</li>
<li id="cite_note-43"><span class="mw-cite-backlink"><b><a href="#cite_ref-43">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1067248974"/><cite id="CITEREFZengYuWangLi2017" class="citation journal cs1">Zeng, Kun; Yu, Jun; Wang, Ruxin; Li, Cuihua; Tao, Dacheng (January 2017). "Coupled Deep Autoencoder for Single Image Super-Resolution". <i>IEEE Transactions on Cybernetics</i>. <b>47</b> (1): 27–37. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1109%2FTCYB.2015.2501373">10.1109/TCYB.2015.2501373</a>. <a href="/wiki/ISSN_(identifier)" class="mw-redirect" title="ISSN (identifier)">ISSN</a>&#160;<a rel="nofollow" class="external text" href="//www.worldcat.org/issn/2168-2267">2168-2267</a>. <a href="/wiki/PMID_(identifier)" class="mw-redirect" title="PMID (identifier)">PMID</a>&#160;<a rel="nofollow" class="external text" href="//pubmed.ncbi.nlm.nih.gov/26625442">26625442</a>. <a href="/wiki/S2CID_(identifier)" class="mw-redirect" title="S2CID (identifier)">S2CID</a>&#160;<a rel="nofollow" class="external text" href="https://api.semanticscholar.org/CorpusID:20787612">20787612</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=IEEE+Transactions+on+Cybernetics&amp;rft.atitle=Coupled+Deep+Autoencoder+for+Single+Image+Super-Resolution&amp;rft.volume=47&amp;rft.issue=1&amp;rft.pages=27-37&amp;rft.date=2017-01&amp;rft.issn=2168-2267&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A20787612%23id-name%3DS2CID&amp;rft_id=info%3Apmid%2F26625442&amp;rft_id=info%3Adoi%2F10.1109%2FTCYB.2015.2501373&amp;rft.aulast=Zeng&amp;rft.aufirst=Kun&amp;rft.au=Yu%2C+Jun&amp;rft.au=Wang%2C+Ruxin&amp;rft.au=Li%2C+Cuihua&amp;rft.au=Tao%2C+Dacheng&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutoencoder" class="Z3988"></span></span>
</li>
<li id="cite_note-44"><span class="mw-cite-backlink"><b><a href="#cite_ref-44">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1067248974"/><cite id="CITEREFTzu-HsiSanchezHeshamNasir_M.2017" class="citation journal cs1">Tzu-Hsi, Song; Sanchez, Victor; Hesham, EIDaly; Nasir M., Rajpoot (2017). "Hybrid deep autoencoder with Curvature Gaussian for detection of various types of cells in bone marrow trephine biopsy images". <i>2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017)</i>: 1040–1043. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1109%2FISBI.2017.7950694">10.1109/ISBI.2017.7950694</a>. <a href="/wiki/ISBN_(identifier)" class="mw-redirect" title="ISBN (identifier)">ISBN</a>&#160;<a href="/wiki/Special:BookSources/978-1-5090-1172-8" title="Special:BookSources/978-1-5090-1172-8"><bdi>978-1-5090-1172-8</bdi></a>. <a href="/wiki/S2CID_(identifier)" class="mw-redirect" title="S2CID (identifier)">S2CID</a>&#160;<a rel="nofollow" class="external text" href="https://api.semanticscholar.org/CorpusID:7433130">7433130</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=2017+IEEE+14th+International+Symposium+on+Biomedical+Imaging+%28ISBI+2017%29&amp;rft.atitle=Hybrid+deep+autoencoder+with+Curvature+Gaussian+for+detection+of+various+types+of+cells+in+bone+marrow+trephine+biopsy+images&amp;rft.pages=1040-1043&amp;rft.date=2017&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A7433130%23id-name%3DS2CID&amp;rft_id=info%3Adoi%2F10.1109%2FISBI.2017.7950694&amp;rft.isbn=978-1-5090-1172-8&amp;rft.aulast=Tzu-Hsi&amp;rft.aufirst=Song&amp;rft.au=Sanchez%2C+Victor&amp;rft.au=Hesham%2C+EIDaly&amp;rft.au=Nasir+M.%2C+Rajpoot&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutoencoder" class="Z3988"></span></span>
</li>
<li id="cite_note-45"><span class="mw-cite-backlink"><b><a href="#cite_ref-45">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1067248974"/><cite id="CITEREFXuXiangLiuGilmore2016" class="citation journal cs1">Xu, Jun; Xiang, Lei; Liu, Qingshan; Gilmore, Hannah; Wu, Jianzhong; Tang, Jinghai; Madabhushi, Anant (January 2016). <a rel="nofollow" class="external text" href="//www.ncbi.nlm.nih.gov/pmc/articles/PMC4729702">"Stacked Sparse Autoencoder (SSAE) for Nuclei Detection on Breast Cancer Histopathology Images"</a>. <i>IEEE Transactions on Medical Imaging</i>. <b>35</b> (1): 119–130. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1109%2FTMI.2015.2458702">10.1109/TMI.2015.2458702</a>. <a href="/wiki/PMC_(identifier)" class="mw-redirect" title="PMC (identifier)">PMC</a>&#160;<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="//www.ncbi.nlm.nih.gov/pmc/articles/PMC4729702">4729702</a></span>. <a href="/wiki/PMID_(identifier)" class="mw-redirect" title="PMID (identifier)">PMID</a>&#160;<a rel="nofollow" class="external text" href="//pubmed.ncbi.nlm.nih.gov/26208307">26208307</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=IEEE+Transactions+on+Medical+Imaging&amp;rft.atitle=Stacked+Sparse+Autoencoder+%28SSAE%29+for+Nuclei+Detection+on+Breast+Cancer+Histopathology+Images&amp;rft.volume=35&amp;rft.issue=1&amp;rft.pages=119-130&amp;rft.date=2016-01&amp;rft_id=%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC4729702%23id-name%3DPMC&amp;rft_id=info%3Apmid%2F26208307&amp;rft_id=info%3Adoi%2F10.1109%2FTMI.2015.2458702&amp;rft.aulast=Xu&amp;rft.aufirst=Jun&amp;rft.au=Xiang%2C+Lei&amp;rft.au=Liu%2C+Qingshan&amp;rft.au=Gilmore%2C+Hannah&amp;rft.au=Wu%2C+Jianzhong&amp;rft.au=Tang%2C+Jinghai&amp;rft.au=Madabhushi%2C+Anant&amp;rft_id=%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC4729702&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutoencoder" class="Z3988"></span></span>
</li>
<li id="cite_note-46"><span class="mw-cite-backlink"><b><a href="#cite_ref-46">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1067248974"/><cite id="CITEREFMartinez-MurciaOrtizGorrizRamirez2020" class="citation journal cs1">Martinez-Murcia, Francisco J.; Ortiz, Andres; Gorriz, Juan M.; Ramirez, Javier; Castillo-Barnes, Diego (2020). <a rel="nofollow" class="external text" href="https://doi.org/10.1109%2FJBHI.2019.2914970">"Studying the Manifold Structure of Alzheimer's Disease: A Deep Learning Approach Using Convolutional Autoencoders"</a>. <i>IEEE Journal of Biomedical and Health Informatics</i>. <b>24</b> (1): 17–26. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://doi.org/10.1109%2FJBHI.2019.2914970">10.1109/JBHI.2019.2914970</a></span>. <a href="/wiki/PMID_(identifier)" class="mw-redirect" title="PMID (identifier)">PMID</a>&#160;<a rel="nofollow" class="external text" href="//pubmed.ncbi.nlm.nih.gov/31217131">31217131</a>. <a href="/wiki/S2CID_(identifier)" class="mw-redirect" title="S2CID (identifier)">S2CID</a>&#160;<a rel="nofollow" class="external text" href="https://api.semanticscholar.org/CorpusID:195187846">195187846</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=IEEE+Journal+of+Biomedical+and+Health+Informatics&amp;rft.atitle=Studying+the+Manifold+Structure+of+Alzheimer%27s+Disease%3A+A+Deep+Learning+Approach+Using+Convolutional+Autoencoders&amp;rft.volume=24&amp;rft.issue=1&amp;rft.pages=17-26&amp;rft.date=2020&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A195187846%23id-name%3DS2CID&amp;rft_id=info%3Apmid%2F31217131&amp;rft_id=info%3Adoi%2F10.1109%2FJBHI.2019.2914970&amp;rft.aulast=Martinez-Murcia&amp;rft.aufirst=Francisco+J.&amp;rft.au=Ortiz%2C+Andres&amp;rft.au=Gorriz%2C+Juan+M.&amp;rft.au=Ramirez%2C+Javier&amp;rft.au=Castillo-Barnes%2C+Diego&amp;rft_id=%2F%2Fdoi.org%2F10.1109%252FJBHI.2019.2914970&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutoencoder" class="Z3988"></span></span>
</li>
<li id="cite_note-47"><span class="mw-cite-backlink"><b><a href="#cite_ref-47">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1067248974"/><cite id="CITEREFZhavoronkov2019" class="citation journal cs1">Zhavoronkov, Alex (2019). "Deep learning enables rapid identification of potent DDR1 kinase inhibitors". <i>Nature Biotechnology</i>. <b>37</b> (9): 1038–1040. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1038%2Fs41587-019-0224-x">10.1038/s41587-019-0224-x</a>. <a href="/wiki/PMID_(identifier)" class="mw-redirect" title="PMID (identifier)">PMID</a>&#160;<a rel="nofollow" class="external text" href="//pubmed.ncbi.nlm.nih.gov/31477924">31477924</a>. <a href="/wiki/S2CID_(identifier)" class="mw-redirect" title="S2CID (identifier)">S2CID</a>&#160;<a rel="nofollow" class="external text" href="https://api.semanticscholar.org/CorpusID:201716327">201716327</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Nature+Biotechnology&amp;rft.atitle=Deep+learning+enables+rapid+identification+of+potent+DDR1+kinase+inhibitors&amp;rft.volume=37&amp;rft.issue=9&amp;rft.pages=1038-1040&amp;rft.date=2019&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A201716327%23id-name%3DS2CID&amp;rft_id=info%3Apmid%2F31477924&amp;rft_id=info%3Adoi%2F10.1038%2Fs41587-019-0224-x&amp;rft.aulast=Zhavoronkov&amp;rft.aufirst=Alex&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutoencoder" class="Z3988"></span></span>
</li>
<li id="cite_note-48"><span class="mw-cite-backlink"><b><a href="#cite_ref-48">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1067248974"/><cite id="CITEREFGregory" class="citation magazine cs1">Gregory, Barber. <a rel="nofollow" class="external text" href="https://www.wired.com/story/molecule-designed-ai-exhibits-druglike-qualities/">"A Molecule Designed By AI Exhibits 'Druglike' Qualities"</a>. <i>Wired</i>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Wired&amp;rft.atitle=A+Molecule+Designed+By+AI+Exhibits+%27Druglike%27+Qualities&amp;rft.aulast=Gregory&amp;rft.aufirst=Barber&amp;rft_id=https%3A%2F%2Fwww.wired.com%2Fstory%2Fmolecule-designed-ai-exhibits-druglike-qualities%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutoencoder" class="Z3988"></span></span>
</li>
<li id="cite_note-49"><span class="mw-cite-backlink"><b><a href="#cite_ref-49">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1067248974"/><cite id="CITEREFDeMaityGoelShitole2017" class="citation book cs1">De, Shaunak; Maity, Abhishek; Goel, Vritti; Shitole, Sanjay; Bhattacharya, Avik (2017). "Predicting the popularity of instagram posts for a lifestyle magazine using deep learning". <i>2017 2nd IEEE International Conference on Communication Systems, Computing and IT Applications (CSCITA)</i>. pp.&#160;174–177. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1109%2FCSCITA.2017.8066548">10.1109/CSCITA.2017.8066548</a>. <a href="/wiki/ISBN_(identifier)" class="mw-redirect" title="ISBN (identifier)">ISBN</a>&#160;<a href="/wiki/Special:BookSources/978-1-5090-4381-1" title="Special:BookSources/978-1-5090-4381-1"><bdi>978-1-5090-4381-1</bdi></a>. <a href="/wiki/S2CID_(identifier)" class="mw-redirect" title="S2CID (identifier)">S2CID</a>&#160;<a rel="nofollow" class="external text" href="https://api.semanticscholar.org/CorpusID:35350962">35350962</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Predicting+the+popularity+of+instagram+posts+for+a+lifestyle+magazine+using+deep+learning&amp;rft.btitle=2017+2nd+IEEE+International+Conference+on+Communication+Systems%2C+Computing+and+IT+Applications+%28CSCITA%29&amp;rft.pages=174-177&amp;rft.date=2017&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A35350962%23id-name%3DS2CID&amp;rft_id=info%3Adoi%2F10.1109%2FCSCITA.2017.8066548&amp;rft.isbn=978-1-5090-4381-1&amp;rft.aulast=De&amp;rft.aufirst=Shaunak&amp;rft.au=Maity%2C+Abhishek&amp;rft.au=Goel%2C+Vritti&amp;rft.au=Shitole%2C+Sanjay&amp;rft.au=Bhattacharya%2C+Avik&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutoencoder" class="Z3988"></span></span>
</li>
<li id="cite_note-50"><span class="mw-cite-backlink"><b><a href="#cite_ref-50">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1067248974"/><cite id="CITEREFChoBart_van_MerrienboerBahdanauBengio2014" class="citation arxiv cs1">Cho, Kyunghyun; Bart van Merrienboer; Bahdanau, Dzmitry; Bengio, Yoshua (2014). "On the Properties of Neural Machine Translation: Encoder-Decoder Approaches". <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="//arxiv.org/abs/1409.1259">1409.1259</a></span> [<a rel="nofollow" class="external text" href="//arxiv.org/archive/cs.CL">cs.CL</a>].</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=On+the+Properties+of+Neural+Machine+Translation%3A+Encoder-Decoder+Approaches&amp;rft.date=2014&amp;rft_id=info%3Aarxiv%2F1409.1259&amp;rft.aulast=Cho&amp;rft.aufirst=Kyunghyun&amp;rft.au=Bart+van+Merrienboer&amp;rft.au=Bahdanau%2C+Dzmitry&amp;rft.au=Bengio%2C+Yoshua&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutoencoder" class="Z3988"></span></span>
</li>
<li id="cite_note-51"><span class="mw-cite-backlink"><b><a href="#cite_ref-51">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1067248974"/><cite id="CITEREFSutskeverVinyalsLe2014" class="citation arxiv cs1">Sutskever, Ilya; Vinyals, Oriol; Le, Quoc V. (2014). "Sequence to Sequence Learning with Neural Networks". <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="//arxiv.org/abs/1409.3215">1409.3215</a></span> [<a rel="nofollow" class="external text" href="//arxiv.org/archive/cs.CL">cs.CL</a>].</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=Sequence+to+Sequence+Learning+with+Neural+Networks&amp;rft.date=2014&amp;rft_id=info%3Aarxiv%2F1409.3215&amp;rft.aulast=Sutskever&amp;rft.aufirst=Ilya&amp;rft.au=Vinyals%2C+Oriol&amp;rft.au=Le%2C+Quoc+V.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutoencoder" class="Z3988"></span></span>
</li>
<li id="cite_note-52"><span class="mw-cite-backlink"><b><a href="#cite_ref-52">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1067248974"/><cite id="CITEREFHanKuang2018" class="citation arxiv cs1">Han, Lifeng; Kuang, Shaohui (2018). "Incorporating Chinese Radicals into Neural Machine Translation: Deeper Than Character Level". <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="//arxiv.org/abs/1805.01565">1805.01565</a></span> [<a rel="nofollow" class="external text" href="//arxiv.org/archive/cs.CL">cs.CL</a>].</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=Incorporating+Chinese+Radicals+into+Neural+Machine+Translation%3A+Deeper+Than+Character+Level&amp;rft.date=2018&amp;rft_id=info%3Aarxiv%2F1805.01565&amp;rft.aulast=Han&amp;rft.aufirst=Lifeng&amp;rft.au=Kuang%2C+Shaohui&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutoencoder" class="Z3988"></span></span>
</li>
</ol></div>
<div class="navbox-styles nomobile"><style data-mw-deduplicate="TemplateStyles:r1061467846">.mw-parser-output .navbox{box-sizing:border-box;border:1px solid #a2a9b1;width:100%;clear:both;font-size:88%;text-align:center;padding:1px;margin:1em auto 0}.mw-parser-output .navbox .navbox{margin-top:0}.mw-parser-output .navbox+.navbox,.mw-parser-output .navbox+.navbox-styles+.navbox{margin-top:-1px}.mw-parser-output .navbox-inner,.mw-parser-output .navbox-subgroup{width:100%}.mw-parser-output .navbox-group,.mw-parser-output .navbox-title,.mw-parser-output .navbox-abovebelow{padding:0.25em 1em;line-height:1.5em;text-align:center}.mw-parser-output .navbox-group{white-space:nowrap;text-align:right}.mw-parser-output .navbox,.mw-parser-output .navbox-subgroup{background-color:#fdfdfd}.mw-parser-output .navbox-list{line-height:1.5em;border-color:#fdfdfd}.mw-parser-output .navbox-list-with-group{text-align:left;border-left-width:2px;border-left-style:solid}.mw-parser-output tr+tr>.navbox-abovebelow,.mw-parser-output tr+tr>.navbox-group,.mw-parser-output tr+tr>.navbox-image,.mw-parser-output tr+tr>.navbox-list{border-top:2px solid #fdfdfd}.mw-parser-output .navbox-title{background-color:#ccf}.mw-parser-output .navbox-abovebelow,.mw-parser-output .navbox-group,.mw-parser-output .navbox-subgroup .navbox-title{background-color:#ddf}.mw-parser-output .navbox-subgroup .navbox-group,.mw-parser-output .navbox-subgroup .navbox-abovebelow{background-color:#e6e6ff}.mw-parser-output .navbox-even{background-color:#f7f7f7}.mw-parser-output .navbox-odd{background-color:transparent}.mw-parser-output .navbox .hlist td dl,.mw-parser-output .navbox .hlist td ol,.mw-parser-output .navbox .hlist td ul,.mw-parser-output .navbox td.hlist dl,.mw-parser-output .navbox td.hlist ol,.mw-parser-output .navbox td.hlist ul{padding:0.125em 0}.mw-parser-output .navbox .navbar{display:block;font-size:100%}.mw-parser-output .navbox-title .navbar{float:left;text-align:left;margin-right:0.5em}</style></div><div role="navigation" class="navbox" aria-labelledby="Differentiable_computing" style="padding:3px"><table class="nowraplinks hlist mw-collapsible autocollapse navbox-inner" style="border-spacing:0;background:transparent;color:inherit"><tbody><tr><th scope="col" class="navbox-title" colspan="2"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1063604349"/><div class="navbar plainlinks hlist navbar-mini"><ul><li class="nv-view"><a href="/wiki/Template:Differentiable_computing" title="Template:Differentiable computing"><abbr title="View this template" style=";;background:none transparent;border:none;box-shadow:none;padding:0;">v</abbr></a></li><li class="nv-talk"><a href="/wiki/Template_talk:Differentiable_computing" title="Template talk:Differentiable computing"><abbr title="Discuss this template" style=";;background:none transparent;border:none;box-shadow:none;padding:0;">t</abbr></a></li><li class="nv-edit"><a class="external text" href="https://en.wikipedia.org/w/index.php?title=Template:Differentiable_computing&amp;action=edit"><abbr title="Edit this template" style=";;background:none transparent;border:none;box-shadow:none;padding:0;">e</abbr></a></li></ul></div><div id="Differentiable_computing" style="font-size:114%;margin:0 4em">Differentiable computing</div></th></tr><tr><th scope="row" class="navbox-group" style="width:1%"><a href="/wiki/Differentiable_function" title="Differentiable function">General</a></th><td class="navbox-list-with-group navbox-list navbox-odd" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/Differentiable_programming" title="Differentiable programming">Differentiable programming</a></li>
<li><a href="/wiki/Neural_Turing_machine" title="Neural Turing machine">Neural Turing machine</a></li>
<li><a href="/wiki/Differentiable_neural_computer" title="Differentiable neural computer">Differentiable neural computer</a></li>
<li><a href="/wiki/Automatic_differentiation" title="Automatic differentiation">Automatic differentiation</a></li>
<li><a href="/wiki/Neuromorphic_engineering" title="Neuromorphic engineering">Neuromorphic engineering</a></li>
<li><a href="/wiki/Cable_theory" title="Cable theory">Cable theory</a></li>
<li><a href="/wiki/Pattern_recognition" title="Pattern recognition">Pattern recognition</a></li>
<li><a href="/wiki/Computational_learning_theory" title="Computational learning theory">Computational learning theory</a></li>
<li><a href="/wiki/Tensor_calculus" title="Tensor calculus">Tensor calculus</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">Concepts</th><td class="navbox-list-with-group navbox-list navbox-even" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/Gradient_descent" title="Gradient descent">Gradient descent</a>
<ul><li><a href="/wiki/Stochastic_gradient_descent" title="Stochastic gradient descent">SGD</a></li></ul></li>
<li><a href="/wiki/Cluster_analysis" title="Cluster analysis">Clustering</a></li>
<li><a href="/wiki/Regression_analysis" title="Regression analysis">Regression</a>
<ul><li><a href="/wiki/Overfitting" title="Overfitting">Overfitting</a></li></ul></li>
<li><a href="/wiki/Adversarial_machine_learning" title="Adversarial machine learning">Adversary</a></li>
<li><a href="/wiki/Attention_(machine_learning)" title="Attention (machine learning)">Attention</a></li>
<li><a href="/wiki/Convolution" title="Convolution">Convolution</a></li>
<li><a href="/wiki/Loss_functions_for_classification" title="Loss functions for classification">Loss functions</a></li>
<li><a href="/wiki/Backpropagation" title="Backpropagation">Backpropagation</a></li>
<li><a href="/wiki/Batch_normalization" title="Batch normalization">Normalization</a></li>
<li><a href="/wiki/Activation_function" title="Activation function">Activation</a>
<ul><li><a href="/wiki/Softmax_function" title="Softmax function">Softmax</a></li>
<li><a href="/wiki/Sigmoid_function" title="Sigmoid function">Sigmoid</a></li>
<li><a href="/wiki/Rectifier_(neural_networks)" title="Rectifier (neural networks)">Rectifier</a></li></ul></li>
<li><a href="/wiki/Regularization_(mathematics)" title="Regularization (mathematics)">Regularization</a></li>
<li><a href="/wiki/Training,_validation,_and_test_sets" title="Training, validation, and test sets">Datasets</a>
<ul><li><a href="/wiki/Data_augmentation" title="Data augmentation">Augmentation</a></li></ul></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">Programming languages</th><td class="navbox-list-with-group navbox-list navbox-odd" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/Python_(programming_language)" title="Python (programming language)">Python</a></li>
<li><a href="/wiki/Julia_(programming_language)" title="Julia (programming language)">Julia</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">Application</th><td class="navbox-list-with-group navbox-list navbox-even" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/Machine_learning" title="Machine learning">Machine learning</a></li>
<li><a href="/wiki/Artificial_neural_network" title="Artificial neural network">Artificial neural network</a>
<ul><li><a href="/wiki/Deep_learning" title="Deep learning">Deep learning</a></li></ul></li>
<li><a href="/wiki/Computational_science" title="Computational science">Scientific computing</a></li>
<li><a href="/wiki/Artificial_intelligence" title="Artificial intelligence">Artificial Intelligence</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">Hardware</th><td class="navbox-list-with-group navbox-list navbox-odd" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/Graphcore" title="Graphcore">IPU</a></li>
<li><a href="/wiki/Tensor_Processing_Unit" title="Tensor Processing Unit">TPU</a></li>
<li><a href="/wiki/Vision_processing_unit" title="Vision processing unit">VPU</a></li>
<li><a href="/wiki/Memristor" title="Memristor">Memristor</a></li>
<li><a href="/wiki/SpiNNaker" title="SpiNNaker">SpiNNaker</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">Software library</th><td class="navbox-list-with-group navbox-list navbox-even" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/TensorFlow" title="TensorFlow">TensorFlow</a></li>
<li><a href="/wiki/PyTorch" title="PyTorch">PyTorch</a></li>
<li><a href="/wiki/Keras" title="Keras">Keras</a></li>
<li><a href="/wiki/Theano_(software)" title="Theano (software)">Theano</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">Implementation</th><td class="navbox-list-with-group navbox-list navbox-odd" style="width:100%;padding:0"><div style="padding:0 0.25em"></div><table class="nowraplinks navbox-subgroup" style="border-spacing:0"><tbody><tr><th scope="row" class="navbox-group" style="width:1%">Audio-visual</th><td class="navbox-list-with-group navbox-list navbox-odd" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/AlexNet" title="AlexNet">AlexNet</a></li>
<li><a href="/wiki/WaveNet" title="WaveNet">WaveNet</a></li>
<li><a href="/wiki/Human_image_synthesis" title="Human image synthesis">Human image synthesis</a></li>
<li><a href="/wiki/Handwriting_recognition" title="Handwriting recognition">HWR</a></li>
<li><a href="/wiki/Optical_character_recognition" title="Optical character recognition">OCR</a></li>
<li><a href="/wiki/Speech_synthesis" title="Speech synthesis">Speech synthesis</a></li>
<li><a href="/wiki/Speech_recognition" title="Speech recognition">Speech recognition</a></li>
<li><a href="/wiki/Facial_recognition_system" title="Facial recognition system">Facial recognition</a></li>
<li><a href="/wiki/AlphaFold" title="AlphaFold">AlphaFold</a></li>
<li><a href="/wiki/DALL-E" title="DALL-E">DALL-E</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">Verbal</th><td class="navbox-list-with-group navbox-list navbox-even" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/Word2vec" title="Word2vec">Word2vec</a></li>
<li><a href="/wiki/Transformer_(machine_learning_model)" title="Transformer (machine learning model)">Transformer</a></li>
<li><a href="/wiki/BERT_(language_model)" title="BERT (language model)">BERT</a></li>
<li><a href="/wiki/Neural_machine_translation" title="Neural machine translation">NMT</a></li>
<li><a href="/wiki/Project_Debater" title="Project Debater">Project Debater</a></li>
<li><a href="/wiki/Watson_(computer)" title="Watson (computer)">Watson</a></li>
<li><a href="/wiki/GPT-2" title="GPT-2">GPT-2</a></li>
<li><a href="/wiki/GPT-3" title="GPT-3">GPT-3</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">Decisional</th><td class="navbox-list-with-group navbox-list navbox-odd" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/AlphaGo" title="AlphaGo">AlphaGo</a></li>
<li><a href="/wiki/AlphaZero" title="AlphaZero">AlphaZero</a></li>
<li><a href="/wiki/Q-learning" title="Q-learning">Q-learning</a></li>
<li><a href="/wiki/State%E2%80%93action%E2%80%93reward%E2%80%93state%E2%80%93action" title="State–action–reward–state–action">SARSA</a></li>
<li><a href="/wiki/OpenAI_Five" title="OpenAI Five">OpenAI Five</a></li>
<li><a href="/wiki/Self-driving_car" title="Self-driving car">Self-driving car</a></li>
<li><a href="/wiki/MuZero" title="MuZero">MuZero</a></li>
<li><a href="/wiki/Action_selection" title="Action selection">Action selection</a></li>
<li><a href="/wiki/Robot_control" title="Robot control">Robot control</a></li></ul>
</div></td></tr></tbody></table><div></div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">People</th><td class="navbox-list-with-group navbox-list navbox-even" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/Alex_Graves_(computer_scientist)" title="Alex Graves (computer scientist)">Alex Graves</a></li>
<li><a href="/wiki/Ian_Goodfellow" title="Ian Goodfellow">Ian Goodfellow</a></li>
<li><a href="/wiki/Yoshua_Bengio" title="Yoshua Bengio">Yoshua Bengio</a></li>
<li><a href="/wiki/Geoffrey_Hinton" title="Geoffrey Hinton">Geoffrey Hinton</a></li>
<li><a href="/wiki/Yann_LeCun" title="Yann LeCun">Yann LeCun</a></li>
<li><a href="/wiki/Andrew_Ng" title="Andrew Ng">Andrew Ng</a></li>
<li><a href="/wiki/Demis_Hassabis" title="Demis Hassabis">Demis Hassabis</a></li>
<li><a href="/wiki/David_Silver_(computer_scientist)" title="David Silver (computer scientist)">David Silver</a></li>
<li><a href="/wiki/Fei-Fei_Li" title="Fei-Fei Li">Fei-Fei Li</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">Organizations</th><td class="navbox-list-with-group navbox-list navbox-odd" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/DeepMind" title="DeepMind">DeepMind</a></li>
<li><a href="/wiki/OpenAI" title="OpenAI">OpenAI</a></li>
<li><a href="/wiki/MIT_Computer_Science_and_Artificial_Intelligence_Laboratory" title="MIT Computer Science and Artificial Intelligence Laboratory">MIT CSAIL</a></li>
<li><a href="/wiki/Mila_(research_institute)" title="Mila (research institute)">Mila</a></li>
<li><a href="/wiki/Google_Brain" title="Google Brain">Google Brain</a></li>
<li><a href="https://fr.wikipedia.org/wiki/Facebook_Artificial_Intelligence_Research" class="extiw" title="fr:Facebook Artificial Intelligence Research">FAIR</a></li></ul>
</div></td></tr><tr><td class="navbox-abovebelow" colspan="2"><div>
<ul><li><a href="/wiki/File:Symbol_portal_class.svg" class="image" title="Portal"><img alt="" src="//upload.wikimedia.org/wikipedia/en/thumb/e/e2/Symbol_portal_class.svg/16px-Symbol_portal_class.svg.png" decoding="async" width="16" height="16" class="noviewer" srcset="//upload.wikimedia.org/wikipedia/en/thumb/e/e2/Symbol_portal_class.svg/23px-Symbol_portal_class.svg.png 1.5x, //upload.wikimedia.org/wikipedia/en/thumb/e/e2/Symbol_portal_class.svg/31px-Symbol_portal_class.svg.png 2x" data-file-width="180" data-file-height="185" /></a> Portals
<ul><li><a href="/wiki/Portal:Computer_programming" title="Portal:Computer programming">Computer programming</a></li>
<li><a href="/wiki/Portal:Technology" title="Portal:Technology">Technology</a></li></ul></li>
<li><img alt="" src="//upload.wikimedia.org/wikipedia/en/thumb/9/96/Symbol_category_class.svg/16px-Symbol_category_class.svg.png" decoding="async" title="Category" width="16" height="16" class="noviewer" srcset="//upload.wikimedia.org/wikipedia/en/thumb/9/96/Symbol_category_class.svg/23px-Symbol_category_class.svg.png 1.5x, //upload.wikimedia.org/wikipedia/en/thumb/9/96/Symbol_category_class.svg/31px-Symbol_category_class.svg.png 2x" data-file-width="180" data-file-height="185" /> Category
<ul><li><a href="/wiki/Category:Artificial_neural_networks" title="Category:Artificial neural networks">Artificial neural networks</a></li>
<li><a href="/wiki/Category:Machine_learning" title="Category:Machine learning">Machine learning</a></li></ul></li></ul>
</div></td></tr></tbody></table></div>
<div class="navbox-styles nomobile"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1061467846"/></div><div role="navigation" class="navbox" aria-labelledby="Noise_(physics_and_telecommunications)" style="padding:3px"><table class="nowraplinks mw-collapsible autocollapse navbox-inner" style="border-spacing:0;background:transparent;color:inherit"><tbody><tr><th scope="col" class="navbox-title" colspan="2"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1063604349"/><div class="navbar plainlinks hlist navbar-mini"><ul><li class="nv-view"><a href="/wiki/Template:Noise" title="Template:Noise"><abbr title="View this template" style=";;background:none transparent;border:none;box-shadow:none;padding:0;">v</abbr></a></li><li class="nv-talk"><a href="/wiki/Template_talk:Noise" title="Template talk:Noise"><abbr title="Discuss this template" style=";;background:none transparent;border:none;box-shadow:none;padding:0;">t</abbr></a></li><li class="nv-edit"><a class="external text" href="https://en.wikipedia.org/w/index.php?title=Template:Noise&amp;action=edit"><abbr title="Edit this template" style=";;background:none transparent;border:none;box-shadow:none;padding:0;">e</abbr></a></li></ul></div><div id="Noise_(physics_and_telecommunications)" style="font-size:114%;margin:0 4em"><a href="/wiki/Noise_(spectral_phenomenon)" title="Noise (spectral phenomenon)">Noise</a> (physics and telecommunications)</div></th></tr><tr><th scope="row" class="navbox-group" style="width:1%">General</th><td class="navbox-list-with-group navbox-list navbox-odd hlist" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/Acoustic_quieting" title="Acoustic quieting">Acoustic quieting</a></li>
<li><a href="/wiki/Distortion" title="Distortion">Distortion</a></li>
<li><a href="/wiki/Active_noise_control" title="Active noise control">Noise cancellation</a></li>
<li><a href="/wiki/Noise_control" title="Noise control">Noise control</a></li>
<li><a href="/wiki/Noise_measurement" title="Noise measurement">Noise measurement</a></li>
<li><a href="/wiki/Noise_power" title="Noise power">Noise power</a></li>
<li><a href="/wiki/Noise_reduction" title="Noise reduction">Noise reduction</a></li>
<li><a href="/wiki/Noise_temperature" title="Noise temperature">Noise temperature</a></li>
<li><a href="/wiki/Phase_distortion" title="Phase distortion">Phase distortion</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">Noise in...</th><td class="navbox-list-with-group navbox-list navbox-even hlist" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/Noise" title="Noise">Audio</a></li>
<li><a href="/wiki/Architectural_acoustics" title="Architectural acoustics">Buildings</a></li>
<li><a href="/wiki/Noise_(electronics)" title="Noise (electronics)">Electronics</a></li>
<li><a href="/wiki/Noise_pollution" title="Noise pollution">Environment</a></li>
<li><a href="/wiki/Noise_regulation" title="Noise regulation">Government regulation</a></li>
<li><a href="/wiki/Health_effects_from_noise" title="Health effects from noise">Human health</a></li>
<li><a href="/wiki/Image_noise" title="Image noise">Images</a></li>
<li><a href="/wiki/Noise_(radio)" class="mw-redirect" title="Noise (radio)">Radio</a></li>
<li><a href="/wiki/Soundproofing" title="Soundproofing">Rooms</a></li>
<li><a href="/wiki/Noise_and_vibration_on_maritime_vessels" title="Noise and vibration on maritime vessels">Ships</a></li>
<li><a href="/wiki/Sound_masking" title="Sound masking">Sound masking</a></li>
<li><a href="/wiki/Noise_barrier" title="Noise barrier">Transportation</a></li>
<li><a href="/wiki/Noise_(video)" title="Noise (video)">Video</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">Class of noise</th><td class="navbox-list-with-group navbox-list navbox-odd hlist" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/Additive_white_Gaussian_noise" title="Additive white Gaussian noise">Additive white Gaussian noise</a> (AWGN)</li>
<li><a href="/wiki/Atmospheric_noise" title="Atmospheric noise">Atmospheric noise</a></li>
<li><a href="/wiki/Background_noise" title="Background noise">Background noise</a></li>
<li><a href="/wiki/Brownian_noise" title="Brownian noise">Brownian noise</a></li>
<li><a href="/wiki/Burst_noise" title="Burst noise">Burst noise</a></li>
<li><a href="/wiki/Cosmic_noise" title="Cosmic noise">Cosmic noise</a></li>
<li><a href="/wiki/Flicker_noise" title="Flicker noise">Flicker noise</a></li>
<li><a href="/wiki/Gaussian_noise" title="Gaussian noise">Gaussian noise</a></li>
<li><a href="/wiki/Grey_noise" title="Grey noise">Grey noise</a></li>
<li><a href="/wiki/Jitter" title="Jitter">Jitter</a></li>
<li><a href="/wiki/Johnson%E2%80%93Nyquist_noise" title="Johnson–Nyquist noise">Johnson–Nyquist noise</a> (thermal noise)</li>
<li><a href="/wiki/Pink_noise" title="Pink noise">Pink noise</a></li>
<li><a href="/wiki/Quantization_error" class="mw-redirect" title="Quantization error">Quantization error</a> (or q. noise)</li>
<li><a href="/wiki/Shot_noise" title="Shot noise">Shot noise</a></li>
<li><a href="/wiki/White_noise" title="White noise">White noise</a></li>
<li>Coherent noise
<ul><li><a href="/wiki/Value_noise" title="Value noise">Value noise</a></li>
<li><a href="/wiki/Gradient_noise" title="Gradient noise">Gradient noise</a></li>
<li><a href="/wiki/Worley_noise" title="Worley noise">Worley noise</a></li></ul></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">Engineering <br />terms</th><td class="navbox-list-with-group navbox-list navbox-even hlist" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/Channel_noise_level" title="Channel noise level">Channel noise level</a></li>
<li><a href="/wiki/Circuit_noise_level" title="Circuit noise level">Circuit noise level</a></li>
<li><a href="/wiki/Effective_input_noise_temperature" title="Effective input noise temperature">Effective input noise temperature</a></li>
<li><a href="/wiki/Equivalent_noise_resistance" title="Equivalent noise resistance">Equivalent noise resistance</a></li>
<li><a href="/wiki/Equivalent_pulse_code_modulation_noise" title="Equivalent pulse code modulation noise">Equivalent pulse code modulation noise</a></li>
<li><a href="/wiki/Impulse_noise_(audio)" class="mw-redirect" title="Impulse noise (audio)">Impulse noise (audio)</a></li>
<li><a href="/wiki/Noise_figure" title="Noise figure">Noise figure</a></li>
<li><a href="/wiki/Noise_floor" title="Noise floor">Noise floor</a></li>
<li><a href="/wiki/Noise_shaping" title="Noise shaping">Noise shaping</a></li>
<li><a href="/wiki/Noise_spectral_density" title="Noise spectral density">Noise spectral density</a></li>
<li><a href="/wiki/Noise,_vibration,_and_harshness" title="Noise, vibration, and harshness">Noise, vibration, and harshness</a> (NVH)</li>
<li><a href="/wiki/Phase_noise" title="Phase noise">Phase noise</a></li>
<li><a href="/wiki/Pseudorandom_noise" title="Pseudorandom noise">Pseudorandom noise</a></li>
<li><a href="/wiki/Statistical_noise" class="mw-redirect" title="Statistical noise">Statistical noise</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">Ratios</th><td class="navbox-list-with-group navbox-list navbox-odd hlist" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/Carrier-to-noise_ratio" title="Carrier-to-noise ratio">Carrier-to-noise ratio</a> (<i>C</i>/<i>N</i>)</li>
<li><a href="/wiki/Carrier-to-receiver_noise_density" class="mw-redirect" title="Carrier-to-receiver noise density">Carrier-to-receiver noise density</a> (<i>C</i>/<i>kT</i>)</li>
<li><i><a href="/wiki/DBrnC" class="mw-redirect" title="DBrnC">dBrnC</a></i></li>
<li><i><a href="/wiki/Eb/N0" title="Eb/N0">E<sub>b</sub>/N<sub>0</sub></a></i> (energy per bit to noise density)</li>
<li><i><a href="/wiki/Eb/N0#Relation_to_Es.2FN0" title="Eb/N0">E<sub>s</sub>/N<sub>0</sub></a></i> (energy per symbol to noise density)</li>
<li><a href="/wiki/Modulation_error_ratio" title="Modulation error ratio">Modulation error ratio</a> (<i>MER</i>)</li>
<li><a href="/wiki/SINAD" title="SINAD">Signal, noise and distortion</a> (<i>SINAD</i>)</li>
<li><a href="/wiki/Signal-to-interference_ratio" title="Signal-to-interference ratio">Signal-to-interference ratio</a> (<i>S</i>/<i>I</i>)</li>
<li><a href="/wiki/Signal-to-noise_ratio" title="Signal-to-noise ratio">Signal-to-noise ratio</a> (<i>S</i>/<i>N</i>,&#160;<i>SNR</i>)</li>
<li><a href="/wiki/Signal-to-noise_ratio_(imaging)" title="Signal-to-noise ratio (imaging)">Signal-to-noise ratio (imaging)</a></li>
<li><a href="/wiki/Signal-to-interference-plus-noise_ratio" title="Signal-to-interference-plus-noise ratio">Signal-to-interference-plus-noise ratio</a> (<i>SINR</i>)</li>
<li><a href="/wiki/Signal-to-quantization-noise_ratio" title="Signal-to-quantization-noise ratio">Signal-to-quantization-noise ratio</a> (<i>SQNR</i>)</li>
<li><a href="/wiki/Contrast-to-noise_ratio" title="Contrast-to-noise ratio">Contrast-to-noise ratio</a> (<i>CNR</i>)</li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">Related topics</th><td class="navbox-list-with-group navbox-list navbox-even hlist" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/List_of_noise_topics" title="List of noise topics">List of noise topics</a></li>
<li><a href="/wiki/Acoustics" title="Acoustics">Acoustics</a></li>
<li><a href="/wiki/Colors_of_noise" title="Colors of noise">Colors of noise</a></li>
<li><a href="/wiki/Interference_(communication)" title="Interference (communication)">Interference (communication)</a></li>
<li><a href="/wiki/Noise_generator" title="Noise generator">Noise generator</a></li>
<li><a href="/wiki/Spectrum_analyzer" title="Spectrum analyzer">Spectrum analyzer</a></li>
<li><a href="/wiki/Thermal_radiation" title="Thermal radiation">Thermal radiation</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">Denoise <br />methods</th><td class="navbox-list-with-group navbox-list navbox-odd hlist" style="width:100%;padding:0"><div style="padding:0 0.25em"></div><table class="nowraplinks navbox-subgroup" style="border-spacing:0"><tbody><tr><th scope="row" class="navbox-group" style="width:1%">General</th><td class="navbox-list-with-group navbox-list navbox-odd" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/Low-pass_filter" title="Low-pass filter">Low-pass filter</a></li>
<li><a href="/wiki/Median_filter" title="Median filter">Median filter</a></li>
<li><a href="/wiki/Total_variation_denoising" title="Total variation denoising">Total variation denoising</a></li>
<li><a href="/wiki/Wavelet#Wavelet_denoising" title="Wavelet">Wavelet denoising</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">2D (Image)</th><td class="navbox-list-with-group navbox-list navbox-even" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/Gaussian_blur" title="Gaussian blur">Gaussian blur</a></li>
<li><a href="/wiki/Anisotropic_diffusion" title="Anisotropic diffusion">Anisotropic diffusion</a></li>
<li><a href="/wiki/Bilateral_filter" title="Bilateral filter">Bilateral filter</a></li>
<li><a href="/wiki/Non-local_means" title="Non-local means">Non-local means</a></li>
<li><a href="/wiki/Block-matching_and_3D_filtering" title="Block-matching and 3D filtering">Block-matching and 3D filtering</a> (BM3D)</li>
<li><a href="/wiki/Shrinkage_Fields_(image_restoration)" title="Shrinkage Fields (image restoration)">Shrinkage Fields</a></li>
<li><a href="/wiki/Autoencoder#Denoising_autoencoder_(DAE)" title="Autoencoder">Denoising autoencoder</a> (DAE)</li>
<li><a href="/wiki/Deep_Image_Prior" title="Deep Image Prior">Deep Image Prior</a></li></ul>
</div></td></tr></tbody></table><div></div></td></tr></tbody></table></div>
<!-- 
NewPP limit report
Parsed by mw1405
Cached time: 20220222162955
Cache expiry: 1814400
Reduced expiry: false
Complications: [vary‐revision‐sha1]
CPU time usage: 1.008 seconds
Real time usage: 1.221 seconds
Preprocessor visited node count: 3708/1000000
Post‐expand include size: 165644/2097152 bytes
Template argument size: 2452/2097152 bytes
Highest expansion depth: 12/100
Expensive parser function count: 4/500
Unstrip recursion depth: 1/20
Unstrip post‐expand size: 157428/5000000 bytes
Lua time usage: 0.488/10.000 seconds
Lua memory usage: 7265990/52428800 bytes
Number of Wikibase entities loaded: 0/400
-->
<!--
Transclusion expansion time report (%,ms,calls,template)
100.00%  833.872      1 -total
 54.29%  452.728      1 Template:Reflist
 28.76%  239.809     22 Template:Cite_journal
 10.63%   88.640      1 Template:Short_description
 10.53%   87.810      1 Template:Machine_learning
 10.20%   85.057      1 Template:Sidebar_with_collapsible_lists
  9.83%   82.011      4 Template:Navbox
  8.75%   72.981      1 Template:Differentiable_computing
  8.73%   72.823     12 Template:Cite_arXiv
  5.49%   45.795      1 Template:Distinguish
-->

<!-- Saved in parser cache with key enwiki:pcache:idhash:6836612-0!canonical and timestamp 20220222162954 and revision id 1068573765. Serialized with JSON.
 -->
</div><noscript><img src="//en.wikipedia.org/wiki/Special:CentralAutoLogin/start?type=1x1" alt="" title="" width="1" height="1" style="border: none; position: absolute;" /></noscript>
<div class="printfooter">Retrieved from "<a dir="ltr" href="https://en.wikipedia.org/w/index.php?title=Autoencoder&amp;oldid=1068573765">https://en.wikipedia.org/w/index.php?title=Autoencoder&amp;oldid=1068573765</a>"</div></div>
		<div id="catlinks" class="catlinks" data-mw="interface"><div id="mw-normal-catlinks" class="mw-normal-catlinks"><a href="/wiki/Help:Category" title="Help:Category">Categories</a>: <ul><li><a href="/wiki/Category:Artificial_neural_networks" title="Category:Artificial neural networks">Artificial neural networks</a></li><li><a href="/wiki/Category:Unsupervised_learning" title="Category:Unsupervised learning">Unsupervised learning</a></li><li><a href="/wiki/Category:Dimension_reduction" title="Category:Dimension reduction">Dimension reduction</a></li></ul></div><div id="mw-hidden-catlinks" class="mw-hidden-catlinks mw-hidden-cats-hidden">Hidden categories: <ul><li><a href="/wiki/Category:CS1_errors:_missing_periodical" title="Category:CS1 errors: missing periodical">CS1 errors: missing periodical</a></li><li><a href="/wiki/Category:Articles_with_short_description" title="Category:Articles with short description">Articles with short description</a></li><li><a href="/wiki/Category:Short_description_matches_Wikidata" title="Category:Short description matches Wikidata">Short description matches Wikidata</a></li><li><a href="/wiki/Category:Use_dmy_dates_from_March_2020" title="Category:Use dmy dates from March 2020">Use dmy dates from March 2020</a></li></ul></div></div>
	</div>
</div>
<div id='mw-data-after-content'>
	<div class="read-more-container"></div>
</div>

<div id="mw-navigation">
	<h2>Navigation menu</h2>
	<div id="mw-head">
		
<nav id="p-personal" class="mw-portlet mw-portlet-personal vector-user-menu-legacy vector-menu" aria-labelledby="p-personal-label" role="navigation" 
	 >
	<label id="p-personal-label" aria-label="" class="vector-menu-heading">
		<span class="vector-menu-heading-label">Personal tools</span>
	</label>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list"><li id="pt-anonuserpage" class="mw-list-item"><span>Not logged in</span></li><li id="pt-anontalk" class="mw-list-item"><a href="/wiki/Special:MyTalk" title="Discussion about edits from this IP address [n]" accesskey="n"><span>Talk</span></a></li><li id="pt-anoncontribs" class="mw-list-item"><a href="/wiki/Special:MyContributions" title="A list of edits made from this IP address [y]" accesskey="y"><span>Contributions</span></a></li><li id="pt-createaccount" class="mw-list-item"><a href="/w/index.php?title=Special:CreateAccount&amp;returnto=Autoencoder&amp;returntoquery=lxml%3D" title="You are encouraged to create an account and log in; however, it is not mandatory"><span>Create account</span></a></li><li id="pt-login" class="mw-list-item"><a href="/w/index.php?title=Special:UserLogin&amp;returnto=Autoencoder&amp;returntoquery=lxml%3D" title="You&#039;re encouraged to log in; however, it&#039;s not mandatory. [o]" accesskey="o"><span>Log in</span></a></li></ul>
		
	</div>
</nav>

		<div id="left-navigation">
			
<nav id="p-namespaces" class="mw-portlet mw-portlet-namespaces vector-menu vector-menu-tabs" aria-labelledby="p-namespaces-label" role="navigation" 
	 >
	<label id="p-namespaces-label" aria-label="" class="vector-menu-heading">
		<span class="vector-menu-heading-label">Namespaces</span>
	</label>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list"><li id="ca-nstab-main" class="selected mw-list-item"><a href="/wiki/Autoencoder" title="View the content page [c]" accesskey="c"><span>Article</span></a></li><li id="ca-talk" class="mw-list-item"><a href="/wiki/Talk:Autoencoder" rel="discussion" title="Discuss improvements to the content page [t]" accesskey="t"><span>Talk</span></a></li></ul>
		
	</div>
</nav>

			
<nav id="p-variants" class="mw-portlet mw-portlet-variants emptyPortlet vector-menu-dropdown-noicon vector-menu vector-menu-dropdown" aria-labelledby="p-variants-label" role="navigation" 
	 >
	<input type="checkbox"
		id="p-variants-checkbox"
		role="button"
		aria-haspopup="true"
		data-event-name="ui.dropdown-p-variants"
		class="vector-menu-checkbox" aria-labelledby="p-variants-label" />
	<label id="p-variants-label" aria-label="Change language variant" class="vector-menu-heading">
		<span class="vector-menu-heading-label">English</span>
			<span class="vector-menu-checkbox-expanded">expanded</span>
			<span class="vector-menu-checkbox-collapsed">collapsed</span>
	</label>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list"></ul>
		
	</div>
</nav>

		</div>
		<div id="right-navigation">
			
<nav id="p-views" class="mw-portlet mw-portlet-views vector-menu vector-menu-tabs" aria-labelledby="p-views-label" role="navigation" 
	 >
	<label id="p-views-label" aria-label="" class="vector-menu-heading">
		<span class="vector-menu-heading-label">Views</span>
	</label>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list"><li id="ca-view" class="selected mw-list-item"><a href="/wiki/Autoencoder"><span>Read</span></a></li><li id="ca-edit" class="mw-list-item"><a href="/w/index.php?title=Autoencoder&amp;action=edit" title="Edit this page [e]" accesskey="e"><span>Edit</span></a></li><li id="ca-history" class="mw-list-item"><a href="/w/index.php?title=Autoencoder&amp;action=history" title="Past revisions of this page [h]" accesskey="h"><span>View history</span></a></li></ul>
		
	</div>
</nav>

			
<nav id="p-cactions" class="mw-portlet mw-portlet-cactions emptyPortlet vector-menu-dropdown-noicon vector-menu vector-menu-dropdown" aria-labelledby="p-cactions-label" role="navigation"  title="More options"
	 >
	<input type="checkbox"
		id="p-cactions-checkbox"
		role="button"
		aria-haspopup="true"
		data-event-name="ui.dropdown-p-cactions"
		class="vector-menu-checkbox" aria-labelledby="p-cactions-label" />
	<label id="p-cactions-label" aria-label="" class="vector-menu-heading">
		<span class="vector-menu-heading-label">More</span>
			<span class="vector-menu-checkbox-expanded">expanded</span>
			<span class="vector-menu-checkbox-collapsed">collapsed</span>
	</label>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list"></ul>
		
	</div>
</nav>

			
<div id="p-search" role="search" class="vector-search-box-vue  vector-search-box-show-thumbnail vector-search-box-auto-expand-width vector-search-box">
	<div>
			<h3 >
				<label for="searchInput">Search</label>
			</h3>
		<form action="/w/index.php" id="searchform"
			class="vector-search-box-form">
			<div id="simpleSearch"
				class="vector-search-box-inner"
				 data-search-loc="header-navigation">
				<input class="vector-search-box-input"
					 type="search" name="search" placeholder="Search Wikipedia" aria-label="Search Wikipedia" autocapitalize="sentences" title="Search Wikipedia [f]" accesskey="f" id="searchInput"
				/>
				<input type="hidden" name="title" value="Special:Search"/>
				<input id="mw-searchButton"
					 class="searchButton mw-fallbackSearchButton" type="submit" name="fulltext" title="Search Wikipedia for this text" value="Search" />
				<input id="searchButton"
					 class="searchButton" type="submit" name="go" title="Go to a page with this exact name if it exists" value="Go" />
			</div>
		</form>
	</div>
</div>

		</div>
	</div>
	

<div id="mw-panel">
	<div id="p-logo" role="banner">
		<a class="mw-wiki-logo" href="/wiki/Main_Page"
			title="Visit the main page"></a>
	</div>
	
<nav id="p-navigation" class="mw-portlet mw-portlet-navigation vector-menu vector-menu-portal portal" aria-labelledby="p-navigation-label" role="navigation" 
	 >
	<label id="p-navigation-label" aria-label="" class="vector-menu-heading">
		<span class="vector-menu-heading-label">Navigation</span>
	</label>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list"><li id="n-mainpage-description" class="mw-list-item"><a href="/wiki/Main_Page" icon="home" title="Visit the main page [z]" accesskey="z"><span>Main page</span></a></li><li id="n-contents" class="mw-list-item"><a href="/wiki/Wikipedia:Contents" title="Guides to browsing Wikipedia"><span>Contents</span></a></li><li id="n-currentevents" class="mw-list-item"><a href="/wiki/Portal:Current_events" title="Articles related to current events"><span>Current events</span></a></li><li id="n-randompage" class="mw-list-item"><a href="/wiki/Special:Random" icon="die" title="Visit a randomly selected article [x]" accesskey="x"><span>Random article</span></a></li><li id="n-aboutsite" class="mw-list-item"><a href="/wiki/Wikipedia:About" title="Learn about Wikipedia and how it works"><span>About Wikipedia</span></a></li><li id="n-contactpage" class="mw-list-item"><a href="//en.wikipedia.org/wiki/Wikipedia:Contact_us" title="How to contact Wikipedia"><span>Contact us</span></a></li><li id="n-sitesupport" class="mw-list-item"><a href="https://donate.wikimedia.org/wiki/Special:FundraiserRedirector?utm_source=donate&amp;utm_medium=sidebar&amp;utm_campaign=C13_en.wikipedia.org&amp;uselang=en" title="Support us by donating to the Wikimedia Foundation"><span>Donate</span></a></li></ul>
		
	</div>
</nav>

	
<nav id="p-interaction" class="mw-portlet mw-portlet-interaction vector-menu vector-menu-portal portal" aria-labelledby="p-interaction-label" role="navigation" 
	 >
	<label id="p-interaction-label" aria-label="" class="vector-menu-heading">
		<span class="vector-menu-heading-label">Contribute</span>
	</label>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list"><li id="n-help" class="mw-list-item"><a href="/wiki/Help:Contents" icon="help" title="Guidance on how to use and edit Wikipedia"><span>Help</span></a></li><li id="n-introduction" class="mw-list-item"><a href="/wiki/Help:Introduction" title="Learn how to edit Wikipedia"><span>Learn to edit</span></a></li><li id="n-portal" class="mw-list-item"><a href="/wiki/Wikipedia:Community_portal" title="The hub for editors"><span>Community portal</span></a></li><li id="n-recentchanges" class="mw-list-item"><a href="/wiki/Special:RecentChanges" icon="recentChanges" title="A list of recent changes to Wikipedia [r]" accesskey="r"><span>Recent changes</span></a></li><li id="n-upload" class="mw-list-item"><a href="/wiki/Wikipedia:File_Upload_Wizard" title="Add images or other media for use on Wikipedia"><span>Upload file</span></a></li></ul>
		
	</div>
</nav>

<nav id="p-tb" class="mw-portlet mw-portlet-tb vector-menu vector-menu-portal portal" aria-labelledby="p-tb-label" role="navigation" 
	 >
	<label id="p-tb-label" aria-label="" class="vector-menu-heading">
		<span class="vector-menu-heading-label">Tools</span>
	</label>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list"><li id="t-whatlinkshere" class="mw-list-item"><a href="/wiki/Special:WhatLinksHere/Autoencoder" title="List of all English Wikipedia pages containing links to this page [j]" accesskey="j"><span>What links here</span></a></li><li id="t-recentchangeslinked" class="mw-list-item"><a href="/wiki/Special:RecentChangesLinked/Autoencoder" rel="nofollow" title="Recent changes in pages linked from this page [k]" accesskey="k"><span>Related changes</span></a></li><li id="t-upload" class="mw-list-item"><a href="/wiki/Wikipedia:File_Upload_Wizard" title="Upload files [u]" accesskey="u"><span>Upload file</span></a></li><li id="t-specialpages" class="mw-list-item"><a href="/wiki/Special:SpecialPages" title="A list of all special pages [q]" accesskey="q"><span>Special pages</span></a></li><li id="t-permalink" class="mw-list-item"><a href="/w/index.php?title=Autoencoder&amp;oldid=1068573765" title="Permanent link to this revision of this page"><span>Permanent link</span></a></li><li id="t-info" class="mw-list-item"><a href="/w/index.php?title=Autoencoder&amp;action=info" title="More information about this page"><span>Page information</span></a></li><li id="t-cite" class="mw-list-item"><a href="/w/index.php?title=Special:CiteThisPage&amp;page=Autoencoder&amp;id=1068573765&amp;wpFormIdentifier=titleform" title="Information on how to cite this page"><span>Cite this page</span></a></li><li id="t-wikibase" class="mw-list-item"><a href="https://www.wikidata.org/wiki/Special:EntityPage/Q786435" title="Structured data on this page hosted by Wikidata [g]" accesskey="g"><span>Wikidata item</span></a></li></ul>
		
	</div>
</nav>

<nav id="p-coll-print_export" class="mw-portlet mw-portlet-coll-print_export vector-menu vector-menu-portal portal" aria-labelledby="p-coll-print_export-label" role="navigation" 
	 >
	<label id="p-coll-print_export-label" aria-label="" class="vector-menu-heading">
		<span class="vector-menu-heading-label">Print/export</span>
	</label>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list"><li id="coll-download-as-rl" class="mw-list-item"><a href="/w/index.php?title=Special:DownloadAsPdf&amp;page=Autoencoder&amp;action=show-download-screen" title="Download this page as a PDF file"><span>Download as PDF</span></a></li><li id="t-print" class="mw-list-item"><a href="/w/index.php?title=Autoencoder&amp;printable=yes" title="Printable version of this page [p]" accesskey="p"><span>Printable version</span></a></li></ul>
		
	</div>
</nav>

	
<nav id="p-lang" class="mw-portlet mw-portlet-lang vector-menu vector-menu-portal portal" aria-labelledby="p-lang-label" role="navigation" 
	 >
	<label id="p-lang-label" aria-label="" class="vector-menu-heading">
		<span class="vector-menu-heading-label">Languages</span>
	</label>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list"><li class="interlanguage-link interwiki-ca mw-list-item"><a href="https://ca.wikipedia.org/wiki/Autoencoder" title="Autoencoder – Catalan" lang="ca" hreflang="ca" class="interlanguage-link-target"><span>Català</span></a></li><li class="interlanguage-link interwiki-de mw-list-item"><a href="https://de.wikipedia.org/wiki/Autoencoder" title="Autoencoder – German" lang="de" hreflang="de" class="interlanguage-link-target"><span>Deutsch</span></a></li><li class="interlanguage-link interwiki-fa mw-list-item"><a href="https://fa.wikipedia.org/wiki/%D8%AE%D9%88%D8%AF%D8%B1%D9%85%D8%B2%DA%AF%D8%B0%D8%A7%D8%B1" title="خودرمزگذار – Persian" lang="fa" hreflang="fa" class="interlanguage-link-target"><span>فارسی</span></a></li><li class="interlanguage-link interwiki-fr mw-list-item"><a href="https://fr.wikipedia.org/wiki/Auto-encodeur" title="Auto-encodeur – French" lang="fr" hreflang="fr" class="interlanguage-link-target"><span>Français</span></a></li><li class="interlanguage-link interwiki-ko mw-list-item"><a href="https://ko.wikipedia.org/wiki/%EC%98%A4%ED%86%A0%EC%9D%B8%EC%BD%94%EB%8D%94" title="오토인코더 – Korean" lang="ko" hreflang="ko" class="interlanguage-link-target"><span>한국어</span></a></li><li class="interlanguage-link interwiki-ja mw-list-item"><a href="https://ja.wikipedia.org/wiki/%E3%82%AA%E3%83%BC%E3%83%88%E3%82%A8%E3%83%B3%E3%82%B3%E3%83%BC%E3%83%80" title="オートエンコーダ – Japanese" lang="ja" hreflang="ja" class="interlanguage-link-target"><span>日本語</span></a></li><li class="interlanguage-link interwiki-ru mw-list-item"><a href="https://ru.wikipedia.org/wiki/%D0%90%D0%B2%D1%82%D0%BE%D0%BA%D0%BE%D0%B4%D0%B8%D1%80%D0%BE%D0%B2%D1%89%D0%B8%D0%BA" title="Автокодировщик – Russian" lang="ru" hreflang="ru" class="interlanguage-link-target"><span>Русский</span></a></li><li class="interlanguage-link interwiki-tr mw-list-item"><a href="https://tr.wikipedia.org/wiki/Otokodlay%C4%B1c%C4%B1" title="Otokodlayıcı – Turkish" lang="tr" hreflang="tr" class="interlanguage-link-target"><span>Türkçe</span></a></li><li class="interlanguage-link interwiki-uk mw-list-item"><a href="https://uk.wikipedia.org/wiki/%D0%90%D0%B2%D1%82%D0%BE%D0%BA%D0%BE%D0%B4%D1%83%D0%B2%D0%B0%D0%BB%D1%8C%D0%BD%D0%B8%D0%BA" title="Автокодувальник – Ukrainian" lang="uk" hreflang="uk" class="interlanguage-link-target"><span>Українська</span></a></li><li class="interlanguage-link interwiki-vi mw-list-item"><a href="https://vi.wikipedia.org/wiki/B%E1%BB%99_t%E1%BB%B1_m%C3%A3_h%C3%B3a" title="Bộ tự mã hóa – Vietnamese" lang="vi" hreflang="vi" class="interlanguage-link-target"><span>Tiếng Việt</span></a></li><li class="interlanguage-link interwiki-zh-yue mw-list-item"><a href="https://zh-yue.wikipedia.org/wiki/%E8%87%AA%E7%B7%A8%E7%A2%BC%E5%99%A8" title="自編碼器 – Cantonese" lang="yue" hreflang="yue" class="interlanguage-link-target"><span>粵語</span></a></li><li class="interlanguage-link interwiki-zh mw-list-item"><a href="https://zh.wikipedia.org/wiki/%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8" title="自编码器 – Chinese" lang="zh" hreflang="zh" class="interlanguage-link-target"><span>中文</span></a></li></ul>
		<div class="after-portlet after-portlet-lang"><span class="wb-langlinks-edit wb-langlinks-link"><a href="https://www.wikidata.org/wiki/Special:EntityPage/Q786435#sitelinks-wikipedia" title="Edit interlanguage links" class="wbc-editpage">Edit links</a></span></div>
	</div>
</nav>

</div>

</div>

<footer id="footer" class="mw-footer" role="contentinfo" >
	<ul id="footer-info">
	<li id="footer-info-lastmod"> This page was last edited on 29 January 2022, at 05:46<span class="anonymous-show">&#160;(UTC)</span>.</li>
	<li id="footer-info-copyright">Text is available under the <a rel="license" href="//en.wikipedia.org/wiki/Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License">Creative Commons Attribution-ShareAlike License 3.0</a><a rel="license" href="//creativecommons.org/licenses/by-sa/3.0/" style="display:none;"></a>;
additional terms may apply.  By using this site, you agree to the <a href="//foundation.wikimedia.org/wiki/Terms_of_Use">Terms of Use</a> and <a href="//foundation.wikimedia.org/wiki/Privacy_policy">Privacy Policy</a>. Wikipedia® is a registered trademark of the <a href="//www.wikimediafoundation.org/">Wikimedia Foundation, Inc.</a>, a non-profit organization.</li>
</ul>

	<ul id="footer-places">
	<li id="footer-places-privacy"><a href="https://foundation.wikimedia.org/wiki/Privacy_policy" class="extiw" title="wmf:Privacy policy">Privacy policy</a></li>
	<li id="footer-places-about"><a href="/wiki/Wikipedia:About" title="Wikipedia:About">About Wikipedia</a></li>
	<li id="footer-places-disclaimer"><a href="/wiki/Wikipedia:General_disclaimer" title="Wikipedia:General disclaimer">Disclaimers</a></li>
	<li id="footer-places-contact"><a href="//en.wikipedia.org/wiki/Wikipedia:Contact_us">Contact Wikipedia</a></li>
	<li id="footer-places-mobileview"><a href="//en.m.wikipedia.org/w/index.php?title=Autoencoder&amp;lxml=&amp;mobileaction=toggle_view_mobile" class="noprint stopMobileRedirectToggle">Mobile view</a></li>
	<li id="footer-places-developers"><a href="https://www.mediawiki.org/wiki/Special:MyLanguage/How_to_contribute">Developers</a></li>
	<li id="footer-places-statslink"><a href="https://stats.wikimedia.org/#/en.wikipedia.org">Statistics</a></li>
	<li id="footer-places-cookiestatement"><a href="https://foundation.wikimedia.org/wiki/Cookie_statement">Cookie statement</a></li>
</ul>

	<ul id="footer-icons" class="noprint">
	<li id="footer-copyrightico"><a href="https://wikimediafoundation.org/"><img src="/static/images/footer/wikimedia-button.png" srcset="/static/images/footer/wikimedia-button-1.5x.png 1.5x, /static/images/footer/wikimedia-button-2x.png 2x" width="88" height="31" alt="Wikimedia Foundation" loading="lazy" /></a></li>
	<li id="footer-poweredbyico"><a href="https://www.mediawiki.org/"><img src="/static/images/footer/poweredby_mediawiki_88x31.png" alt="Powered by MediaWiki" srcset="/static/images/footer/poweredby_mediawiki_132x47.png 1.5x, /static/images/footer/poweredby_mediawiki_176x62.png 2x" width="88" height="31" loading="lazy"/></a></li>
</ul>

</footer>

<script>(RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgPageParseReport":{"limitreport":{"cputime":"1.008","walltime":"1.221","ppvisitednodes":{"value":3708,"limit":1000000},"postexpandincludesize":{"value":165644,"limit":2097152},"templateargumentsize":{"value":2452,"limit":2097152},"expansiondepth":{"value":12,"limit":100},"expensivefunctioncount":{"value":4,"limit":500},"unstrip-depth":{"value":1,"limit":20},"unstrip-size":{"value":157428,"limit":5000000},"entityaccesscount":{"value":0,"limit":400},"timingprofile":["100.00%  833.872      1 -total"," 54.29%  452.728      1 Template:Reflist"," 28.76%  239.809     22 Template:Cite_journal"," 10.63%   88.640      1 Template:Short_description"," 10.53%   87.810      1 Template:Machine_learning"," 10.20%   85.057      1 Template:Sidebar_with_collapsible_lists","  9.83%   82.011      4 Template:Navbox","  8.75%   72.981      1 Template:Differentiable_computing","  8.73%   72.823     12 Template:Cite_arXiv","  5.49%   45.795      1 Template:Distinguish"]},"scribunto":{"limitreport-timeusage":{"value":"0.488","limit":"10.000"},"limitreport-memusage":{"value":7265990,"limit":52428800}},"cachereport":{"origin":"mw1405","timestamp":"20220222162955","ttl":1814400,"transientcontent":false}}});});</script>
<script type="application/ld+json">{"@context":"https:\/\/schema.org","@type":"Article","name":"Autoencoder","url":"https:\/\/en.wikipedia.org\/wiki\/Autoencoder","sameAs":"http:\/\/www.wikidata.org\/entity\/Q786435","mainEntity":"http:\/\/www.wikidata.org\/entity\/Q786435","author":{"@type":"Organization","name":"Contributors to Wikimedia projects"},"publisher":{"@type":"Organization","name":"Wikimedia Foundation, Inc.","logo":{"@type":"ImageObject","url":"https:\/\/www.wikimedia.org\/static\/images\/wmf-hor-googpub.png"}},"datePublished":"2006-09-04T09:11:50Z","dateModified":"2022-01-29T05:46:07Z","image":"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/f\/fe\/Kernel_Machine.svg","headline":"neural network that learns efficient data encoding in an unsupervised manner"}</script>
<script>(RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgBackendResponseTime":173,"wgHostname":"mw1372"});});</script>
</body>
</html>